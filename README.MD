# Обучение языковой модели

## Эксперименты с гиперпараметрами

### Batch Size и Gradient Accumulation
- Проведено 15 экспериментов через `wandb.sweep`
- Лучший результат показало сочетание `per_device_train_batch_size=6` и `gradient_accumulation_steps=6`
- Возможно, большие значения дадут ещё больший буст

### Learning Rate и Scheduler
- После 15 экспериментов через `wandb.sweep` я делал кастомные эксперименты руками
- При высоком lr (1e-4 или 5e-5) модель перестаёт стабильно сходиться при достижении loss ≈ 4
- Добавлен кастомный scheduler, который снижает lr после N шагов
- Изначально использовался triangular scheduler, но он показался избыточным (хотя и дал лучший результат)
- Оставлено только снижение lr до определённого уровня

### Текущие результаты
- Снижение lr дало буст в обучении (результаты видны в wandb)
- Попытка снизить lr до ~5e-5 на 20-й минуте была не самой удачной
- Снижение lr помогает, но баланс сложно нащупать

### Планы на будущее
- Увеличить batch size и lr
- Сделать снижение lr более плавным
- Эксперимент: `per_device_train_batch_size=10` и `gradient_accumulation_steps=8` с большим, но более плавным lr показывает более медленную, но устойчивую сходимость
- На данный момент лучший показатель c `per_device_train_batch_size=16` и `gradient_accumulation_steps=4`. Пожалуй пока на этом остановлюсь
## Мониторинг
[Wandb Dashboard](https://wandb.ai/ksorzz/llm_hw1-aylesnov/workspace?nw=nwuserksorzz)


## Примеры генераций

**Промпт:** "В начале было слово"  
**Ограничение:** 64 токена

### Пример 1
```
В начале было слово, и слово было сделано в СССР и в Москве.

В культуре 

В романе М. И. Степанова принимал участие в съёмках фильма «Вторая история» (1957), и в «Пушкинском театре» (1957).

Примечания

Ссылки 

Дети кино
```
*Метрики: eval/loss=4.69, train/loss=4.09, lr=0.00049*

### Пример 2
```
В начале было слово, и слово было сделано от этого лица. В начале XIX века в XVIII веке на месте дома помещицы, в частности, «Афики» В. И. Гурова был убит под угрозой исследования в «Пушкинском государстве».

Примечания

Литература 

 Могии А
```
*Метрики: eval/loss=4.71, train/loss=3.65, lr=0.0001*

### Пример 3
```
В начале было слово, и слово было дано в честь святого Иоанна Иоанна Предтеи Ибн.

В честь святого Иоанна Дамасканцев, которое в честь святого Иоанна Крестителя Ибн Абу ибн Алир был снят в 1821 году и назван в честь великого визинина Ибн Мухаммеда, который был открыт в честь святого Петра I. В
```
*Метрики: eval/loss=4.20, train/loss=3.40, lr=0.0005*

### Пример 4
```
В начале было слово, и слово было сделано. В результате его пребывания в течение всего дня в «Крук-кэмк» (1958—1968).

Памятники 
В 1928 году в ходе Великой Отечественной войны в ходе операции «Кабло-Боттингем Форест» была расформирована.
```
*Метрики: eval/loss=4.73, train/loss=4.10, lr=0.0002*

### Пример 5
```
В начале было слово, и слово было дано от имени Ф. Г. Букша. В отличие от названия, в частности, «Новые» иероглифические «хождения» — буква «престига», в «грозу» обозначает буква «попа».

Изменность 
В некоторых источниках используется
```
*Метрики: eval/loss=3.91, train/loss=3.09, lr=5e-05*

### Пример 6 (Финальный лучший результат)
```
В начале было слово, и слово было дано от — «камень».

Впервые, в своей работе, в 1920-е годы (содержание в переводе «Памятники») был издан новый исторический тип издания (в переводе с английского — «Учение»).

В 1929 году в городе было создано отделение «Петровская
```
*Метрики: eval/loss=3.81, train/loss=3.17, lr=0.00017*