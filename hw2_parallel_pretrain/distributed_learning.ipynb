{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "# from lib.notebook_utils import setup_multiprocessing_for_notebook\n",
    "# setup_multiprocessing_for_notebook()\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "from solution import (\n",
    "    prepare_dataset,\n",
    "    build_baseline_setup,\n",
    "    build_deepspeed_setup,\n",
    "    build_fsdp_setup,\n",
    "    run_training_session,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# _ = prepare_dataset() # Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·)\n",
    "print('-----------------------')\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.getenv(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"WANDB_PROJECT = \", os.getenv(\"WANDB_PROJECT\"))\n",
    "print(\"WANDB_API_KEY =\", f'{os.getenv(\"WANDB_API_KEY\")[:3]}...{os.getenv(\"WANDB_API_KEY\")[-3:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75994b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch \\\n",
    "  --num_processes 2 --num_machines 1 --gpu_ids 2,3 \\\n",
    "  --mixed_precision bf16 \\\n",
    "  train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --timeout 1800 \\\n",
    "    --run-name \"ds1_accel_2gpus\" \\\n",
    "    --torch-compile false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 245370.62it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 3990.35it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "/app/lib/training.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_065905-v771ak37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_baseline_fc1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/v771ak37\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/30314 [00:00<?, ?it/s]W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 06:59:07.281000 755270 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 06:59:15.578000 755270 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 06:59:15.578000 755270 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 06:59:15.578000 755270 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 06:59:15.578000 755270 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 06:59:15.578000 755270 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.8741, 'grad_norm': 40.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 10.3258, 'grad_norm': 3.203125, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.7158, 'grad_norm': 3.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.5298, 'grad_norm': 1.7421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.3893, 'grad_norm': 1.296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2517, 'grad_norm': 1.2421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2149, 'grad_norm': 1.296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0619, 'grad_norm': 1.2734375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0437, 'grad_norm': 0.74609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8679, 'grad_norm': 0.74609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8454, 'grad_norm': 1.109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7326, 'grad_norm': 0.890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5864, 'grad_norm': 1.0625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4379, 'grad_norm': 0.71875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3529, 'grad_norm': 1.1484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.251, 'grad_norm': 0.796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1765, 'grad_norm': 0.7734375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0906, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.9857, 'grad_norm': 1.0234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.7636, 'grad_norm': 0.7578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6205, 'grad_norm': 0.73046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6193, 'grad_norm': 0.65625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.4276, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.4472, 'grad_norm': 0.75, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2523, 'grad_norm': 0.9609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2331, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1266, 'grad_norm': 0.65625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.0839, 'grad_norm': 0.70703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1056, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.9661, 'grad_norm': 0.69921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.8914, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9627, 'grad_norm': 0.703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8905, 'grad_norm': 0.79296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7682, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8758, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6952, 'grad_norm': 0.80078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7193, 'grad_norm': 0.63671875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5608, 'grad_norm': 0.62890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5284, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5567, 'grad_norm': 0.61328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4426, 'grad_norm': 0.625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4799, 'grad_norm': 0.75, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4944, 'grad_norm': 0.6640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4382, 'grad_norm': 0.5703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.2965, 'grad_norm': 0.6171875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3993, 'grad_norm': 0.5859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3989, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3443, 'grad_norm': 0.6328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.2724, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1299, 'grad_norm': 0.5703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.2133, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0879, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.31, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1292, 'grad_norm': 0.578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1629, 'grad_norm': 0.6953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1079, 'grad_norm': 0.57421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0466, 'grad_norm': 0.5703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0353, 'grad_norm': 0.56640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.9849, 'grad_norm': 0.4765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0858, 'grad_norm': 0.48046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0303, 'grad_norm': 0.546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8323, 'grad_norm': 0.5, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0141, 'grad_norm': 0.640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0076, 'grad_norm': 0.54296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8418, 'grad_norm': 0.55859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8855, 'grad_norm': 0.498046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7633, 'grad_norm': 0.578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8289, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8709, 'grad_norm': 0.59375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8877, 'grad_norm': 0.59375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8793, 'grad_norm': 0.48828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7392, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7456, 'grad_norm': 0.6484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7125, 'grad_norm': 0.5859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.653, 'grad_norm': 0.55859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.694, 'grad_norm': 0.5078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6853, 'grad_norm': 0.453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6313, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8305, 'grad_norm': 0.45703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7047, 'grad_norm': 0.484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7211, 'grad_norm': 0.51171875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6955, 'grad_norm': 0.51953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5641, 'grad_norm': 0.57421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5301, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5225, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.583, 'grad_norm': 0.515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.4781, 'grad_norm': 0.474609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5055, 'grad_norm': 0.5546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6165, 'grad_norm': 0.470703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "  1%|â–Œ                                    | 448/30314 [07:43<8:20:52,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --num_processes 1 --gpu_ids 1 ../mini.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba68059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 06:32:15,748 [INFO] __main__: ============================================================\n",
      "2025-11-03 06:32:15,748 [INFO] __main__: Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: baseline\n",
      "2025-11-03 06:32:15,748 [INFO] __main__: ============================================================\n",
      "2025-11-03 06:32:15,748 [INFO] __main__: World size: 1\n",
      "2025-11-03 06:32:15,749 [INFO] __main__: CUDA_VISIBLE_DEVICES: 1\n",
      "2025-11-03 06:32:15,749 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 06:32:15,749 [INFO] __main__: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ baseline setup (single GPU)\n",
      "--------------------------------\n",
      "{'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'bf16': True, 'report_to': 'wandb'}\n",
      "--------------------------------\n",
      "Namespace(mode='baseline', deepspeed_stage=2, offload_optimizer=False, offload_params=False, batch_size=16, grad_accum=4, max_steps=None, timeout=1800, run_name='baseline_test', no_wandb=False, data_dir='/app/output_dir', master_port=29500)\n",
      "--------------------------------\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 199431.99it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 4212.09it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 06:32:53,960 [INFO] lib.modeling: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: 960,881,664 Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
      "/app/lib/training.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_063257-573r3qpk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_baseline_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/573r3qpk\u001b[0m\n",
      "2025-11-03 06:32:58,447 [INFO] __main__: Setup ÑÐ¾Ð·Ð´Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾\n",
      "2025-11-03 06:32:58,447 [INFO] __main__: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb'}\n",
      "2025-11-03 06:32:58,447 [INFO] __main__: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ...\n",
      "2025-11-03 06:32:58,447 [INFO] lib.training: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/30314 [00:00<?, ?it/s][rank0]:W1103 06:32:59.461000 679041 torch/_logging/_internal.py:1080] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] or:\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] to include these operations in the captured graph.\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] \n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] Graph break: from user code at:\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     return model_forward(*args, **kwargs)\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     return func(*args, **kwargs)\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     causal_mask = self._update_causal_mask(\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] \n",
      "[rank0]:W1103 06:32:59.627000 679041 torch/_dynamo/variables/tensor.py:780] [1/0] \n",
      "Traceback (most recent call last):\n",
      "  File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 220, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 210, in main\n",
      "    metrics = run_training_session(setup, final_evaluation=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/solution.py\", line 242, in run_training_session\n",
      "    return run_training(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/app/lib/training.py\", line 138, in run_training\n",
      "    train_output = trainer.train()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 3745, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 3810, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py\", line 40, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n",
      "    else self._run_ddp_forward(*inputs, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n",
      "    return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "                                       ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 397, in forward\n",
      "    @can_return_tuple\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 463, in torch_dynamo_resume_in_forward_at_446\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 48, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 284, in forward\n",
      "    hidden_states, self_attn_weights = self.self_attn(\n",
      "                                       ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 235, in forward\n",
      "    attn_output, attn_weights = attention_interface(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/integrations/flash_attention.py\", line 49, in flash_attention_forward\n",
      "    attn_output = _flash_attention_forward(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_flash_attention_utils.py\", line 279, in _flash_attention_forward\n",
      "    def _flash_attention_forward(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_flash_attention_utils.py\", line 111, in _upad_input\n",
      "    def _upad_input(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1327, in __call__\n",
      "    return hijacked_callback(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1124, in __call__\n",
      "    result = self._inner_convert(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 528, in __call__\n",
      "    return _compile(\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 948, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 679, in compile_inner\n",
      "    return _compile_inner(code, one_graph, hooks, transform)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 712, in _compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1337, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 221, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 641, in transform\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2766, in run\n",
      "    super().run()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 973, in run\n",
      "    while self.step():\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 885, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2957, in RETURN_VALUE\n",
      "    self._return(inst)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2942, in _return\n",
      "    self.output.compile_subgraph(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
      "    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "    compiled_fn = self.call_user_compiler(gm)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "    return self._call_user_compiler(gm)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "    compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/distributed.py\", line 424, in compile_fn\n",
      "    raise NotImplementedError(\n",
      "torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:\n",
      "NotImplementedError: DDPOptimizer backend: Found a higher order op in the graph. This is not supported. Please turn off DDP optimizer using torch._dynamo.config.optimize_ddp=False. Note that this can cause performance degradation because there will be one bucket for the entire Dynamo graph. Please refer to this issue - https://github.com/pytorch/pytorch/issues/104674.\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 220, in <module>\n",
      "[rank0]:     sys.exit(main())\n",
      "[rank0]:              ^^^^^^\n",
      "[rank0]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 210, in main\n",
      "[rank0]:     metrics = run_training_session(setup, final_evaluation=True)\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/app/solution.py\", line 242, in run_training_session\n",
      "[rank0]:     return run_training(\n",
      "[rank0]:            ^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/app/lib/training.py\", line 138, in run_training\n",
      "[rank0]:     train_output = trainer.train()\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2240, in train\n",
      "[rank0]:     return inner_training_loop(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 3745, in training_step\n",
      "[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 3810, in compute_loss\n",
      "[rank0]:     outputs = model(**inputs)\n",
      "[rank0]:               ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\", line 465, in _fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py\", line 40, in inner\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n",
      "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
      "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n",
      "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "[rank0]:     return model_forward(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:     output = func(self, *args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:                                        ^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:     output = func(self, *args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 397, in forward\n",
      "[rank0]:     @can_return_tuple\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 463, in torch_dynamo_resume_in_forward_at_446\n",
      "[rank0]:     layer_outputs = decoder_layer(\n",
      "[rank0]:                     ^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 48, in __call__\n",
      "[rank0]:     return super().__call__(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 284, in forward\n",
      "[rank0]:     hidden_states, self_attn_weights = self.self_attn(\n",
      "[rank0]:                                        ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 235, in forward\n",
      "[rank0]:     attn_output, attn_weights = attention_interface(\n",
      "[rank0]:                                 ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/integrations/flash_attention.py\", line 49, in flash_attention_forward\n",
      "[rank0]:     attn_output = _flash_attention_forward(\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_flash_attention_utils.py\", line 279, in _flash_attention_forward\n",
      "[rank0]:     def _flash_attention_forward(\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_flash_attention_utils.py\", line 111, in _upad_input\n",
      "[rank0]:     def _upad_input(\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1327, in __call__\n",
      "[rank0]:     return hijacked_callback(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1124, in __call__\n",
      "[rank0]:     result = self._inner_convert(\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 528, in __call__\n",
      "[rank0]:     return _compile(\n",
      "[rank0]:            ^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 948, in _compile\n",
      "[rank0]:     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 679, in compile_inner\n",
      "[rank0]:     return _compile_inner(code, one_graph, hooks, transform)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "[rank0]:     return function(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 712, in _compile_inner\n",
      "[rank0]:     out_code = transform_code_object(code, transform)\n",
      "[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1337, in transform_code_object\n",
      "[rank0]:     transformations(instructions, code_options)\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 221, in _fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 641, in transform\n",
      "[rank0]:     tracer.run()\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2766, in run\n",
      "[rank0]:     super().run()\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 973, in run\n",
      "[rank0]:     while self.step():\n",
      "[rank0]:           ^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 885, in step\n",
      "[rank0]:     self.dispatch_table[inst.opcode](self, inst)\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2957, in RETURN_VALUE\n",
      "[rank0]:     self._return(inst)\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2942, in _return\n",
      "[rank0]:     self.output.compile_subgraph(\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
      "[rank0]:     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "[rank0]:     compiled_fn = self.call_user_compiler(gm)\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "[rank0]:     return self._call_user_compiler(gm)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "[rank0]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "[rank0]:     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/backends/distributed.py\", line 424, in compile_fn\n",
      "[rank0]:     raise NotImplementedError(\n",
      "[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:\n",
      "[rank0]: NotImplementedError: DDPOptimizer backend: Found a higher order op in the graph. This is not supported. Please turn off DDP optimizer using torch._dynamo.config.optimize_ddp=False. Note that this can cause performance degradation because there will be one bucket for the entire Dynamo graph. Please refer to this issue - https://github.com/pytorch/pytorch/issues/104674.\n",
      "\n",
      "[rank0]: Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "[rank0]: You can suppress this exception and fall back to eager by setting:\n",
      "[rank0]:     import torch._dynamo\n",
      "[rank0]:     torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_baseline_test\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/573r3qpk\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_063257-573r3qpk/logs\u001b[0m\n",
      "E1103 06:33:08.324000 679010 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 679041) of binary: /usr/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.6.0a0+df5bbc09d1.nv24.12', 'console_scripts', 'torchrun')())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "../train_distributed.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-03_06:33:08\n",
      "  host      : da18cebda4f2\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 679041)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 torchrun --nproc_per_node=1 --master_port=29501 ../train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"baseline_test\"\n",
    "\n",
    "# ~ 29.5Ð³Ð± Ð½Ð° 1 GPU, Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾Ð»Ð½Ð°Ñ (95-100%)\n",
    "# Ð£ÑÐ¿ÐµÐ»Ð¾ Ð¿Ñ€Ð¾Ð¹Ñ‚Ð¸ 1175 ÑˆÐ°Ð³Ð¾Ð² Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf21aab",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 18:36:29,438 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,438 [INFO] __main__: Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: deepspeed\n",
      "2025-11-02 18:36:29,438 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: deepspeed\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: World size: 2\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DeepSpeed setup (stage 1)\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 270600.26it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 2788.49it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 254682.60it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 3978.58it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-02 18:37:07,957 [INFO] lib.modeling: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: 960,881,664 Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
      "2025-11-02 18:37:08,732 [INFO] lib.modeling: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: 960,881,664 Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
      "2025-11-02 18:37:10,905 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-11-02 18:37:10,919 [INFO] lib.training: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
      "2025-11-02 18:37:10,922 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251102_183711-q6scqvdn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/q6scqvdn\u001b[0m\n",
      "2025-11-02 18:37:12,857 [INFO] __main__: Setup ÑÐ¾Ð·Ð´Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾\n",
      "2025-11-02 18:37:12,857 [INFO] __main__: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-02 18:37:12,858 [INFO] __main__: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ...\n",
      "2025-11-02 18:37:12,858 [INFO] lib.training: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/15157 [00:00<?, ?it/s][rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 10.6919, 'grad_norm': 3.667879104614258, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5142, 'grad_norm': 4.834280014038086, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8381, 'grad_norm': 3.3495538234710693, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4606, 'grad_norm': 1.1649107933044434, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1784, 'grad_norm': 1.1234673261642456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0081, 'grad_norm': 0.9271871447563171, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8174, 'grad_norm': 0.9527190327644348, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7291, 'grad_norm': 0.7766323685646057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6891, 'grad_norm': 0.7055681347846985, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.579, 'grad_norm': 0.6939873099327087, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4326, 'grad_norm': 0.9865968823432922, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3369, 'grad_norm': 0.8434524536132812, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2052, 'grad_norm': 0.7207794785499573, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1416, 'grad_norm': 1.0892233848571777, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1121, 'grad_norm': 0.924543023109436, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0345, 'grad_norm': 1.024427890777588, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9505, 'grad_norm': 0.9839465022087097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.938, 'grad_norm': 0.9596324563026428, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8168, 'grad_norm': 0.922659158706665, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7224, 'grad_norm': 0.8615850210189819, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6355, 'grad_norm': 0.8510178923606873, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6369, 'grad_norm': 0.7569057941436768, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5431, 'grad_norm': 0.7975549101829529, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5706, 'grad_norm': 0.8864257335662842, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3867, 'grad_norm': 0.8010669350624084, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3187, 'grad_norm': 0.8247262835502625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4063, 'grad_norm': 1.0405585765838623, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3246, 'grad_norm': 0.8971840739250183, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2296, 'grad_norm': 0.8301199078559875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2263, 'grad_norm': 0.8263239860534668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1102, 'grad_norm': 0.8279992938041687, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1778, 'grad_norm': 1.0444562435150146, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0663, 'grad_norm': 0.9371616244316101, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9674, 'grad_norm': 0.9814410209655762, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0441, 'grad_norm': 0.8351292610168457, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.981, 'grad_norm': 0.9901759624481201, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8896, 'grad_norm': 0.9831438660621643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8088, 'grad_norm': 0.9669746160507202, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8419, 'grad_norm': 0.9979885220527649, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.896, 'grad_norm': 0.8153659105300903, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8679, 'grad_norm': 0.9968246817588806, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6678, 'grad_norm': 1.005164623260498, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6786, 'grad_norm': 0.8769382834434509, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6353, 'grad_norm': 0.8790581226348877, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6899, 'grad_norm': 0.8514677286148071, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5394, 'grad_norm': 1.1342464685440063, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6428, 'grad_norm': 1.093551754951477, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5992, 'grad_norm': 0.8555868268013, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5517, 'grad_norm': 0.9614889025688171, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5031, 'grad_norm': 0.9059622883796692, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4323, 'grad_norm': 0.917082667350769, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5109, 'grad_norm': 0.832839846611023, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.456, 'grad_norm': 0.8498347401618958, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4512, 'grad_norm': 0.9275631308555603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3715, 'grad_norm': 1.0327497720718384, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3933, 'grad_norm': 0.9068701267242432, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2537, 'grad_norm': 0.9387128353118896, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3144, 'grad_norm': 0.9276350140571594, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4114, 'grad_norm': 0.787821888923645, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2858, 'grad_norm': 0.860021710395813, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3146, 'grad_norm': 0.8115845322608948, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3107, 'grad_norm': 0.8559317588806152, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2852, 'grad_norm': 0.8631620407104492, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2138, 'grad_norm': 1.000807523727417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1666, 'grad_norm': 1.0203839540481567, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.239, 'grad_norm': 1.0554753541946411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1035, 'grad_norm': 0.9241543412208557, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1167, 'grad_norm': 0.8686804175376892, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0631, 'grad_norm': 0.9896292686462402, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0961, 'grad_norm': 0.8021072745323181, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1158, 'grad_norm': 0.9412711262702942, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0807, 'grad_norm': 0.8521290421485901, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0746, 'grad_norm': 1.255803108215332, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0749, 'grad_norm': 0.9653118252754211, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0875, 'grad_norm': 0.8498719334602356, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.985, 'grad_norm': 0.8668097853660583, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9725, 'grad_norm': 0.8692824244499207, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9274, 'grad_norm': 0.8487938046455383, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8984, 'grad_norm': 0.9646814465522766, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9856, 'grad_norm': 0.9075605273246765, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9639, 'grad_norm': 0.8378884196281433, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.983, 'grad_norm': 0.901380717754364, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8973, 'grad_norm': 0.8806772828102112, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.989, 'grad_norm': 0.926916778087616, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7466, 'grad_norm': 0.9136208891868591, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8408, 'grad_norm': 0.8051066398620605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7971, 'grad_norm': 0.9543333649635315, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8591, 'grad_norm': 0.9228493571281433, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8108, 'grad_norm': 0.8425719738006592, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8353, 'grad_norm': 0.8186773657798767, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8174, 'grad_norm': 0.966996431350708, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.767, 'grad_norm': 0.8752743601799011, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.7999, 'grad_norm': 0.9559480547904968, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.69, 'grad_norm': 0.8767609000205994, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7658, 'grad_norm': 0.8830179572105408, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7873, 'grad_norm': 0.979434609413147, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.6995, 'grad_norm': 0.7963792085647583, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7015, 'grad_norm': 0.8574976921081543, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7628, 'grad_norm': 0.9147696495056152, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6845, 'grad_norm': 0.931691586971283, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7021, 'grad_norm': 0.8537668585777283, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6919, 'grad_norm': 0.8725757598876953, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.629, 'grad_norm': 0.9671273827552795, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6728, 'grad_norm': 0.9306085109710693, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6205, 'grad_norm': 0.9877721667289734, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5798, 'grad_norm': 0.8931983709335327, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.592, 'grad_norm': 1.0162192583084106, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5825, 'grad_norm': 0.834335446357727, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5738, 'grad_norm': 0.9541210532188416, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5727, 'grad_norm': 0.9583180546760559, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5376, 'grad_norm': 0.8774888515472412, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.583, 'grad_norm': 0.9095102548599243, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6139, 'grad_norm': 0.8344876766204834, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5453, 'grad_norm': 0.9107238054275513, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5297, 'grad_norm': 0.7896857857704163, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.54, 'grad_norm': 0.8841339349746704, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5222, 'grad_norm': 0.8691622018814087, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4877, 'grad_norm': 0.880927324295044, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4702, 'grad_norm': 0.8557435870170593, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.4655, 'grad_norm': 0.9839643239974976, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5504, 'grad_norm': 0.9460034370422363, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.457, 'grad_norm': 0.9450525045394897, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4385, 'grad_norm': 0.9350773692131042, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5125, 'grad_norm': 0.9361116886138916, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4843, 'grad_norm': 0.9770078659057617, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4331, 'grad_norm': 1.0450639724731445, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.461, 'grad_norm': 1.105711817741394, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4767, 'grad_norm': 0.8797963857650757, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4578, 'grad_norm': 0.8582158088684082, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4914, 'grad_norm': 0.9478503465652466, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4626, 'grad_norm': 0.8924883604049683, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4274, 'grad_norm': 0.8795104622840881, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.3764, 'grad_norm': 0.9075436592102051, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4696, 'grad_norm': 0.9432616829872131, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3367, 'grad_norm': 0.8024809956550598, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3395, 'grad_norm': 0.8065676093101501, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3803, 'grad_norm': 0.8119853734970093, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4291, 'grad_norm': 0.8572216033935547, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4599, 'grad_norm': 0.8669925332069397, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3264, 'grad_norm': 0.894651472568512, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3719, 'grad_norm': 0.8172014355659485, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4648, 'grad_norm': 0.8716773986816406, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.3564, 'grad_norm': 0.8373059034347534, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.315, 'grad_norm': 0.8717809915542603, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2834, 'grad_norm': 0.8495769500732422, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2581, 'grad_norm': 0.8540050983428955, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.2998, 'grad_norm': 0.8447644114494324, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3678, 'grad_norm': 0.8612174391746521, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3278, 'grad_norm': 0.9611419439315796, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.195, 'grad_norm': 0.8545340299606323, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.319, 'grad_norm': 0.9166994094848633, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3153, 'grad_norm': 0.8658009171485901, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2568, 'grad_norm': 0.8619129061698914, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3265, 'grad_norm': 0.8345709443092346, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2417, 'grad_norm': 0.8969289660453796, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2441, 'grad_norm': 1.0291699171066284, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2773, 'grad_norm': 1.01828932762146, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2018, 'grad_norm': 0.8149434924125671, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.1988, 'grad_norm': 0.8818775415420532, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2771, 'grad_norm': 0.9484167695045471, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2271, 'grad_norm': 0.8450266122817993, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1545, 'grad_norm': 0.8303521275520325, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2274, 'grad_norm': 0.8602861762046814, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2262, 'grad_norm': 0.8777468800544739, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.269, 'grad_norm': 0.8417526483535767, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.274, 'grad_norm': 0.8356276154518127, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1682, 'grad_norm': 0.8843129277229309, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1708, 'grad_norm': 0.8013651967048645, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1895, 'grad_norm': 0.8695991635322571, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2231, 'grad_norm': 0.8430740833282471, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2174, 'grad_norm': 0.9432917833328247, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1528, 'grad_norm': 0.9237110614776611, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.195, 'grad_norm': 0.8522312641143799, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 4.1067, 'grad_norm': 0.9452381730079651, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 4.17, 'grad_norm': 0.7784274816513062, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "{'loss': 4.1252, 'grad_norm': 0.9013524651527405, 'learning_rate': 0.0005256322580645161, 'epoch': 0.06}\n",
      "{'loss': 4.1164, 'grad_norm': 0.7856761813163757, 'learning_rate': 0.0005235999999999999, 'epoch': 0.06}\n",
      "{'loss': 4.1233, 'grad_norm': 0.7758393287658691, 'learning_rate': 0.0005215677419354839, 'epoch': 0.06}\n",
      "{'loss': 4.1145, 'grad_norm': 0.8431938886642456, 'learning_rate': 0.0005195354838709677, 'epoch': 0.06}\n",
      "{'loss': 4.1548, 'grad_norm': 1.0190341472625732, 'learning_rate': 0.0005175032258064515, 'epoch': 0.06}\n",
      "{'loss': 4.1786, 'grad_norm': 0.9283260703086853, 'learning_rate': 0.0005154709677419354, 'epoch': 0.06}\n",
      "{'loss': 4.1751, 'grad_norm': 0.9026321768760681, 'learning_rate': 0.0005134387096774193, 'epoch': 0.06}\n",
      "{'loss': 4.1144, 'grad_norm': 0.9255345463752747, 'learning_rate': 0.0005114064516129032, 'epoch': 0.06}\n",
      "{'loss': 4.123, 'grad_norm': 0.8433995842933655, 'learning_rate': 0.0005093741935483871, 'epoch': 0.06}\n",
      "{'loss': 4.1115, 'grad_norm': 0.827038049697876, 'learning_rate': 0.0005073419354838709, 'epoch': 0.06}\n",
      "{'loss': 4.1883, 'grad_norm': 0.8610161542892456, 'learning_rate': 0.0005053096774193548, 'epoch': 0.06}\n",
      "{'loss': 4.0977, 'grad_norm': 0.8711548447608948, 'learning_rate': 0.0005032774193548386, 'epoch': 0.06}\n",
      "{'loss': 4.0821, 'grad_norm': 0.9176371097564697, 'learning_rate': 0.0005012451612903226, 'epoch': 0.06}\n",
      "{'loss': 4.1035, 'grad_norm': 0.8772139549255371, 'learning_rate': 0.0004992129032258064, 'epoch': 0.06}\n",
      "{'loss': 4.0909, 'grad_norm': 0.874914288520813, 'learning_rate': 0.0004971806451612902, 'epoch': 0.06}\n",
      "{'loss': 4.0282, 'grad_norm': 0.8976165056228638, 'learning_rate': 0.0004951483870967742, 'epoch': 0.06}\n",
      "{'loss': 4.0559, 'grad_norm': 0.8373813033103943, 'learning_rate': 0.000493116129032258, 'epoch': 0.06}\n",
      "{'loss': 4.1733, 'grad_norm': 0.8405694365501404, 'learning_rate': 0.000491083870967742, 'epoch': 0.06}\n",
      "{'loss': 4.0052, 'grad_norm': 0.87267005443573, 'learning_rate': 0.0004890516129032258, 'epoch': 0.06}\n",
      "{'loss': 4.0633, 'grad_norm': 0.9377471804618835, 'learning_rate': 0.0004870193548387096, 'epoch': 0.06}\n",
      "{'loss': 4.1776, 'grad_norm': 0.8689622282981873, 'learning_rate': 0.00048498709677419346, 'epoch': 0.06}\n",
      "{'loss': 4.0152, 'grad_norm': 0.8716417551040649, 'learning_rate': 0.0004829548387096773, 'epoch': 0.06}\n",
      "{'loss': 4.1405, 'grad_norm': 0.7633777856826782, 'learning_rate': 0.0004809225806451613, 'epoch': 0.07}\n",
      "{'loss': 4.0923, 'grad_norm': 0.789438009262085, 'learning_rate': 0.00047889032258064513, 'epoch': 0.07}\n",
      "{'loss': 4.0622, 'grad_norm': 0.8585381507873535, 'learning_rate': 0.000476858064516129, 'epoch': 0.07}\n",
      "{'loss': 4.0008, 'grad_norm': 0.7862159013748169, 'learning_rate': 0.00047482580645161285, 'epoch': 0.07}\n",
      "{'loss': 4.0483, 'grad_norm': 0.8336480855941772, 'learning_rate': 0.0004727935483870967, 'epoch': 0.07}\n",
      "{'loss': 3.936, 'grad_norm': 0.7503958344459534, 'learning_rate': 0.0004707612903225806, 'epoch': 0.07}\n",
      "{'loss': 4.0581, 'grad_norm': 0.8322245478630066, 'learning_rate': 0.0004687290322580644, 'epoch': 0.07}\n",
      "{'loss': 4.0316, 'grad_norm': 0.8769118189811707, 'learning_rate': 0.0004666967741935484, 'epoch': 0.07}\n",
      "{'loss': 3.9827, 'grad_norm': 0.8828204274177551, 'learning_rate': 0.00046466451612903225, 'epoch': 0.07}\n",
      "{'loss': 4.0278, 'grad_norm': 0.826027512550354, 'learning_rate': 0.0004626322580645161, 'epoch': 0.07}\n",
      "{'loss': 3.9745, 'grad_norm': 0.8396434187889099, 'learning_rate': 0.0004606, 'epoch': 0.07}\n",
      "{'loss': 3.985, 'grad_norm': 0.9325892925262451, 'learning_rate': 0.0004585677419354838, 'epoch': 0.07}\n",
      "{'loss': 3.9536, 'grad_norm': 0.7828598618507385, 'learning_rate': 0.0004565354838709677, 'epoch': 0.07}\n",
      "{'loss': 3.9383, 'grad_norm': 0.7971042394638062, 'learning_rate': 0.0004545032258064516, 'epoch': 0.07}\n",
      "{'loss': 3.9207, 'grad_norm': 0.9213614463806152, 'learning_rate': 0.0004524709677419354, 'epoch': 0.07}\n",
      "{'loss': 4.0187, 'grad_norm': 0.8571704030036926, 'learning_rate': 0.00045043870967741937, 'epoch': 0.07}\n",
      "{'loss': 4.004, 'grad_norm': 0.8306555151939392, 'learning_rate': 0.0004484064516129032, 'epoch': 0.07}\n",
      "{'loss': 3.9743, 'grad_norm': 0.7866419553756714, 'learning_rate': 0.0004463741935483871, 'epoch': 0.07}\n",
      "{'loss': 3.9674, 'grad_norm': 0.822945237159729, 'learning_rate': 0.00044434193548387093, 'epoch': 0.07}\n",
      "{'loss': 3.9546, 'grad_norm': 0.7810685038566589, 'learning_rate': 0.00044230967741935477, 'epoch': 0.07}\n",
      "{'loss': 3.9679, 'grad_norm': 0.8626220226287842, 'learning_rate': 0.0004402774193548387, 'epoch': 0.07}\n",
      "{'loss': 4.0492, 'grad_norm': 0.8550321459770203, 'learning_rate': 0.00043824516129032254, 'epoch': 0.07}\n",
      "{'loss': 3.9202, 'grad_norm': 0.9097416400909424, 'learning_rate': 0.0004362129032258064, 'epoch': 0.07}\n",
      "{'loss': 4.0377, 'grad_norm': 0.9113086462020874, 'learning_rate': 0.0004341806451612903, 'epoch': 0.07}\n",
      "{'loss': 3.937, 'grad_norm': 0.9276189804077148, 'learning_rate': 0.00043214838709677416, 'epoch': 0.07}\n",
      "{'loss': 3.8298, 'grad_norm': 0.9051833748817444, 'learning_rate': 0.00043011612903225805, 'epoch': 0.07}\n",
      "  7%|â–ˆâ–ˆâ–‹                                 | 1119/15157 [29:59<6:12:42,  1.59s/it]2025-11-02 19:07:18,538 [INFO] lib.callbacks: ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñƒ: 1800.85 c\n",
      "{'loss': 3.9992, 'grad_norm': 0.824562668800354, 'learning_rate': 0.0004280838709677419, 'epoch': 0.07}\n",
      "{'train_runtime': 1800.8586, 'train_samples_per_second': 1077.299, 'train_steps_per_second': 8.417, 'train_loss': 4.978606733254024, 'epoch': 0.07}\n",
      "  7%|â–ˆâ–ˆâ–‹                                 | 1120/15157 [30:00<6:16:10,  1.61s/it]\n",
      "2025-11-02 19:07:18,688 [INFO] lib.training: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
      "2025-11-02 19:07:18,726 [INFO] lib.training: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:19<00:00, 31.85it/s]\n",
      "2025-11-02 19:07:40,431 [INFO] lib.training: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ñ‚ÐµÐºÑÑ‚Ð°\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-02 19:07:44,718 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-02 19:07:44,718 [INFO] lib.training: Ð’ Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ»Ð¾Ð²Ð¾, Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð±Ñ‹Ð»Ð¾ ÑÐ´ÐµÐ»Ð°Ð½Ð¾.\n",
      "\n",
      "Ð£Ñ‡Ð¸Ð»ÑÑ Ð² ÑˆÐºÐ¾Ð»Ðµ Ð² Ð’Ð°Ñ€ÑˆÐ°Ð²Ðµ Ð² 1978 Ð³Ð¾Ð´Ñƒ, Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð² ÑˆÐºÐ¾Ð»Ðµ, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð² Â«ÐÐ¾Ð²Ð¾Ð¹Â», Ð³Ð´Ðµ Ð² 2002 Ð³Ð¾Ð´Ñƒ Ð²Ð¾Ð·Ð³Ð»Ð°Ð²Ð¸Ð» Â«Ð•Ñ€Ð¼Â» Ð¸ Â«ÐÑÑÐ¾Ñ†Ð¸Ð°Ñ†Ð¸ÑÂ», Ð° Ð² 2003 Ð³Ð¾Ð´Ñƒ ÑÑ‚Ð°Ð» Ñ‡Ð»ÐµÐ½Ð¾Ð¼-ÐºÐ¾Ñ€Ñ€ÐµÑÐ¿Ð¾Ð½Ð´ÐµÐ½Ñ‚ ÐÐÐ, Ð¸ Ð² 2007 Ð³Ð¾Ð´Ñƒ. Ð¡ 2004 Ð¿Ð¾ 2007 Ð³Ð¾Ð´\n",
      "2025-11-02 19:07:44,719 [INFO] __main__: ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!\n",
      "2025-11-02 19:07:44,719 [INFO] __main__: ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸: {'train': {'train_runtime': 1800.8586, 'train_samples_per_second': 1077.299, 'train_steps_per_second': 8.417, 'total_flos': 3.7784517372542976e+17, 'train_loss': 4.978606733254024, 'epoch': 0.07389446946080129}, 'eval': {'eval_loss': 4.615426540374756, 'eval_runtime': 21.7023, 'eval_samples_per_second': 230.391, 'eval_steps_per_second': 28.799, 'epoch': 0.07389446946080129}, 'generation': 'Ð’ Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ»Ð¾Ð²Ð¾, Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð±Ñ‹Ð»Ð¾ ÑÐ´ÐµÐ»Ð°Ð½Ð¾.\\n\\nÐ£Ñ‡Ð¸Ð»ÑÑ Ð² ÑˆÐºÐ¾Ð»Ðµ Ð² Ð’Ð°Ñ€ÑˆÐ°Ð²Ðµ Ð² 1978 Ð³Ð¾Ð´Ñƒ, Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð² ÑˆÐºÐ¾Ð»Ðµ, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð² Â«ÐÐ¾Ð²Ð¾Ð¹Â», Ð³Ð´Ðµ Ð² 2002 Ð³Ð¾Ð´Ñƒ Ð²Ð¾Ð·Ð³Ð»Ð°Ð²Ð¸Ð» Â«Ð•Ñ€Ð¼Â» Ð¸ Â«ÐÑÑÐ¾Ñ†Ð¸Ð°Ñ†Ð¸ÑÂ», Ð° Ð² 2003 Ð³Ð¾Ð´Ñƒ ÑÑ‚Ð°Ð» Ñ‡Ð»ÐµÐ½Ð¾Ð¼-ÐºÐ¾Ñ€Ñ€ÐµÑÐ¿Ð¾Ð½Ð´ÐµÐ½Ñ‚ ÐÐÐ, Ð¸ Ð² 2007 Ð³Ð¾Ð´Ñƒ. Ð¡ 2004 Ð¿Ð¾ 2007 Ð³Ð¾Ð´'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/q6scqvdn\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251102_183711-q6scqvdn/logs\u001b[0m\n",
      "[rank0]:[W1102 19:07:48.714223810 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=29502 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"deep_speed_setup_1\"\n",
    "\n",
    "# ~ Ð¿Ð¾ 53Ð³Ð± Ð½Ð° ÐºÐ°Ð¶Ð´Ñ‹Ð¹ GPU, Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾Ð»Ð½Ð°Ñ (95-100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412c424",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f9951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 18:35:36,764 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: deepspeed\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: World size: 2\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ DeepSpeed setup (stage 2)\n",
      "2025-11-02 18:35:37,066 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:37,066 [INFO] __main__: Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ: deepspeed\n",
      "2025-11-02 18:35:37,067 [INFO] __main__: ============================================================\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 250874.26it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 2488.00it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 213044.01it/s]\n",
      "Loading dataset shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:00<00:00, 4174.36it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-02 18:36:14,590 [INFO] lib.modeling: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: 960,881,664 Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
      "2025-11-02 18:36:15,179 [INFO] lib.modeling: Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: 960,881,664 Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²\n",
      "2025-11-02 18:36:17,365 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-11-02 18:36:17,378 [INFO] lib.training: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
      "2025-11-02 18:36:17,379 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251102_183618-ohdroyih\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ohdroyih\u001b[0m\n",
      "2025-11-02 18:36:19,340 [INFO] __main__: Setup ÑÐ¾Ð·Ð´Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾\n",
      "2025-11-02 18:36:19,340 [INFO] __main__: ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-02 18:36:19,341 [INFO] __main__: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ...\n",
      "2025-11-02 18:36:19,341 [INFO] lib.training: Ð¡Ñ‚Ð°Ñ€Ñ‚ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/15157 [00:00<?, ?it/s][rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 10.7802, 'grad_norm': 4.457711696624756, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.814, 'grad_norm': 1.1450443267822266, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 9.1123, 'grad_norm': 0.6917006373405457, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6894, 'grad_norm': 0.4294556975364685, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4075, 'grad_norm': 0.3068470358848572, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2607, 'grad_norm': 0.46698296070098877, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0793, 'grad_norm': 0.3000651001930237, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0013, 'grad_norm': 0.30320146679878235, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9758, 'grad_norm': 0.2829124927520752, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9002, 'grad_norm': 0.2770126760005951, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8058, 'grad_norm': 0.22830359637737274, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7603, 'grad_norm': 0.30593517422676086, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6773, 'grad_norm': 0.25265219807624817, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6612, 'grad_norm': 0.3519551157951355, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6512, 'grad_norm': 0.2797471582889557, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6075, 'grad_norm': 0.30536147952079773, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5875, 'grad_norm': 0.35861480236053467, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5731, 'grad_norm': 0.36572617292404175, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.496, 'grad_norm': 0.33908629417419434, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4459, 'grad_norm': 0.32602888345718384, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3744, 'grad_norm': 0.3620181679725647, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3807, 'grad_norm': 0.3962878882884979, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.313, 'grad_norm': 0.3175576627254486, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3354, 'grad_norm': 0.3439221680164337, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1872, 'grad_norm': 0.33288681507110596, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1349, 'grad_norm': 0.33349698781967163, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.2125, 'grad_norm': 0.3194226622581482, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1455, 'grad_norm': 0.3063974380493164, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0892, 'grad_norm': 0.31547030806541443, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0741, 'grad_norm': 0.31945890188217163, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9759, 'grad_norm': 0.32254472374916077, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0206, 'grad_norm': 0.3286651074886322, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9289, 'grad_norm': 0.29229632019996643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8534, 'grad_norm': 0.30113258957862854, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9235, 'grad_norm': 0.3840930759906769, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8702, 'grad_norm': 0.3180418014526367, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7868, 'grad_norm': 0.36939695477485657, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7187, 'grad_norm': 0.3668995201587677, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7464, 'grad_norm': 0.41002270579338074, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7711, 'grad_norm': 0.3205455243587494, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7618, 'grad_norm': 0.33544132113456726, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5692, 'grad_norm': 0.3724505603313446, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5679, 'grad_norm': 0.3030220866203308, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5567, 'grad_norm': 0.35912564396858215, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5775, 'grad_norm': 0.36484673619270325, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4596, 'grad_norm': 0.4041992425918579, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.5205, 'grad_norm': 0.2982620596885681, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4909, 'grad_norm': 0.31290310621261597, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4435, 'grad_norm': 0.32285112142562866, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3926, 'grad_norm': 0.3311881124973297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3433, 'grad_norm': 0.366845965385437, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4061, 'grad_norm': 0.34642431139945984, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.354, 'grad_norm': 0.3501901924610138, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3238, 'grad_norm': 0.29291340708732605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2694, 'grad_norm': 0.33467787504196167, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2843, 'grad_norm': 0.327462375164032, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1601, 'grad_norm': 0.34262213110923767, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2047, 'grad_norm': 0.30100950598716736, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2935, 'grad_norm': 0.3307054340839386, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1703, 'grad_norm': 0.43660768866539, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1843, 'grad_norm': 0.31481683254241943, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2083, 'grad_norm': 0.31412428617477417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1647, 'grad_norm': 0.3290216326713562, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1138, 'grad_norm': 0.3842146396636963, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0524, 'grad_norm': 0.4256753623485565, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1355, 'grad_norm': 0.3093962073326111, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9936, 'grad_norm': 0.35658201575279236, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0147, 'grad_norm': 0.3519841432571411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9549, 'grad_norm': 0.36895686388015747, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9765, 'grad_norm': 0.34011754393577576, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.009, 'grad_norm': 0.34997400641441345, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9609, 'grad_norm': 0.3140244483947754, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9476, 'grad_norm': 0.3796195983886719, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9339, 'grad_norm': 0.3468732535839081, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.952, 'grad_norm': 0.3201925456523895, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.8772, 'grad_norm': 0.3055405914783478, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8731, 'grad_norm': 0.29584717750549316, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8292, 'grad_norm': 0.3274056911468506, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7677, 'grad_norm': 0.31807228922843933, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8797, 'grad_norm': 0.3259202837944031, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8626, 'grad_norm': 0.35894858837127686, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8613, 'grad_norm': 0.3046533465385437, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.781, 'grad_norm': 0.3606114983558655, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.83, 'grad_norm': 0.30257540941238403, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6409, 'grad_norm': 0.3422982394695282, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7197, 'grad_norm': 0.3144806921482086, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.671, 'grad_norm': 0.33248984813690186, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7495, 'grad_norm': 0.34157228469848633, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6899, 'grad_norm': 0.30253836512565613, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7077, 'grad_norm': 0.3258510231971741, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6644, 'grad_norm': 0.3498285710811615, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 5.6336, 'grad_norm': 0.341901570558548, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 5.6662, 'grad_norm': 0.33505648374557495, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 5.5652, 'grad_norm': 0.2948814034461975, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 5.6279, 'grad_norm': 0.34074094891548157, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 5.6393, 'grad_norm': 0.313926637172699, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 5.5563, 'grad_norm': 0.3419014811515808, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 5.5615, 'grad_norm': 0.42348378896713257, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 5.6167, 'grad_norm': 0.36434975266456604, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 5.5421, 'grad_norm': 0.2790813148021698, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 5.5693, 'grad_norm': 0.3176637291908264, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 5.5641, 'grad_norm': 0.31082892417907715, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 5.5002, 'grad_norm': 0.3292393982410431, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 5.5474, 'grad_norm': 0.3477751910686493, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 5.4902, 'grad_norm': 0.3017272353172302, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 5.4458, 'grad_norm': 0.544732391834259, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 5.4659, 'grad_norm': 0.3258715271949768, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 5.4492, 'grad_norm': 0.3748248517513275, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 5.4243, 'grad_norm': 0.37313714623451233, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 5.4582, 'grad_norm': 0.3185122311115265, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 5.4027, 'grad_norm': 0.3214268088340759, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 5.4507, 'grad_norm': 0.38999342918395996, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 5.4698, 'grad_norm': 0.30842965841293335, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 5.3942, 'grad_norm': 0.296842485666275, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 5.4003, 'grad_norm': 0.3190489113330841, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 5.3966, 'grad_norm': 0.3261561095714569, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 5.3826, 'grad_norm': 0.3070858120918274, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 5.3638, 'grad_norm': 0.4066886603832245, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 5.3252, 'grad_norm': 0.3182021379470825, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 5.3245, 'grad_norm': 0.34209904074668884, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 5.4032, 'grad_norm': 0.31804507970809937, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 5.3064, 'grad_norm': 0.33731040358543396, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 5.293, 'grad_norm': 0.31189024448394775, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 5.3553, 'grad_norm': 0.3341033458709717, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 5.3405, 'grad_norm': 0.3273971676826477, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 5.2921, 'grad_norm': 0.3570025861263275, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 5.3044, 'grad_norm': 0.8820928931236267, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 5.3403, 'grad_norm': 0.3153234124183655, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 5.3268, 'grad_norm': 0.33260735869407654, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 5.3429, 'grad_norm': 0.33333227038383484, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 5.2959, 'grad_norm': 0.31877583265304565, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 5.274, 'grad_norm': 0.32106199860572815, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 5.2137, 'grad_norm': 0.3771252930164337, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 5.307, 'grad_norm': 0.3325613737106323, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 5.1725, 'grad_norm': 0.317754328250885, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 5.1742, 'grad_norm': 0.29978471994400024, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "  4%|â–ˆâ–Œ                                  | 680/15157 [29:59<10:33:58,  2.63s/it]2025-11-02 19:06:25,953 [INFO] lib.callbacks: ÐŸÑ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ñƒ: 1801.95 c\n",
      "{'train_runtime': 1801.9604, 'train_samples_per_second': 1076.64, 'train_steps_per_second': 8.411, 'train_loss': 6.316314328959685, 'epoch': 0.04}\n",
      "  4%|â–ˆâ–Œ                                  | 681/15157 [30:01<10:38:24,  2.65s/it]\n",
      "2025-11-02 19:06:26,111 [INFO] lib.training: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
      "2025-11-02 19:06:26,134 [INFO] lib.training: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:20<00:00, 30.66it/s]\n",
      "2025-11-02 19:06:48,639 [INFO] lib.training: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ñ‚ÐµÐºÑÑ‚Ð°\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-02 19:06:59,416 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-02 19:06:59,416 [INFO] lib.training: Ð’ Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ»Ð¾Ð²Ð¾, Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð±Ñ‹Ð»Ð¾, Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð² ÐµÐ³Ð¾.\n",
      "\n",
      "ÐšÐ°Ñ€ÑŒÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° \n",
      "\n",
      "Ð’ Ñ„ÐµÐ²Ñ€Ð°Ð»Ðµ 2006 Ð³Ð¾Ð´Ð° Ð±Ñ‹Ð» ÑƒÐ¿Ñ€Ð°Ð·Ð´Ð½Ñ‘Ð½ Ð² Ð³Ð¾Ñ€Ð¾Ð´Ðµ ÐžÐ¾Ð³Ð¸Ð½, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ XX Ð²ÐµÐºÐ°.\n",
      "\n",
      "Ð’ 2013 Ð³Ð¾Ð´Ñƒ Ð¾ÐºÐ¾Ð½Ñ‡Ð¸Ð» Ð’.Â Ð˜.Â Ð§Ð°Ð¹ÑÐºÐ¾Ðµ ÐºÐ»Ð°Ð´Ð±Ð¸Ñ‰Ðµ.\n",
      "\n",
      "Ð’ 1937 Ð³Ð¾Ð´Ñƒ Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½ Ð² ÑÐ¾ÑÑ‚Ð°Ð² ÑÐ±Ð¾Ñ€Ð½Ð¾Ð¹ ÐŸÐ¾Ð»ÑŒÑˆÐ¸.\n",
      "2025-11-02 19:06:59,417 [INFO] __main__: ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾!\n",
      "2025-11-02 19:06:59,417 [INFO] __main__: ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸: {'train': {'train_runtime': 1801.9604, 'train_samples_per_second': 1076.64, 'train_steps_per_second': 8.411, 'total_flos': 2.297433600955515e+17, 'train_loss': 6.316314328959685, 'epoch': 0.044930476520362216}, 'eval': {'eval_loss': 5.872274398803711, 'eval_runtime': 22.525, 'eval_samples_per_second': 221.976, 'eval_steps_per_second': 27.747, 'epoch': 0.044930476520362216}, 'generation': 'Ð’ Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð±Ñ‹Ð»Ð¾ ÑÐ»Ð¾Ð²Ð¾, Ð¸ ÑÐ»Ð¾Ð²Ð¾ Ð±Ñ‹Ð»Ð¾, Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð² ÐµÐ³Ð¾.\\n\\nÐšÐ°Ñ€ÑŒÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° \\n\\nÐ’ Ñ„ÐµÐ²Ñ€Ð°Ð»Ðµ 2006 Ð³Ð¾Ð´Ð° Ð±Ñ‹Ð» ÑƒÐ¿Ñ€Ð°Ð·Ð´Ð½Ñ‘Ð½ Ð² Ð³Ð¾Ñ€Ð¾Ð´Ðµ ÐžÐ¾Ð³Ð¸Ð½, Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð» Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ XX Ð²ÐµÐºÐ°.\\n\\nÐ’ 2013 Ð³Ð¾Ð´Ñƒ Ð¾ÐºÐ¾Ð½Ñ‡Ð¸Ð» Ð’.\\xa0Ð˜.\\xa0Ð§Ð°Ð¹ÑÐºÐ¾Ðµ ÐºÐ»Ð°Ð´Ð±Ð¸Ñ‰Ðµ.\\n\\nÐ’ 1937 Ð³Ð¾Ð´Ñƒ Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½ Ð² ÑÐ¾ÑÑ‚Ð°Ð² ÑÐ±Ð¾Ñ€Ð½Ð¾Ð¹ ÐŸÐ¾Ð»ÑŒÑˆÐ¸.'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ohdroyih\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251102_183618-ohdroyih/logs\u001b[0m\n",
      "[rank0]:[W1102 19:07:02.380080020 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2,3 torchrun --nproc_per_node=2 --master_port=29501 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 2 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"deep_speed_setup_2\"\n",
    "\n",
    "# ~ Ð¿Ð¾ 50Ð³Ð± Ð½Ð° ÐºÐ°Ð¶Ð´Ñ‹Ð¹ GPU, Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð¾Ð»Ð½Ð°Ñ (95-100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29710b33",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e3d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 ../train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"fsdp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ba0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
