{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab4ed44",
   "metadata": {},
   "source": [
    "# ZeRO –∏ FSDP: –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "## __[DeepSpeed ZeRO](https://www.deepspeed.ai/docs/config-json/):__\n",
    "\n",
    "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å data-parallel:  \n",
    "- —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (optimizer states)  \n",
    "- –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (gradients)  \n",
    "- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (parameters)  \n",
    "\n",
    "–≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å/—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "__–£—Ä–æ–≤–Ω–∏ (stage):__\n",
    "\n",
    "| Stage | –ß—Ç–æ —à–∞—Ä–¥–∏—Ç—Å—è | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n",
    "|-------|-------------|-------------|\n",
    "| 0 | –Ω–∏—á–µ–≥–æ (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π data-parallel, —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π) | –æ–±—ã—á–Ω—ã–π DDP |\n",
    "| 1 | optimizer states | –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Å—Ç–∞—é—Ç—Å—è —Ä–µ–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏. –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 0, –Ω–µ–±–æ–ª—å—à–æ–π –Ω–∞–∫–ª–∞–¥ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. |\n",
    "| 2 | optimizer states + gradients | –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å—ë –µ—â—ë —Ä–µ–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã. –ë–æ–ª—å—à–µ —ç–∫–æ–Ω–æ–º–∏—è, –±–æ–ª—å—à–µ –Ω–∞–∫–ª–∞–¥. |\n",
    "| 3 | optimizer states + gradients + parameters | –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –∑–∞ —Å—á—ë—Ç –ø–æ–ª–Ω–æ–π —à–∞—Ä–¥–∏–∑–∞—Ü–∏–∏. –ù–æ —Å–∞–º–∞—è –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. |\n",
    "\n",
    "__DeepSpeed config:__\n",
    "\n",
    "```python\n",
    "zero_optimization = {\n",
    "    \"stage\": 0,                                         # 0|1|2|3 ‚Äî —É—Ä–æ–≤–µ–Ω—å ZeRO, 0 = –æ–±—ã—á–Ω—ã–π DDP\n",
    "    \"allgather_partitions\": True,                       # —Å–æ–±–∏—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ —à–∞–≥–∞ (True ‚Äî –¥–µ—Ñ–æ–ª—Ç)\n",
    "    \"reduce_scatter\": True,                             # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reduce_scatter –≤–º–µ—Å—Ç–æ allreduce\n",
    "    \"allgather_bucket_size\": 5e8,                       # —Ä–∞–∑–º–µ—Ä –±–∞–∫–µ—Ç–∞ (–≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö, –Ω–µ –±–∞–π—Ç–∞—Ö), –¥–µ—Ñ–æ–ª—Ç 500–ú\n",
    "    \"reduce_bucket_size\": 5e8,                          # —Ä–∞–∑–º–µ—Ä –±–∞–∫–µ—Ç–∞ –¥–ª—è reduce-scatter, –¥–µ—Ñ–æ–ª—Ç 500–ú\n",
    "    \"overlap_comm\": False,                              # –¥–µ—Ñ–æ–ª—Ç False; True –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏\n",
    "    \"contiguous_gradients\": True,                       # —Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –±–ª–æ–∫–µ –ø–∞–º—è—Ç–∏\n",
    "    \"offload_optimizer\": {                              # offload –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–∞–∫—Ç—É–∞–ª—å–Ω–æ –ø—Ä–∏ stage >= 2)\n",
    "        \"device\": \"none\",                               # –≤–∞—Ä–∏–∞–Ω—Ç—ã: \"none\" | \"cpu\" | \"nvme\"\n",
    "        \"pin_memory\": False\n",
    "    },\n",
    "    \"offload_param\": {                                  # offload –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∞–∫—Ç—É–∞–ª—å–Ω–æ –ø—Ä–∏ stage == 3)\n",
    "        \"device\": \"none\",                               # \"none\" | \"cpu\" | \"nvme\"\n",
    "        \"pin_memory\": False\n",
    "    },\n",
    "    \"sub_group_size\": 1e9,                              # –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã –ø—Ä–∏ stage3 (–±–∞–π—Ç—ã)\n",
    "    \"stage3_max_live_parameters\": 1e9,                  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏\n",
    "    \"stage3_max_reuse_distance\": 1e9,                   # –º–∞–∫—Å. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    \"stage3_prefetch_bucket_size\": 5e8,                 # —Ä–∞–∑–º–µ—Ä –ø—Ä–µ—Ñ–µ—Ç—á-–±–∞–∫–µ—Ç–∞\n",
    "    \"stage3_param_persistence_threshold\": 1e5,          # –ø–æ—Ä–æ–≥ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ–º—ã—Ö\n",
    "    \"stage3_gather_fp16_weights_on_model_save\": True,   # —Å–æ–±–∏—Ä–∞—Ç—å –≤–µ—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏\n",
    "}\n",
    "```\n",
    "___\n",
    "\n",
    "## [FSDP](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.fsdp) (Fully Sharded Data Parallel)\n",
    "\n",
    "–ú–µ—Ç–æ–¥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤ PyTorch –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É Accelerate / Transformers. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —à–∞—Ä–¥–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏/–∏–ª–∏ optimizer-—Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU.\n",
    "\n",
    "__FSDP config:__\n",
    "\n",
    "```python\n",
    "fsdp_config = {\n",
    "    \"fsdp_sharding_strategy\": \"FULL_SHARD\",             # FULL_SHARD | SHARD_GRAD_OP | NO_SHARD | HYBRID_SHARD | HYBRID_SHARD_ZERO2\n",
    "    \"fsdp_offload_params\": False,                       # False = –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è –Ω–∞ CPU\n",
    "    \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",  # TRANSFORMER_BASED_WRAP | SIZE_BASED_WRAP | NO_WRAP\n",
    "    \"fsdp_transformer_layer_cls_to_wrap\": None,         # —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ —Å–ª–æ—ë–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä [\"BertLayer\"]); None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "    \"fsdp_backward_prefetch_policy\": \"BACKWARD_POST\",   # BACKWARD_PRE | BACKWARD_POST | NO_PREFETCH\n",
    "    \"fsdp_forward_prefetch\": False,                     # False = –Ω–µ –ø—Ä–µ—Ñ–µ—Ç—á–∏—Ç—å –≤–ø–µ—Ä—ë–¥\n",
    "    \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",       # FULL_STATE_DICT | SHARDED_STATE_DICT | LOCAL_STATE_DICT (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ SHARDED)\n",
    "    \"fsdp_cpu_ram_efficient_loading\": True,             # True = –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ RAM\n",
    "    \"fsdp_min_num_params\": 1e8,                         # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è SIZE_BASED_WRAP (100 –º–ª–Ω)\n",
    "    \"fsdp_sync_module_states\": True,                    # —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    \"fsdp_use_orig_params\": True,                       # True = —Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "    \"fsdp_activation_checkpointing\": False,             # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ checkpointing –∞–∫—Ç–∏–≤–∞—Ü–∏–π (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "[link](https://huggingface.co/docs/accelerate/en/usage_guides/fsdp?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e2d8f",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcc33a",
   "metadata": {},
   "source": [
    "___\n",
    "### Baseline_1_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: baseline\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: World size: 1\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: CUDA_VISIBLE_DEVICES: 1\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ baseline setup (single GPU)\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:35:16,757 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_173519-sd6qsqe4\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_Baseline_1_GPU\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/sd6qsqe4\n",
      "2025-11-03 17:35:21,167 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:35:21,167 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:35:21,168 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:35:21,168 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/30314 [00:00<?, ?it/s]W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.8721, 'grad_norm': 40.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 10.3303, 'grad_norm': 3.21875, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.7224, 'grad_norm': 4.1875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.5379, 'grad_norm': 1.8203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.41, 'grad_norm': 1.375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2709, 'grad_norm': 1.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.215, 'grad_norm': 1.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.08, 'grad_norm': 1.1953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0479, 'grad_norm': 0.7578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8773, 'grad_norm': 0.703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8597, 'grad_norm': 0.8515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7418, 'grad_norm': 0.94140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5995, 'grad_norm': 1.109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4574, 'grad_norm': 1.2578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3744, 'grad_norm': 0.96484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2797, 'grad_norm': 0.8125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2186, 'grad_norm': 1.0859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.121, 'grad_norm': 0.75, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0157, 'grad_norm': 0.84375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.7949, 'grad_norm': 0.87890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6535, 'grad_norm': 0.7109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6468, 'grad_norm': 0.75390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.455, 'grad_norm': 0.796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.4746, 'grad_norm': 0.76953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2777, 'grad_norm': 0.8515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2574, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1485, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1064, 'grad_norm': 0.796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1286, 'grad_norm': 0.6953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.9833, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.9077, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9811, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9083, 'grad_norm': 0.71875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7807, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8899, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7069, 'grad_norm': 0.68359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7366, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5773, 'grad_norm': 0.62890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5452, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5706, 'grad_norm': 0.5546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.455, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4878, 'grad_norm': 0.6796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4999, 'grad_norm': 0.69140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4484, 'grad_norm': 0.54296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3111, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4163, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4154, 'grad_norm': 0.640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3586, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.292, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1492, 'grad_norm': 0.52734375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.2227, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0989, 'grad_norm': 0.5234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3193, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1377, 'grad_norm': 0.6015625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1691, 'grad_norm': 0.69140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1182, 'grad_norm': 0.56640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0525, 'grad_norm': 0.55859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0464, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.9923, 'grad_norm': 0.48046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0926, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.041, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8398, 'grad_norm': 0.484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0249, 'grad_norm': 0.65234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0166, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8451, 'grad_norm': 0.58984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8894, 'grad_norm': 0.498046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.772, 'grad_norm': 0.58984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8341, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8778, 'grad_norm': 0.5859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8943, 'grad_norm': 0.62890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8777, 'grad_norm': 0.484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7398, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7571, 'grad_norm': 0.6484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.721, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6558, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6991, 'grad_norm': 0.486328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6919, 'grad_norm': 0.4921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6391, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8391, 'grad_norm': 0.458984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7064, 'grad_norm': 0.4921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7305, 'grad_norm': 0.5, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7011, 'grad_norm': 0.5078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5659, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5317, 'grad_norm': 0.48828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5267, 'grad_norm': 0.474609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5892, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.4835, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.511, 'grad_norm': 0.5546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6248, 'grad_norm': 0.49609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5176, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.3683, 'grad_norm': 0.49609375, 'learning_rate': 0.000698374193548387, 'epoch': 0.02}\n",
      "{'loss': 4.4851, 'grad_norm': 0.55078125, 'learning_rate': 0.0006963419354838709, 'epoch': 0.02}\n",
      "{'loss': 4.5789, 'grad_norm': 0.4765625, 'learning_rate': 0.0006943096774193547, 'epoch': 0.02}\n",
      "{'loss': 4.5082, 'grad_norm': 0.5390625, 'learning_rate': 0.0006922774193548388, 'epoch': 0.02}\n",
      "{'loss': 4.383, 'grad_norm': 0.515625, 'learning_rate': 0.0006902451612903226, 'epoch': 0.02}\n",
      "{'loss': 4.5932, 'grad_norm': 0.4921875, 'learning_rate': 0.0006882129032258064, 'epoch': 0.02}\n",
      "{'loss': 4.548, 'grad_norm': 0.59375, 'learning_rate': 0.0006861806451612903, 'epoch': 0.02}\n",
      "{'loss': 4.3619, 'grad_norm': 0.51953125, 'learning_rate': 0.0006841483870967741, 'epoch': 0.02}\n",
      "{'loss': 4.4733, 'grad_norm': 0.453125, 'learning_rate': 0.0006821161290322579, 'epoch': 0.02}\n",
      "{'loss': 4.3207, 'grad_norm': 0.52734375, 'learning_rate': 0.0006800838709677419, 'epoch': 0.02}\n",
      "{'loss': 4.365, 'grad_norm': 0.466796875, 'learning_rate': 0.0006780516129032257, 'epoch': 0.02}\n",
      "{'loss': 4.3066, 'grad_norm': 0.515625, 'learning_rate': 0.0006760193548387097, 'epoch': 0.02}\n",
      "{'loss': 4.4256, 'grad_norm': 0.46484375, 'learning_rate': 0.0006739870967741935, 'epoch': 0.02}\n",
      "{'loss': 4.4189, 'grad_norm': 0.462890625, 'learning_rate': 0.0006719548387096773, 'epoch': 0.02}\n",
      "{'loss': 4.3297, 'grad_norm': 0.451171875, 'learning_rate': 0.0006699225806451613, 'epoch': 0.02}\n",
      "{'loss': 4.3721, 'grad_norm': 0.439453125, 'learning_rate': 0.0006678903225806451, 'epoch': 0.02}\n",
      "{'loss': 4.454, 'grad_norm': 0.462890625, 'learning_rate': 0.000665858064516129, 'epoch': 0.02}\n",
      "{'loss': 4.2769, 'grad_norm': 0.453125, 'learning_rate': 0.0006638258064516128, 'epoch': 0.02}\n",
      "{'loss': 4.2232, 'grad_norm': 0.5, 'learning_rate': 0.0006617935483870967, 'epoch': 0.02}\n",
      "{'loss': 4.3592, 'grad_norm': 0.4453125, 'learning_rate': 0.0006597612903225807, 'epoch': 0.02}\n",
      "{'loss': 4.3141, 'grad_norm': 0.5, 'learning_rate': 0.0006577290322580645, 'epoch': 0.02}\n",
      "{'loss': 4.3016, 'grad_norm': 0.458984375, 'learning_rate': 0.0006556967741935483, 'epoch': 0.02}\n",
      "{'loss': 4.1957, 'grad_norm': 0.47265625, 'learning_rate': 0.0006536645161290322, 'epoch': 0.02}\n",
      "{'loss': 4.1466, 'grad_norm': 0.439453125, 'learning_rate': 0.000651632258064516, 'epoch': 0.02}\n",
      "{'loss': 4.2662, 'grad_norm': 0.462890625, 'learning_rate': 0.0006495999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.2014, 'grad_norm': 0.4921875, 'learning_rate': 0.0006475677419354838, 'epoch': 0.02}\n",
      "{'loss': 4.3977, 'grad_norm': 0.458984375, 'learning_rate': 0.0006455354838709677, 'epoch': 0.02}\n",
      "{'loss': 4.2665, 'grad_norm': 0.46875, 'learning_rate': 0.0006435032258064516, 'epoch': 0.02}\n",
      "{'loss': 4.2421, 'grad_norm': 0.462890625, 'learning_rate': 0.0006414709677419354, 'epoch': 0.02}\n",
      "{'loss': 4.2164, 'grad_norm': 0.55078125, 'learning_rate': 0.0006394387096774192, 'epoch': 0.02}\n",
      "{'loss': 4.2853, 'grad_norm': 0.5390625, 'learning_rate': 0.0006374064516129032, 'epoch': 0.02}\n",
      "{'loss': 4.2224, 'grad_norm': 0.53125, 'learning_rate': 0.000635374193548387, 'epoch': 0.02}\n",
      "{'loss': 4.2244, 'grad_norm': 0.44921875, 'learning_rate': 0.0006333419354838709, 'epoch': 0.02}\n",
      "{'loss': 4.2336, 'grad_norm': 0.52734375, 'learning_rate': 0.0006313096774193548, 'epoch': 0.02}\n",
      "{'loss': 4.2468, 'grad_norm': 0.419921875, 'learning_rate': 0.0006292774193548386, 'epoch': 0.02}\n",
      "{'loss': 4.1884, 'grad_norm': 0.482421875, 'learning_rate': 0.0006272451612903226, 'epoch': 0.02}\n",
      "{'loss': 4.1778, 'grad_norm': 0.47265625, 'learning_rate': 0.0006252129032258064, 'epoch': 0.02}\n",
      "{'loss': 4.1299, 'grad_norm': 0.4375, 'learning_rate': 0.0006231806451612903, 'epoch': 0.02}\n",
      "{'loss': 4.0729, 'grad_norm': 0.5078125, 'learning_rate': 0.0006211483870967741, 'epoch': 0.02}\n",
      "{'loss': 4.1565, 'grad_norm': 0.419921875, 'learning_rate': 0.0006191161290322579, 'epoch': 0.02}\n",
      "{'loss': 4.167, 'grad_norm': 0.421875, 'learning_rate': 0.000617083870967742, 'epoch': 0.02}\n",
      "{'loss': 4.166, 'grad_norm': 0.439453125, 'learning_rate': 0.0006150516129032258, 'epoch': 0.02}\n",
      "{'loss': 4.2098, 'grad_norm': 0.431640625, 'learning_rate': 0.0006130193548387097, 'epoch': 0.02}\n",
      "{'loss': 3.9133, 'grad_norm': 0.482421875, 'learning_rate': 0.0006109870967741935, 'epoch': 0.02}\n",
      "{'loss': 4.1249, 'grad_norm': 0.470703125, 'learning_rate': 0.0006089548387096773, 'epoch': 0.02}\n",
      "{'loss': 3.9969, 'grad_norm': 0.470703125, 'learning_rate': 0.0006069225806451612, 'epoch': 0.02}\n",
      "{'loss': 4.033, 'grad_norm': 0.50390625, 'learning_rate': 0.0006048903225806451, 'epoch': 0.02}\n",
      "{'loss': 3.9884, 'grad_norm': 0.4140625, 'learning_rate': 0.0006028580645161289, 'epoch': 0.02}\n",
      "{'loss': 4.0232, 'grad_norm': 0.42578125, 'learning_rate': 0.0006008258064516129, 'epoch': 0.02}\n",
      "{'loss': 4.0789, 'grad_norm': 0.408203125, 'learning_rate': 0.0005987935483870967, 'epoch': 0.02}\n",
      "{'loss': 4.0805, 'grad_norm': 0.435546875, 'learning_rate': 0.0005967612903225807, 'epoch': 0.02}\n",
      "{'loss': 4.0462, 'grad_norm': 0.4140625, 'learning_rate': 0.0005947290322580645, 'epoch': 0.02}\n",
      "{'loss': 3.9913, 'grad_norm': 0.41796875, 'learning_rate': 0.0005926967741935483, 'epoch': 0.02}\n",
      "{'loss': 4.0652, 'grad_norm': 0.43359375, 'learning_rate': 0.0005906645161290322, 'epoch': 0.02}\n",
      "{'loss': 4.0017, 'grad_norm': 0.435546875, 'learning_rate': 0.000588632258064516, 'epoch': 0.02}\n",
      "{'loss': 4.0467, 'grad_norm': 0.4375, 'learning_rate': 0.0005866000000000001, 'epoch': 0.02}\n",
      "{'loss': 3.9504, 'grad_norm': 0.427734375, 'learning_rate': 0.0005845677419354839, 'epoch': 0.02}\n",
      "{'loss': 4.1199, 'grad_norm': 0.4453125, 'learning_rate': 0.0005825354838709677, 'epoch': 0.02}\n",
      "{'loss': 4.044, 'grad_norm': 0.4375, 'learning_rate': 0.0005805032258064516, 'epoch': 0.02}\n",
      "{'loss': 4.0121, 'grad_norm': 0.46484375, 'learning_rate': 0.0005784709677419354, 'epoch': 0.02}\n",
      "{'loss': 3.9407, 'grad_norm': 0.41796875, 'learning_rate': 0.0005764387096774192, 'epoch': 0.02}\n",
      "{'loss': 3.9057, 'grad_norm': 0.40234375, 'learning_rate': 0.0005744064516129033, 'epoch': 0.03}\n",
      "{'loss': 3.8537, 'grad_norm': 0.5234375, 'learning_rate': 0.000572374193548387, 'epoch': 0.03}\n",
      "{'loss': 3.9901, 'grad_norm': 0.44921875, 'learning_rate': 0.000570341935483871, 'epoch': 0.03}\n",
      "{'loss': 3.9606, 'grad_norm': 0.447265625, 'learning_rate': 0.0005683096774193548, 'epoch': 0.03}\n",
      "{'loss': 3.8117, 'grad_norm': 0.421875, 'learning_rate': 0.0005662774193548386, 'epoch': 0.03}\n",
      "{'loss': 3.872, 'grad_norm': 0.462890625, 'learning_rate': 0.0005642451612903226, 'epoch': 0.03}\n",
      "{'loss': 3.8629, 'grad_norm': 0.408203125, 'learning_rate': 0.0005622129032258064, 'epoch': 0.03}\n",
      "{'loss': 3.8546, 'grad_norm': 0.44140625, 'learning_rate': 0.0005601806451612902, 'epoch': 0.03}\n",
      "{'loss': 3.9864, 'grad_norm': 0.4296875, 'learning_rate': 0.0005581483870967742, 'epoch': 0.03}\n",
      "{'loss': 3.9252, 'grad_norm': 0.4453125, 'learning_rate': 0.000556116129032258, 'epoch': 0.03}\n",
      "{'loss': 3.9008, 'grad_norm': 0.466796875, 'learning_rate': 0.000554083870967742, 'epoch': 0.03}\n",
      "{'loss': 3.9787, 'grad_norm': 0.4453125, 'learning_rate': 0.0005520516129032258, 'epoch': 0.03}\n",
      "{'loss': 3.853, 'grad_norm': 0.412109375, 'learning_rate': 0.0005500193548387096, 'epoch': 0.03}\n",
      "{'loss': 3.8547, 'grad_norm': 0.4140625, 'learning_rate': 0.0005479870967741935, 'epoch': 0.03}\n",
      "{'loss': 3.834, 'grad_norm': 0.43359375, 'learning_rate': 0.0005459548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.8953, 'grad_norm': 0.4140625, 'learning_rate': 0.0005439225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.9946, 'grad_norm': 0.408203125, 'learning_rate': 0.0005418903225806451, 'epoch': 0.03}\n",
      "{'loss': 3.6884, 'grad_norm': 0.44140625, 'learning_rate': 0.0005398580645161289, 'epoch': 0.03}\n",
      "{'loss': 3.7325, 'grad_norm': 0.384765625, 'learning_rate': 0.0005378258064516129, 'epoch': 0.03}\n",
      "{'loss': 3.779, 'grad_norm': 0.38671875, 'learning_rate': 0.0005357935483870967, 'epoch': 0.03}\n",
      "{'loss': 3.7886, 'grad_norm': 0.400390625, 'learning_rate': 0.0005337612903225805, 'epoch': 0.03}\n",
      "{'loss': 3.7665, 'grad_norm': 0.4296875, 'learning_rate': 0.0005317290322580645, 'epoch': 0.03}\n",
      "{'loss': 3.7348, 'grad_norm': 0.431640625, 'learning_rate': 0.0005296967741935483, 'epoch': 0.03}\n",
      "{'loss': 3.7802, 'grad_norm': 0.416015625, 'learning_rate': 0.0005276645161290323, 'epoch': 0.03}\n",
      "{'loss': 3.797, 'grad_norm': 0.40234375, 'learning_rate': 0.0005256322580645161, 'epoch': 0.03}\n",
      "{'loss': 3.7704, 'grad_norm': 0.455078125, 'learning_rate': 0.0005235999999999999, 'epoch': 0.03}\n",
      "{'loss': 3.7399, 'grad_norm': 0.427734375, 'learning_rate': 0.0005215677419354839, 'epoch': 0.03}\n",
      "{'loss': 3.7339, 'grad_norm': 0.412109375, 'learning_rate': 0.0005195354838709677, 'epoch': 0.03}\n",
      "{'loss': 3.7947, 'grad_norm': 0.41015625, 'learning_rate': 0.0005175032258064515, 'epoch': 0.03}\n",
      "{'loss': 3.8254, 'grad_norm': 0.455078125, 'learning_rate': 0.0005154709677419354, 'epoch': 0.03}\n",
      "{'loss': 3.7258, 'grad_norm': 0.40234375, 'learning_rate': 0.0005134387096774193, 'epoch': 0.03}\n",
      "{'loss': 3.7509, 'grad_norm': 0.400390625, 'learning_rate': 0.0005114064516129032, 'epoch': 0.03}\n",
      "{'loss': 3.677, 'grad_norm': 0.42578125, 'learning_rate': 0.0005093741935483871, 'epoch': 0.03}\n",
      "{'loss': 3.7139, 'grad_norm': 0.376953125, 'learning_rate': 0.0005073419354838709, 'epoch': 0.03}\n",
      "{'loss': 3.7375, 'grad_norm': 0.41015625, 'learning_rate': 0.0005053096774193548, 'epoch': 0.03}\n",
      "{'loss': 3.6206, 'grad_norm': 0.42578125, 'learning_rate': 0.0005032774193548386, 'epoch': 0.03}\n",
      "{'loss': 3.6434, 'grad_norm': 0.384765625, 'learning_rate': 0.0005012451612903226, 'epoch': 0.03}\n",
      "{'loss': 3.7604, 'grad_norm': 0.416015625, 'learning_rate': 0.0004992129032258064, 'epoch': 0.03}\n",
      "{'loss': 3.659, 'grad_norm': 0.421875, 'learning_rate': 0.0004971806451612902, 'epoch': 0.03}\n",
      "{'loss': 3.7299, 'grad_norm': 0.416015625, 'learning_rate': 0.0004951483870967742, 'epoch': 0.03}\n",
      "{'loss': 3.7111, 'grad_norm': 0.41015625, 'learning_rate': 0.000493116129032258, 'epoch': 0.03}\n",
      "{'loss': 3.5974, 'grad_norm': 0.38671875, 'learning_rate': 0.000491083870967742, 'epoch': 0.03}\n",
      "{'loss': 3.7257, 'grad_norm': 0.37109375, 'learning_rate': 0.0004890516129032258, 'epoch': 0.03}\n",
      "{'loss': 3.6866, 'grad_norm': 0.37890625, 'learning_rate': 0.0004870193548387096, 'epoch': 0.03}\n",
      "{'loss': 3.6072, 'grad_norm': 0.423828125, 'learning_rate': 0.00048498709677419346, 'epoch': 0.03}\n",
      "{'loss': 3.6581, 'grad_norm': 0.40234375, 'learning_rate': 0.0004829548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.7355, 'grad_norm': 0.37890625, 'learning_rate': 0.0004809225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.5404, 'grad_norm': 0.38671875, 'learning_rate': 0.00047889032258064513, 'epoch': 0.03}\n",
      "{'loss': 3.6842, 'grad_norm': 0.419921875, 'learning_rate': 0.000476858064516129, 'epoch': 0.03}\n",
      "{'loss': 3.5375, 'grad_norm': 0.396484375, 'learning_rate': 0.00047482580645161285, 'epoch': 0.03}\n",
      "{'loss': 3.7285, 'grad_norm': 0.400390625, 'learning_rate': 0.0004727935483870967, 'epoch': 0.03}\n",
      "{'loss': 3.6084, 'grad_norm': 0.41015625, 'learning_rate': 0.0004707612903225806, 'epoch': 0.03}\n",
      "{'loss': 3.6129, 'grad_norm': 0.3984375, 'learning_rate': 0.0004687290322580644, 'epoch': 0.03}\n",
      "{'loss': 3.5092, 'grad_norm': 0.43359375, 'learning_rate': 0.0004666967741935484, 'epoch': 0.03}\n",
      "{'loss': 3.5876, 'grad_norm': 0.41015625, 'learning_rate': 0.00046466451612903225, 'epoch': 0.03}\n",
      "{'loss': 3.5284, 'grad_norm': 0.390625, 'learning_rate': 0.0004626322580645161, 'epoch': 0.03}\n",
      "{'loss': 3.638, 'grad_norm': 0.384765625, 'learning_rate': 0.0004606, 'epoch': 0.03}\n",
      "{'loss': 3.5263, 'grad_norm': 0.37890625, 'learning_rate': 0.0004585677419354838, 'epoch': 0.03}\n",
      "{'loss': 3.5353, 'grad_norm': 0.400390625, 'learning_rate': 0.0004565354838709677, 'epoch': 0.03}\n",
      "{'loss': 3.5116, 'grad_norm': 0.38671875, 'learning_rate': 0.0004545032258064516, 'epoch': 0.03}\n",
      "{'loss': 3.4871, 'grad_norm': 0.4375, 'learning_rate': 0.0004524709677419354, 'epoch': 0.03}\n",
      "{'loss': 3.4933, 'grad_norm': 0.365234375, 'learning_rate': 0.00045043870967741937, 'epoch': 0.04}\n",
      "{'loss': 3.5265, 'grad_norm': 0.40625, 'learning_rate': 0.0004484064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.5162, 'grad_norm': 0.396484375, 'learning_rate': 0.0004463741935483871, 'epoch': 0.04}\n",
      "{'loss': 3.5155, 'grad_norm': 0.388671875, 'learning_rate': 0.00044434193548387093, 'epoch': 0.04}\n",
      "{'loss': 3.4806, 'grad_norm': 0.359375, 'learning_rate': 0.00044230967741935477, 'epoch': 0.04}\n",
      "{'loss': 3.5072, 'grad_norm': 0.3828125, 'learning_rate': 0.0004402774193548387, 'epoch': 0.04}\n",
      "{'loss': 3.4144, 'grad_norm': 0.3671875, 'learning_rate': 0.00043824516129032254, 'epoch': 0.04}\n",
      "{'loss': 3.5585, 'grad_norm': 0.376953125, 'learning_rate': 0.0004362129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.5678, 'grad_norm': 0.3671875, 'learning_rate': 0.0004341806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.358, 'grad_norm': 0.427734375, 'learning_rate': 0.00043214838709677416, 'epoch': 0.04}\n",
      "{'loss': 3.5108, 'grad_norm': 0.419921875, 'learning_rate': 0.00043011612903225805, 'epoch': 0.04}\n",
      "{'loss': 3.4789, 'grad_norm': 0.4140625, 'learning_rate': 0.0004280838709677419, 'epoch': 0.04}\n",
      "{'loss': 3.558, 'grad_norm': 0.388671875, 'learning_rate': 0.0004260516129032258, 'epoch': 0.04}\n",
      "{'loss': 3.4933, 'grad_norm': 0.373046875, 'learning_rate': 0.00042401935483870966, 'epoch': 0.04}\n",
      "{'loss': 3.4828, 'grad_norm': 0.376953125, 'learning_rate': 0.0004219870967741935, 'epoch': 0.04}\n",
      "{'loss': 3.4499, 'grad_norm': 0.400390625, 'learning_rate': 0.00041995483870967734, 'epoch': 0.04}\n",
      "{'loss': 3.4912, 'grad_norm': 0.35546875, 'learning_rate': 0.0004179225806451613, 'epoch': 0.04}\n",
      "{'loss': 3.4443, 'grad_norm': 0.3671875, 'learning_rate': 0.0004158903225806451, 'epoch': 0.04}\n",
      "{'loss': 3.4784, 'grad_norm': 0.388671875, 'learning_rate': 0.000413858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.4652, 'grad_norm': 0.376953125, 'learning_rate': 0.0004118258064516129, 'epoch': 0.04}\n",
      "{'loss': 3.5129, 'grad_norm': 0.369140625, 'learning_rate': 0.00040979354838709673, 'epoch': 0.04}\n",
      "{'loss': 3.3576, 'grad_norm': 0.384765625, 'learning_rate': 0.0004077612903225806, 'epoch': 0.04}\n",
      "{'loss': 3.4001, 'grad_norm': 0.357421875, 'learning_rate': 0.00040572903225806446, 'epoch': 0.04}\n",
      "{'loss': 3.4312, 'grad_norm': 0.39453125, 'learning_rate': 0.00040369677419354835, 'epoch': 0.04}\n",
      "{'loss': 3.4591, 'grad_norm': 0.369140625, 'learning_rate': 0.00040166451612903224, 'epoch': 0.04}\n",
      "{'loss': 3.3329, 'grad_norm': 0.40625, 'learning_rate': 0.00039963225806451607, 'epoch': 0.04}\n",
      "{'loss': 3.3461, 'grad_norm': 0.3515625, 'learning_rate': 0.00039759999999999996, 'epoch': 0.04}\n",
      "{'loss': 3.3844, 'grad_norm': 0.412109375, 'learning_rate': 0.00039556774193548385, 'epoch': 0.04}\n",
      "{'loss': 3.4108, 'grad_norm': 0.369140625, 'learning_rate': 0.00039353548387096774, 'epoch': 0.04}\n",
      "{'loss': 3.5249, 'grad_norm': 0.365234375, 'learning_rate': 0.0003915032258064516, 'epoch': 0.04}\n",
      "{'loss': 3.3536, 'grad_norm': 0.443359375, 'learning_rate': 0.0003894709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.3982, 'grad_norm': 0.443359375, 'learning_rate': 0.0003874387096774193, 'epoch': 0.04}\n",
      "{'loss': 3.327, 'grad_norm': 0.3671875, 'learning_rate': 0.0003854064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.3699, 'grad_norm': 0.384765625, 'learning_rate': 0.000383374193548387, 'epoch': 0.04}\n",
      "{'loss': 3.3567, 'grad_norm': 0.35546875, 'learning_rate': 0.0003813419354838709, 'epoch': 0.04}\n",
      "{'loss': 3.4795, 'grad_norm': 0.3515625, 'learning_rate': 0.0003793096774193548, 'epoch': 0.04}\n",
      "{'loss': 3.4111, 'grad_norm': 0.34765625, 'learning_rate': 0.00037727741935483875, 'epoch': 0.04}\n",
      "{'loss': 3.3832, 'grad_norm': 0.37109375, 'learning_rate': 0.00037524516129032253, 'epoch': 0.04}\n",
      "{'loss': 3.356, 'grad_norm': 0.3515625, 'learning_rate': 0.0003732129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.3264, 'grad_norm': 0.37890625, 'learning_rate': 0.0003711806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.3712, 'grad_norm': 0.376953125, 'learning_rate': 0.00036914838709677415, 'epoch': 0.04}\n",
      "{'loss': 3.3841, 'grad_norm': 0.52734375, 'learning_rate': 0.000367116129032258, 'epoch': 0.04}\n",
      "{'loss': 3.3686, 'grad_norm': 0.38671875, 'learning_rate': 0.0003650838709677419, 'epoch': 0.04}\n",
      "{'loss': 3.4301, 'grad_norm': 0.373046875, 'learning_rate': 0.00036305161290322576, 'epoch': 0.04}\n",
      "{'loss': 3.3936, 'grad_norm': 0.375, 'learning_rate': 0.00036101935483870965, 'epoch': 0.04}\n",
      "{'loss': 3.3648, 'grad_norm': 0.376953125, 'learning_rate': 0.00035898709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.4222, 'grad_norm': 0.37109375, 'learning_rate': 0.0003569548387096773, 'epoch': 0.04}\n",
      "{'loss': 3.3901, 'grad_norm': 0.3671875, 'learning_rate': 0.00035492258064516127, 'epoch': 0.04}\n",
      "{'loss': 3.3875, 'grad_norm': 0.345703125, 'learning_rate': 0.0003528903225806451, 'epoch': 0.04}\n",
      "{'loss': 3.3992, 'grad_norm': 0.353515625, 'learning_rate': 0.000350858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.335, 'grad_norm': 0.357421875, 'learning_rate': 0.0003488258064516129, 'epoch': 0.04}\n",
      "{'loss': 3.357, 'grad_norm': 0.380859375, 'learning_rate': 0.0003467935483870967, 'epoch': 0.04}\n",
      "{'loss': 3.3721, 'grad_norm': 0.36328125, 'learning_rate': 0.00034476129032258066, 'epoch': 0.04}\n",
      "{'loss': 3.2303, 'grad_norm': 0.37109375, 'learning_rate': 0.00034272903225806444, 'epoch': 0.04}\n",
      "{'loss': 3.3624, 'grad_norm': 0.357421875, 'learning_rate': 0.0003406967741935484, 'epoch': 0.04}\n",
      "{'loss': 3.4316, 'grad_norm': 0.365234375, 'learning_rate': 0.0003386645161290322, 'epoch': 0.04}\n",
      "{'loss': 3.3784, 'grad_norm': 0.375, 'learning_rate': 0.00033663225806451606, 'epoch': 0.04}\n",
      "{'loss': 3.1377, 'grad_norm': 0.36328125, 'learning_rate': 0.0003346, 'epoch': 0.04}\n",
      "{'loss': 3.1974, 'grad_norm': 0.3671875, 'learning_rate': 0.00033256774193548384, 'epoch': 0.04}\n",
      "{'loss': 3.3239, 'grad_norm': 0.35546875, 'learning_rate': 0.0003305354838709678, 'epoch': 0.04}\n",
      "{'loss': 3.3289, 'grad_norm': 0.37890625, 'learning_rate': 0.00032850322580645156, 'epoch': 0.05}\n",
      "{'loss': 3.2574, 'grad_norm': 0.375, 'learning_rate': 0.00032647096774193545, 'epoch': 0.05}\n",
      "{'loss': 3.318, 'grad_norm': 0.337890625, 'learning_rate': 0.0003244387096774194, 'epoch': 0.05}\n",
      "{'loss': 3.3911, 'grad_norm': 0.3515625, 'learning_rate': 0.0003224064516129032, 'epoch': 0.05}\n",
      "{'loss': 3.3898, 'grad_norm': 0.375, 'learning_rate': 0.00032037419354838707, 'epoch': 0.05}\n",
      "{'loss': 3.3202, 'grad_norm': 0.369140625, 'learning_rate': 0.00031834193548387096, 'epoch': 0.05}\n",
      "{'loss': 3.2586, 'grad_norm': 0.3515625, 'learning_rate': 0.0003163096774193548, 'epoch': 0.05}\n",
      "{'loss': 3.2599, 'grad_norm': 0.400390625, 'learning_rate': 0.0003142774193548387, 'epoch': 0.05}\n",
      "{'loss': 3.3186, 'grad_norm': 0.3515625, 'learning_rate': 0.00031224516129032257, 'epoch': 0.05}\n",
      "{'loss': 3.2553, 'grad_norm': 0.33984375, 'learning_rate': 0.0003102129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.3423, 'grad_norm': 0.35546875, 'learning_rate': 0.0003081806451612903, 'epoch': 0.05}\n",
      "{'loss': 3.4161, 'grad_norm': 0.40234375, 'learning_rate': 0.0003061483870967742, 'epoch': 0.05}\n",
      "{'loss': 3.2174, 'grad_norm': 0.3984375, 'learning_rate': 0.00030411612903225797, 'epoch': 0.05}\n",
      "{'loss': 3.365, 'grad_norm': 0.353515625, 'learning_rate': 0.0003020838709677419, 'epoch': 0.05}\n",
      "{'loss': 3.2268, 'grad_norm': 0.359375, 'learning_rate': 0.00030005161290322575, 'epoch': 0.05}\n",
      "{'loss': 3.2756, 'grad_norm': 0.353515625, 'learning_rate': 0.0002980193548387097, 'epoch': 0.05}\n",
      "{'loss': 3.1288, 'grad_norm': 0.357421875, 'learning_rate': 0.00029598709677419353, 'epoch': 0.05}\n",
      "{'loss': 3.281, 'grad_norm': 0.345703125, 'learning_rate': 0.00029395483870967736, 'epoch': 0.05}\n",
      "{'loss': 3.1725, 'grad_norm': 0.341796875, 'learning_rate': 0.0002919225806451613, 'epoch': 0.05}\n",
      "{'loss': 3.2327, 'grad_norm': 0.34375, 'learning_rate': 0.0002898903225806451, 'epoch': 0.05}\n",
      "{'loss': 3.2344, 'grad_norm': 0.3671875, 'learning_rate': 0.00028785806451612903, 'epoch': 0.05}\n",
      "{'loss': 3.249, 'grad_norm': 0.328125, 'learning_rate': 0.00028582580645161287, 'epoch': 0.05}\n",
      "{'loss': 3.2599, 'grad_norm': 0.37890625, 'learning_rate': 0.00028379354838709676, 'epoch': 0.05}\n",
      "{'loss': 3.304, 'grad_norm': 0.337890625, 'learning_rate': 0.00028176129032258065, 'epoch': 0.05}\n",
      "{'loss': 3.1906, 'grad_norm': 0.33984375, 'learning_rate': 0.0002797290322580645, 'epoch': 0.05}\n",
      "{'loss': 3.3135, 'grad_norm': 0.3515625, 'learning_rate': 0.00027769677419354843, 'epoch': 0.05}\n",
      "{'loss': 3.0904, 'grad_norm': 0.359375, 'learning_rate': 0.0002756645161290322, 'epoch': 0.05}\n",
      "{'loss': 3.1944, 'grad_norm': 0.353515625, 'learning_rate': 0.0002736322580645161, 'epoch': 0.05}\n",
      "{'loss': 3.2556, 'grad_norm': 0.37109375, 'learning_rate': 0.0002716, 'epoch': 0.05}\n",
      "{'loss': 3.1987, 'grad_norm': 0.3359375, 'learning_rate': 0.0002695677419354838, 'epoch': 0.05}\n",
      "{'loss': 3.2256, 'grad_norm': 0.337890625, 'learning_rate': 0.0002675354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.2466, 'grad_norm': 0.353515625, 'learning_rate': 0.0002655032258064516, 'epoch': 0.05}\n",
      "{'loss': 3.2351, 'grad_norm': 0.376953125, 'learning_rate': 0.00026347096774193544, 'epoch': 0.05}\n",
      "{'loss': 3.1488, 'grad_norm': 0.3515625, 'learning_rate': 0.00026143870967741933, 'epoch': 0.05}\n",
      "{'loss': 3.2345, 'grad_norm': 0.41015625, 'learning_rate': 0.0002594064516129032, 'epoch': 0.05}\n",
      "{'loss': 3.2462, 'grad_norm': 0.34375, 'learning_rate': 0.000257374193548387, 'epoch': 0.05}\n",
      "{'loss': 3.1951, 'grad_norm': 0.337890625, 'learning_rate': 0.00025534193548387094, 'epoch': 0.05}\n",
      "{'loss': 3.163, 'grad_norm': 0.361328125, 'learning_rate': 0.00025330967741935483, 'epoch': 0.05}\n",
      "{'loss': 3.1352, 'grad_norm': 0.369140625, 'learning_rate': 0.0002512774193548387, 'epoch': 0.05}\n",
      "{'loss': 3.2105, 'grad_norm': 0.33203125, 'learning_rate': 0.00024924516129032256, 'epoch': 0.05}\n",
      "{'loss': 3.1889, 'grad_norm': 0.349609375, 'learning_rate': 0.0002472129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.2055, 'grad_norm': 0.353515625, 'learning_rate': 0.00024518064516129034, 'epoch': 0.05}\n",
      "{'loss': 3.1189, 'grad_norm': 0.3515625, 'learning_rate': 0.00024314838709677412, 'epoch': 0.05}\n",
      "{'loss': 3.1413, 'grad_norm': 0.341796875, 'learning_rate': 0.00024111612903225804, 'epoch': 0.05}\n",
      "{'loss': 3.1009, 'grad_norm': 0.34375, 'learning_rate': 0.00023908387096774195, 'epoch': 0.05}\n",
      "{'loss': 3.168, 'grad_norm': 0.322265625, 'learning_rate': 0.00023705161290322582, 'epoch': 0.05}\n",
      "{'loss': 3.2234, 'grad_norm': 0.35546875, 'learning_rate': 0.00023501935483870965, 'epoch': 0.05}\n",
      "{'loss': 3.1853, 'grad_norm': 0.330078125, 'learning_rate': 0.00023298709677419351, 'epoch': 0.05}\n",
      "{'loss': 3.1, 'grad_norm': 0.33203125, 'learning_rate': 0.00023095483870967743, 'epoch': 0.05}\n",
      "{'loss': 3.233, 'grad_norm': 0.3515625, 'learning_rate': 0.0002289225806451612, 'epoch': 0.05}\n",
      "{'loss': 3.1278, 'grad_norm': 0.326171875, 'learning_rate': 0.00022689032258064513, 'epoch': 0.05}\n",
      "{'loss': 3.1078, 'grad_norm': 0.353515625, 'learning_rate': 0.00022485806451612905, 'epoch': 0.05}\n",
      "{'loss': 3.1849, 'grad_norm': 0.33984375, 'learning_rate': 0.00022282580645161285, 'epoch': 0.05}\n",
      "{'loss': 3.1357, 'grad_norm': 0.353515625, 'learning_rate': 0.00022079354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.2365, 'grad_norm': 0.3515625, 'learning_rate': 0.0002187612903225806, 'epoch': 0.05}\n",
      "{'loss': 3.0625, 'grad_norm': 0.373046875, 'learning_rate': 0.00021672903225806447, 'epoch': 0.05}\n",
      "{'loss': 3.2634, 'grad_norm': 0.353515625, 'learning_rate': 0.0002146967741935484, 'epoch': 0.05}\n",
      "{'loss': 3.1346, 'grad_norm': 0.33203125, 'learning_rate': 0.00021266451612903225, 'epoch': 0.05}\n",
      "{'loss': 3.166, 'grad_norm': 0.337890625, 'learning_rate': 0.00021063225806451617, 'epoch': 0.05}\n",
      "{'loss': 3.1896, 'grad_norm': 0.322265625, 'learning_rate': 0.00020859999999999995, 'epoch': 0.05}\n",
      "{'loss': 3.0857, 'grad_norm': 0.359375, 'learning_rate': 0.00020656774193548386, 'epoch': 0.05}\n",
      "{'loss': 3.1155, 'grad_norm': 0.3828125, 'learning_rate': 0.00020453548387096773, 'epoch': 0.06}\n",
      "{'loss': 3.1326, 'grad_norm': 0.33203125, 'learning_rate': 0.00020250322580645156, 'epoch': 0.06}\n",
      "{'loss': 3.0766, 'grad_norm': 0.361328125, 'learning_rate': 0.00020047096774193548, 'epoch': 0.06}\n",
      "{'loss': 3.0949, 'grad_norm': 0.361328125, 'learning_rate': 0.00019843870967741934, 'epoch': 0.06}\n",
      "{'loss': 3.138, 'grad_norm': 0.36328125, 'learning_rate': 0.00019640645161290326, 'epoch': 0.06}\n",
      "{'loss': 3.1731, 'grad_norm': 0.3671875, 'learning_rate': 0.00019437419354838704, 'epoch': 0.06}\n",
      "{'loss': 3.1285, 'grad_norm': 0.341796875, 'learning_rate': 0.00019234193548387096, 'epoch': 0.06}\n",
      "{'loss': 3.1379, 'grad_norm': 0.3515625, 'learning_rate': 0.00019030967741935482, 'epoch': 0.06}\n",
      "{'loss': 3.1755, 'grad_norm': 0.333984375, 'learning_rate': 0.00018827741935483868, 'epoch': 0.06}\n",
      "{'loss': 3.0262, 'grad_norm': 0.3359375, 'learning_rate': 0.0001862451612903226, 'epoch': 0.06}\n",
      "{'loss': 3.1404, 'grad_norm': 0.349609375, 'learning_rate': 0.00018421290322580646, 'epoch': 0.06}\n",
      "{'loss': 3.06, 'grad_norm': 0.3203125, 'learning_rate': 0.0001821806451612903, 'epoch': 0.06}\n",
      "{'loss': 3.2104, 'grad_norm': 0.353515625, 'learning_rate': 0.00018014838709677416, 'epoch': 0.06}\n",
      "{'loss': 3.0479, 'grad_norm': 0.337890625, 'learning_rate': 0.00017811612903225808, 'epoch': 0.06}\n",
      "{'loss': 3.0623, 'grad_norm': 0.357421875, 'learning_rate': 0.00017608387096774186, 'epoch': 0.06}\n",
      "{'loss': 3.1323, 'grad_norm': 0.333984375, 'learning_rate': 0.00017405161290322578, 'epoch': 0.06}\n",
      "{'loss': 3.131, 'grad_norm': 0.337890625, 'learning_rate': 0.0001720193548387097, 'epoch': 0.06}\n",
      "{'loss': 3.1011, 'grad_norm': 0.328125, 'learning_rate': 0.0001699870967741935, 'epoch': 0.06}\n",
      "{'loss': 3.0483, 'grad_norm': 0.34375, 'learning_rate': 0.00016795483870967742, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 1763/30314 [29:59<7:58:56,  1.01s/it]2025-11-03 18:05:21,956 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.46 c\n",
      "{'train_runtime': 1800.4817, 'train_samples_per_second': 1077.524, 'train_steps_per_second': 16.837, 'train_loss': 4.1928955946379505, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 1764/30314 [30:00<8:05:40,  1.02s/it]\n",
      "2025-11-03 18:05:22,144 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:48<00:00, 25.89it/s]\n",
      "2025-11-03 18:06:10,884 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:06:12,030 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:06:12,030 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ. –í –Ω–∞–∑–≤–∞–Ω–∏–∏ ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–æ–π¬ª (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–∞—è¬ª), –∫–æ—Ç–æ—Ä–æ–µ –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\n",
      "\n",
      "–í –Ω–∞—á–∞–ª–µ XVI –≤–µ–∫–∞, –∫–æ–≥–¥–∞ –≤ XVII –≤–µ–∫–µ, –≤ –Ω–∞—á–∞–ª–µ XIX –≤–µ–∫–∞, –≤ —Å–æ—Å—Ç–∞–≤ –Ω–æ–≤–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏—è –ú–æ–≥–∏–ª–∞-–ö–æ–∫—É—Å—Å–∫–æ–≥–æ (–≤\n",
      "2025-11-03 18:06:12,030 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:06:12,030 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.4817, 'train_samples_per_second': 1077.524, 'train_steps_per_second': 16.837, 'total_flos': 2.9755307430877594e+17, 'train_loss': 4.1928955946379505, 'epoch': 0.05819189470038102}, 'eval': {'eval_loss': 3.7759594917297363, 'eval_runtime': 48.738, 'eval_samples_per_second': 102.589, 'eval_steps_per_second': 25.647, 'epoch': 0.05819189470038102}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ. –í –Ω–∞–∑–≤–∞–Ω–∏–∏ ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–æ–π¬ª (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–∞—è¬ª), –∫–æ—Ç–æ—Ä–æ–µ –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\\n\\n–í –Ω–∞—á–∞–ª–µ XVI –≤–µ–∫–∞, –∫–æ–≥–¥–∞ –≤ XVII –≤–µ–∫–µ, –≤ –Ω–∞—á–∞–ª–µ XIX –≤–µ–∫–∞, –≤ —Å–æ—Å—Ç–∞–≤ –Ω–æ–≤–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏—è –ú–æ–≥–∏–ª–∞-–ö–æ–∫—É—Å—Å–∫–æ–≥–æ (–≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_Baseline_1_GPU\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/sd6qsqe4\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_173519-sd6qsqe4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "name = 'Baseline_1_GPU'  # 21005 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=1 accelerate launch \\\n",
    "  --num_processes 1 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --bf16 \\\n",
    "    --torch-compile \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46259d7a",
   "metadata": {},
   "source": [
    "___\n",
    "### 1_GPU_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b39728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: baseline\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: World size: 1\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:38:49,236 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ baseline setup (single GPU)\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:39:25,437 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_173928-fi6xxlxh\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs32_ga4_1_GPU_v2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/fi6xxlxh\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:39:29,928 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/15157 [00:00<?, ?it/s]W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.2178, 'grad_norm': 20.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.6646, 'grad_norm': 4.375, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6086, 'grad_norm': 3.125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4485, 'grad_norm': 2.125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2855, 'grad_norm': 1.5703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2741, 'grad_norm': 0.8046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1588, 'grad_norm': 0.70703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0711, 'grad_norm': 1.046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0311, 'grad_norm': 0.53515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9181, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7596, 'grad_norm': 0.63671875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.65, 'grad_norm': 1.0078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4664, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3416, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.234, 'grad_norm': 0.9921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0975, 'grad_norm': 0.76953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0061, 'grad_norm': 0.87890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9141, 'grad_norm': 1.0703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7888, 'grad_norm': 1.1953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6668, 'grad_norm': 0.80859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 1.203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5382, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4091, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3719, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1828, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0909, 'grad_norm': 0.61328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1185, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0292, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9103, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8807, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7463, 'grad_norm': 0.6328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7806, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6389, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5508, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.591, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.52, 'grad_norm': 0.486328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4166, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3317, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3242, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3827, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3428, 'grad_norm': 0.60546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1412, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1289, 'grad_norm': 0.453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0632, 'grad_norm': 0.466796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.115, 'grad_norm': 0.4296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.9602, 'grad_norm': 0.494140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0676, 'grad_norm': 0.494140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0047, 'grad_norm': 0.490234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9609, 'grad_norm': 0.45703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9024, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8252, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9043, 'grad_norm': 0.462890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8285, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8326, 'grad_norm': 0.44140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.744, 'grad_norm': 0.455078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.7536, 'grad_norm': 0.43359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6146, 'grad_norm': 0.46484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6642, 'grad_norm': 0.474609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.7544, 'grad_norm': 0.408203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.649, 'grad_norm': 0.439453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6637, 'grad_norm': 0.53515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6458, 'grad_norm': 0.5234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6243, 'grad_norm': 0.40625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5585, 'grad_norm': 0.390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5093, 'grad_norm': 0.423828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5621, 'grad_norm': 0.4609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4421, 'grad_norm': 0.400390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4455, 'grad_norm': 0.431640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.388, 'grad_norm': 0.384765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4292, 'grad_norm': 0.37890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4434, 'grad_norm': 0.43359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3991, 'grad_norm': 0.349609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3895, 'grad_norm': 0.4140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3978, 'grad_norm': 0.455078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.395, 'grad_norm': 0.392578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.2923, 'grad_norm': 0.443359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.287, 'grad_norm': 0.40234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2475, 'grad_norm': 0.390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2237, 'grad_norm': 0.42578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2793, 'grad_norm': 0.44140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2741, 'grad_norm': 0.39453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.283, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2027, 'grad_norm': 0.361328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2928, 'grad_norm': 0.41015625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.0646, 'grad_norm': 0.375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1405, 'grad_norm': 0.392578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1003, 'grad_norm': 0.443359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1559, 'grad_norm': 0.37109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1283, 'grad_norm': 0.466796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1304, 'grad_norm': 0.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1321, 'grad_norm': 0.373046875, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.0703, 'grad_norm': 0.3515625, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.0967, 'grad_norm': 0.48046875, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 3.992, 'grad_norm': 0.3828125, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.0744, 'grad_norm': 0.359375, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.0947, 'grad_norm': 0.3359375, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.0249, 'grad_norm': 0.333984375, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.0022, 'grad_norm': 0.345703125, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.0675, 'grad_norm': 0.349609375, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 3.9764, 'grad_norm': 0.3359375, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.005, 'grad_norm': 0.35546875, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 3.9821, 'grad_norm': 0.34375, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 3.9256, 'grad_norm': 0.35546875, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 3.9588, 'grad_norm': 0.3515625, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.9006, 'grad_norm': 0.396484375, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.8624, 'grad_norm': 0.33203125, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 3.8759, 'grad_norm': 0.380859375, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.8686, 'grad_norm': 0.3203125, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 3.8634, 'grad_norm': 0.345703125, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 3.8465, 'grad_norm': 0.34765625, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 3.8116, 'grad_norm': 0.373046875, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 3.8536, 'grad_norm': 0.330078125, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 3.8893, 'grad_norm': 0.345703125, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 3.8247, 'grad_norm': 0.375, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 3.8143, 'grad_norm': 0.3203125, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 3.8233, 'grad_norm': 0.314453125, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 3.7915, 'grad_norm': 0.384765625, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 3.7602, 'grad_norm': 0.3203125, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 3.7373, 'grad_norm': 0.328125, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.7163, 'grad_norm': 0.3359375, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 3.814, 'grad_norm': 0.34765625, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.7119, 'grad_norm': 0.341796875, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 3.6795, 'grad_norm': 0.390625, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 3.7561, 'grad_norm': 0.341796875, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 3.7325, 'grad_norm': 0.376953125, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 3.6696, 'grad_norm': 0.357421875, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 3.7054, 'grad_norm': 0.41796875, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.7237, 'grad_norm': 0.328125, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.7056, 'grad_norm': 0.32421875, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 3.7291, 'grad_norm': 0.34765625, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 3.7102, 'grad_norm': 0.30859375, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 3.6603, 'grad_norm': 0.306640625, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 3.6216, 'grad_norm': 0.337890625, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 3.7018, 'grad_norm': 0.3125, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 3.5705, 'grad_norm': 0.34375, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 3.5655, 'grad_norm': 0.306640625, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 3.6044, 'grad_norm': 0.322265625, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 3.6511, 'grad_norm': 0.30859375, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 3.6614, 'grad_norm': 0.3125, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 3.5474, 'grad_norm': 0.322265625, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 3.572, 'grad_norm': 0.32421875, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 3.6659, 'grad_norm': 0.376953125, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 3.5706, 'grad_norm': 0.31640625, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 3.5303, 'grad_norm': 0.353515625, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 3.4868, 'grad_norm': 0.318359375, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 3.4716, 'grad_norm': 0.32421875, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 3.5134, 'grad_norm': 0.3203125, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 3.5556, 'grad_norm': 0.33203125, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.5228, 'grad_norm': 0.345703125, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 3.4094, 'grad_norm': 0.30078125, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 3.4992, 'grad_norm': 0.296875, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 3.4978, 'grad_norm': 0.318359375, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 3.4548, 'grad_norm': 0.32421875, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 3.5087, 'grad_norm': 0.28515625, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 3.4366, 'grad_norm': 0.310546875, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 3.4329, 'grad_norm': 0.296875, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 3.4529, 'grad_norm': 0.302734375, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 3.3886, 'grad_norm': 0.30078125, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.3886, 'grad_norm': 0.271484375, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 3.4548, 'grad_norm': 0.279296875, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 3.4107, 'grad_norm': 0.287109375, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 3.3581, 'grad_norm': 0.2890625, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 3.4017, 'grad_norm': 0.3046875, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 3.3939, 'grad_norm': 0.3046875, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 3.439, 'grad_norm': 0.29296875, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 3.4206, 'grad_norm': 0.27734375, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 3.334, 'grad_norm': 0.29296875, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 3.3388, 'grad_norm': 0.294921875, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 3.355, 'grad_norm': 0.291015625, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 3.3845, 'grad_norm': 0.28515625, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 3.3881, 'grad_norm': 0.296875, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 3.3138, 'grad_norm': 0.28125, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 3.3624, 'grad_norm': 0.294921875, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 3.2829, 'grad_norm': 0.3125, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 3.3625, 'grad_norm': 0.29296875, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "{'loss': 3.2962, 'grad_norm': 0.28125, 'learning_rate': 0.0005256322580645161, 'epoch': 0.06}\n",
      "{'loss': 3.2926, 'grad_norm': 0.27734375, 'learning_rate': 0.0005235999999999999, 'epoch': 0.06}\n",
      "{'loss': 3.2885, 'grad_norm': 0.27734375, 'learning_rate': 0.0005215677419354839, 'epoch': 0.06}\n",
      "{'loss': 3.2715, 'grad_norm': 0.29296875, 'learning_rate': 0.0005195354838709677, 'epoch': 0.06}\n",
      "{'loss': 3.3443, 'grad_norm': 0.287109375, 'learning_rate': 0.0005175032258064515, 'epoch': 0.06}\n",
      "{'loss': 3.3378, 'grad_norm': 0.2890625, 'learning_rate': 0.0005154709677419354, 'epoch': 0.06}\n",
      "{'loss': 3.3411, 'grad_norm': 0.31640625, 'learning_rate': 0.0005134387096774193, 'epoch': 0.06}\n",
      "{'loss': 3.2782, 'grad_norm': 0.29296875, 'learning_rate': 0.0005114064516129032, 'epoch': 0.06}\n",
      "{'loss': 3.2895, 'grad_norm': 0.29296875, 'learning_rate': 0.0005093741935483871, 'epoch': 0.06}\n",
      "{'loss': 3.2627, 'grad_norm': 0.28515625, 'learning_rate': 0.0005073419354838709, 'epoch': 0.06}\n",
      "{'loss': 3.365, 'grad_norm': 0.287109375, 'learning_rate': 0.0005053096774193548, 'epoch': 0.06}\n",
      "{'loss': 3.258, 'grad_norm': 0.283203125, 'learning_rate': 0.0005032774193548386, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 939/15157 [29:58<7:32:45,  1.91s/it]2025-11-03 18:09:30,750 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.46 c\n",
      "{'loss': 3.2427, 'grad_norm': 0.279296875, 'learning_rate': 0.0005012451612903226, 'epoch': 0.06}\n",
      "{'train_runtime': 1800.4886, 'train_samples_per_second': 1077.52, 'train_steps_per_second': 8.418, 'train_loss': 4.574666688797322, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 940/15157 [30:00<7:33:51,  1.92s/it]\n",
      "2025-11-03 18:09:30,978 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:47<00:00, 26.46it/s]\n",
      "2025-11-03 18:10:18,677 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:10:19,744 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:10:19,744 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ –æ—Ç –Ω–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª (–≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Å ¬´–ê—Ä–∏-–ö–≤–∞–¥—Ä–∞—Ç¬ª).\n",
      "\n",
      "–í –Ω–∞—á–∞–ª–µ ¬´–¢—Ä–æ–ø–æ–ª–∏–π—Å–∫–æ–π¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª –≤ —Å–æ—Å—Ç–∞–≤–µ –≥—Ä—É–ø–ø—ã ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª —Å—Ç–∞–ª–∏ ¬´–ö—Ä—É–ø–Ω—ã–π –∫–∞–º–µ–Ω—å¬ª. –í\n",
      "2025-11-03 18:10:19,744 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:10:19,745 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.4886, 'train_samples_per_second': 1077.52, 'train_steps_per_second': 8.418, 'total_flos': 3.171200565195571e+17, 'train_loss': 4.574666688797322, 'epoch': 0.06201857258317251}, 'eval': {'eval_loss': 3.939300060272217, 'eval_runtime': 47.6959, 'eval_samples_per_second': 104.831, 'eval_steps_per_second': 26.208, 'epoch': 0.06201857258317251}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ –æ—Ç –Ω–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª (–≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Å ¬´–ê—Ä–∏-–ö–≤–∞–¥—Ä–∞—Ç¬ª).\\n\\n–í –Ω–∞—á–∞–ª–µ ¬´–¢—Ä–æ–ø–æ–ª–∏–π—Å–∫–æ–π¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª –≤ —Å–æ—Å—Ç–∞–≤–µ –≥—Ä—É–ø–ø—ã ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª —Å—Ç–∞–ª–∏ ¬´–ö—Ä—É–ø–Ω—ã–π –∫–∞–º–µ–Ω—å¬ª. –í'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs32_ga4_1_GPU_v2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/fi6xxlxh\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_173928-fi6xxlxh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "name = '1_GPU_v2'  # 34353 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 accelerate launch \\\n",
    "  --num_processes 1 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29502 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --bf16 \\\n",
    "    --torch-compile \\\n",
    "    --batch-size 32 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a79970",
   "metadata": {},
   "source": [
    "___\n",
    "### DeepSpeed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8434d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: World size: 2\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:41:17,721 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:41:53,995 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 17:41:54,121 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 17:41:56,263 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 17:41:56,275 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 17:41:56,278 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_174157-zci3incb\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_1\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zci3incb\n",
      "2025-11-03 17:41:58,242 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:41:58,243 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:41:58,243 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:41:58,243 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6922, 'grad_norm': 3.668818712234497, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5162, 'grad_norm': 5.018797397613525, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8406, 'grad_norm': 3.383873462677002, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4637, 'grad_norm': 1.1404584646224976, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1786, 'grad_norm': 1.0485060214996338, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0072, 'grad_norm': 0.9469280242919922, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8163, 'grad_norm': 0.9315477609634399, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7282, 'grad_norm': 0.7519756555557251, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6889, 'grad_norm': 0.6929640769958496, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5823, 'grad_norm': 0.6466392278671265, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4359, 'grad_norm': 0.9969204664230347, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3418, 'grad_norm': 0.7500238418579102, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2126, 'grad_norm': 0.7365084290504456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1456, 'grad_norm': 1.0828458070755005, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1163, 'grad_norm': 0.8652408719062805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0385, 'grad_norm': 1.0536725521087646, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9519, 'grad_norm': 0.9354153871536255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9417, 'grad_norm': 1.0707335472106934, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8195, 'grad_norm': 0.9179897904396057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7262, 'grad_norm': 0.8731324672698975, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6415, 'grad_norm': 0.9537513852119446, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6421, 'grad_norm': 0.798117995262146, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5473, 'grad_norm': 0.7531899213790894, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 0.8972017765045166, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3912, 'grad_norm': 0.7779728770256042, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3237, 'grad_norm': 0.9014979600906372, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4111, 'grad_norm': 1.1039460897445679, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3302, 'grad_norm': 0.957070529460907, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2359, 'grad_norm': 0.8378677368164062, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2309, 'grad_norm': 0.8541988730430603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1162, 'grad_norm': 0.8503371477127075, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1832, 'grad_norm': 1.0437371730804443, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0714, 'grad_norm': 0.9317355751991272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9724, 'grad_norm': 1.0102925300598145, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0475, 'grad_norm': 0.8593927621841431, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9847, 'grad_norm': 1.0534253120422363, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8935, 'grad_norm': 0.9747653007507324, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8134, 'grad_norm': 0.9832426309585571, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8463, 'grad_norm': 0.9903159737586975, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9006, 'grad_norm': 0.8788219690322876, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.873, 'grad_norm': 1.0442148447036743, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6724, 'grad_norm': 0.8916881680488586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6812, 'grad_norm': 0.8051306009292603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6383, 'grad_norm': 0.8629149794578552, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6937, 'grad_norm': 0.8521005511283875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.544, 'grad_norm': 1.1062430143356323, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6471, 'grad_norm': 1.1298426389694214, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6054, 'grad_norm': 0.8776903748512268, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5568, 'grad_norm': 0.9645063877105713, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.508, 'grad_norm': 0.8895986080169678, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4407, 'grad_norm': 0.9209122061729431, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5146, 'grad_norm': 0.8386083245277405, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4619, 'grad_norm': 0.8752357363700867, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4552, 'grad_norm': 0.8572624921798706, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3756, 'grad_norm': 1.0075575113296509, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3977, 'grad_norm': 0.8936896324157715, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2583, 'grad_norm': 0.9287660717964172, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3186, 'grad_norm': 0.9208709001541138, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4158, 'grad_norm': 0.7969366908073425, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.29, 'grad_norm': 0.8502371907234192, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3189, 'grad_norm': 0.8190291523933411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3151, 'grad_norm': 0.8564655184745789, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2886, 'grad_norm': 0.8515342473983765, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2173, 'grad_norm': 0.988594651222229, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1702, 'grad_norm': 1.038339376449585, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2426, 'grad_norm': 1.0462836027145386, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1066, 'grad_norm': 0.909101665019989, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1209, 'grad_norm': 0.8730531930923462, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0664, 'grad_norm': 0.9852443933486938, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0997, 'grad_norm': 0.7898673415184021, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1196, 'grad_norm': 0.9493786692619324, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0843, 'grad_norm': 0.8290815949440002, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0771, 'grad_norm': 1.2102653980255127, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0775, 'grad_norm': 0.9372451305389404, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0902, 'grad_norm': 0.8382816910743713, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9881, 'grad_norm': 0.8766090869903564, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9753, 'grad_norm': 0.8322007060050964, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.93, 'grad_norm': 0.8284767866134644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9013, 'grad_norm': 0.9744827151298523, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9883, 'grad_norm': 0.905106246471405, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9666, 'grad_norm': 0.8460886478424072, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9859, 'grad_norm': 0.9141154289245605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8999, 'grad_norm': 0.8842290639877319, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9915, 'grad_norm': 0.9232208132743835, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7489, 'grad_norm': 0.9172796010971069, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8429, 'grad_norm': 0.8144638538360596, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7998, 'grad_norm': 0.9605206251144409, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.862, 'grad_norm': 0.9390643239021301, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8131, 'grad_norm': 0.834696888923645, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8376, 'grad_norm': 0.8131949305534363, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8199, 'grad_norm': 0.9954076409339905, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.7698, 'grad_norm': 0.8776119947433472, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8024, 'grad_norm': 0.9735174775123596, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6924, 'grad_norm': 0.8690549731254578, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7678, 'grad_norm': 0.8721895217895508, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7896, 'grad_norm': 0.9969426989555359, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7024, 'grad_norm': 0.8049042224884033, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.704, 'grad_norm': 0.8616464734077454, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7652, 'grad_norm': 0.9246629476547241, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6866, 'grad_norm': 0.9274408221244812, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7041, 'grad_norm': 0.8567649126052856, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6942, 'grad_norm': 0.8865391612052917, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.6316, 'grad_norm': 0.9642078876495361, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.675, 'grad_norm': 0.9411467909812927, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6225, 'grad_norm': 0.9812774658203125, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5814, 'grad_norm': 0.8918157815933228, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5937, 'grad_norm': 1.0066958665847778, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5841, 'grad_norm': 0.8227797746658325, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5754, 'grad_norm': 0.9473503232002258, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5744, 'grad_norm': 0.9501177668571472, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5395, 'grad_norm': 0.871414303779602, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.5847, 'grad_norm': 0.9024458527565002, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6158, 'grad_norm': 0.8368567228317261, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.547, 'grad_norm': 0.9039610624313354, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5312, 'grad_norm': 0.791866660118103, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5414, 'grad_norm': 0.8737545609474182, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5235, 'grad_norm': 0.8648928999900818, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4892, 'grad_norm': 0.8738812804222107, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4718, 'grad_norm': 0.8620582818984985, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.467, 'grad_norm': 0.9816555380821228, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5523, 'grad_norm': 0.9490968585014343, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.4588, 'grad_norm': 0.9404143691062927, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4402, 'grad_norm': 0.9303212761878967, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5139, 'grad_norm': 0.9257434606552124, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4854, 'grad_norm': 0.96829754114151, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4346, 'grad_norm': 1.050953984260559, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4629, 'grad_norm': 1.118993878364563, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4783, 'grad_norm': 0.8649080991744995, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.8566702604293823, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.9625371098518372, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4649, 'grad_norm': 0.9149383306503296, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4291, 'grad_norm': 0.8810011744499207, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.378, 'grad_norm': 0.9099262952804565, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4707, 'grad_norm': 0.9336453676223755, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3377, 'grad_norm': 0.8025274276733398, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3405, 'grad_norm': 0.8041309714317322, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3813, 'grad_norm': 0.8100106716156006, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4301, 'grad_norm': 0.8509664535522461, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4611, 'grad_norm': 0.8618844747543335, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3274, 'grad_norm': 0.8955902457237244, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3731, 'grad_norm': 0.8158877491950989, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4656, 'grad_norm': 0.8626905083656311, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.358, 'grad_norm': 0.8353629112243652, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.3167, 'grad_norm': 0.8798050880432129, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2846, 'grad_norm': 0.8547061681747437, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2597, 'grad_norm': 0.8513766527175903, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.3015, 'grad_norm': 0.8333733677864075, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3691, 'grad_norm': 0.8628431558609009, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3291, 'grad_norm': 0.9778262972831726, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.1961, 'grad_norm': 0.8584219217300415, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.3205, 'grad_norm': 0.9154632091522217, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3164, 'grad_norm': 0.8557528257369995, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2579, 'grad_norm': 0.8773623704910278, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3276, 'grad_norm': 0.8342890739440918, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2425, 'grad_norm': 0.8907840847969055, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2454, 'grad_norm': 1.0231133699417114, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 1.007142186164856, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2036, 'grad_norm': 0.812870979309082, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.2003, 'grad_norm': 0.8798156976699829, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 0.9473292827606201, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2285, 'grad_norm': 0.8432570695877075, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1559, 'grad_norm': 0.830669641494751, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8659570217132568, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2275, 'grad_norm': 0.8856301307678223, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.2703, 'grad_norm': 0.8541915416717529, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.2755, 'grad_norm': 0.815159022808075, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1696, 'grad_norm': 0.8821364641189575, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1721, 'grad_norm': 0.8121861219406128, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1909, 'grad_norm': 0.8712433576583862, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2245, 'grad_norm': 0.8510169982910156, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2186, 'grad_norm': 0.9506018757820129, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1542, 'grad_norm': 0.932791531085968, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.1958, 'grad_norm': 0.8545206189155579, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 4.108, 'grad_norm': 0.9410622119903564, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 4.1713, 'grad_norm': 0.7748789191246033, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 876/15157 [29:59<8:10:38,  2.06s/it]2025-11-03 18:12:04,207 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.22 c\n",
      "{'train_runtime': 1801.2283, 'train_samples_per_second': 1077.078, 'train_steps_per_second': 8.415, 'train_loss': 5.239923280348392, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 877/15157 [30:01<8:08:48,  2.05s/it]\n",
      "2025-11-03 18:12:04,389 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:12:04,393 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.25it/s]\n",
      "2025-11-03 18:12:28,374 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:12:29,454 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:12:29,454 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 2006 –≥–æ–¥—É. –í 2010 –≥–æ–¥—É, –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 36 –ª–µ—Ç, –≤ 2013 –≥–æ–¥—É –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º –∑–∞–≤–µ–¥—É—é—â–∏–º.\n",
      "\n",
      "–í 2015 –≥–æ–¥—É, –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –≤ ¬´–ü—è—Ç—å—É¬ª.\n",
      "\n",
      "–í 2014 –≥–æ–¥—É –≤ —Ç–µ–∞—Ç—Ä–µ, –ø–æ –¥–∞–Ω–Ω—ã–º –ø–µ—Ä–µ–ø–∏—Å–∏ 2011 –≥–æ–¥–∞, –Ω–∞ –º–µ—Å—Ç–µ –Ω–∞—á–∞–ª–∞ –≤\n",
      "2025-11-03 18:12:29,455 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:12:29,455 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.2283, 'train_samples_per_second': 1077.078, 'train_steps_per_second': 8.415, 'total_flos': 2.958662654975017e+17, 'train_loss': 5.239923280348392, 'epoch': 0.05786200867600244}, 'eval': {'eval_loss': 4.827373504638672, 'eval_runtime': 23.9817, 'eval_samples_per_second': 208.492, 'eval_steps_per_second': 26.062, 'epoch': 0.05786200867600244}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 2006 –≥–æ–¥—É. –í 2010 –≥–æ–¥—É, –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 36 –ª–µ—Ç, –≤ 2013 –≥–æ–¥—É –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º –∑–∞–≤–µ–¥—É—é—â–∏–º.\\n\\n–í 2015 –≥–æ–¥—É, –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –≤ ¬´–ü—è—Ç—å—É¬ª.\\n\\n–í 2014 –≥–æ–¥—É –≤ —Ç–µ–∞—Ç—Ä–µ, –ø–æ –¥–∞–Ω–Ω—ã–º –ø–µ—Ä–µ–ø–∏—Å–∏ 2011 –≥–æ–¥–∞, –Ω–∞ –º–µ—Å—Ç–µ –Ω–∞—á–∞–ª–∞ –≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_DeepSpeed_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zci3incb\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_174157-zci3incb/logs\u001b[0m\n",
      "[rank0]:[W1103 18:12:31.364719994 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_1'  # 35523 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29503 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922de6bd",
   "metadata": {},
   "source": [
    "___\n",
    "### DeepSpeed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91229ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: World size: 2\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 2)\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 18:13:09,586 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:13:09,774 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:13:11,958 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:13:11,970 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 18:13:11,974 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_181312-nft9o7va\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/nft9o7va\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 18:13:13,961 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.7798, 'grad_norm': 4.454235553741455, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.812, 'grad_norm': 1.1511505842208862, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 9.1108, 'grad_norm': 0.6907618641853333, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6884, 'grad_norm': 0.4392479360103607, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4066, 'grad_norm': 0.3064589202404022, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2598, 'grad_norm': 0.46854639053344727, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0787, 'grad_norm': 0.2971721887588501, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.001, 'grad_norm': 0.30156153440475464, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9758, 'grad_norm': 0.28211864829063416, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9005, 'grad_norm': 0.27629169821739197, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8063, 'grad_norm': 0.2253260463476181, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7609, 'grad_norm': 0.3038266599178314, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6779, 'grad_norm': 0.25365614891052246, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6621, 'grad_norm': 0.34839579463005066, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6522, 'grad_norm': 0.27485111355781555, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6091, 'grad_norm': 0.3047346770763397, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5894, 'grad_norm': 0.35588470101356506, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5749, 'grad_norm': 0.3648209273815155, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4981, 'grad_norm': 0.34213685989379883, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4486, 'grad_norm': 0.3252623975276947, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3769, 'grad_norm': 0.36113014817237854, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3832, 'grad_norm': 0.3960557281970978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3152, 'grad_norm': 0.31657683849334717, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.337, 'grad_norm': 0.3433096706867218, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1899, 'grad_norm': 0.3341600298881531, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1403, 'grad_norm': 0.3360576331615448, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.2185, 'grad_norm': 0.3185850977897644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1562, 'grad_norm': 0.3112231492996216, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1007, 'grad_norm': 0.31848135590553284, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.083, 'grad_norm': 0.32603323459625244, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9926, 'grad_norm': 0.32045257091522217, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0254, 'grad_norm': 0.3346325159072876, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9342, 'grad_norm': 0.29385778307914734, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8594, 'grad_norm': 0.2963966727256775, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.932, 'grad_norm': 0.3823137879371643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8761, 'grad_norm': 0.32335802912712097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7925, 'grad_norm': 0.3637615740299225, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.724, 'grad_norm': 0.3655754327774048, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7519, 'grad_norm': 0.40341511368751526, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.779, 'grad_norm': 0.32469722628593445, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7674, 'grad_norm': 0.3360317647457123, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 0.3705793619155884, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5738, 'grad_norm': 0.30030012130737305, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5623, 'grad_norm': 0.3573712408542633, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5826, 'grad_norm': 0.36503905057907104, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4648, 'grad_norm': 0.4010322391986847, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.5251, 'grad_norm': 0.29925772547721863, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4958, 'grad_norm': 0.31168434023857117, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4494, 'grad_norm': 0.3172328472137451, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3963, 'grad_norm': 0.3231424391269684, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.348, 'grad_norm': 0.36385414004325867, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4111, 'grad_norm': 0.34250786900520325, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3578, 'grad_norm': 0.3524267375469208, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3286, 'grad_norm': 0.29518604278564453, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2773, 'grad_norm': 0.33538439869880676, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2908, 'grad_norm': 0.3221695125102997, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1683, 'grad_norm': 0.35099560022354126, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2085, 'grad_norm': 0.2988779842853546, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2991, 'grad_norm': 0.33221280574798584, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1761, 'grad_norm': 0.40171706676483154, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1873, 'grad_norm': 0.3142092227935791, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2123, 'grad_norm': 0.31641751527786255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1663, 'grad_norm': 0.32732561230659485, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1133, 'grad_norm': 0.36741089820861816, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0557, 'grad_norm': 0.4393484592437744, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.138, 'grad_norm': 0.3109776973724365, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9981, 'grad_norm': 0.35914888978004456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0191, 'grad_norm': 0.3538433015346527, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9597, 'grad_norm': 0.37219491600990295, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9799, 'grad_norm': 0.3403673470020294, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0109, 'grad_norm': 0.3522985279560089, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9644, 'grad_norm': 0.31357496976852417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9513, 'grad_norm': 0.37986108660697937, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9389, 'grad_norm': 0.34722381830215454, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9565, 'grad_norm': 0.3252619504928589, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.8815, 'grad_norm': 0.30623528361320496, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8752, 'grad_norm': 0.29321974515914917, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8339, 'grad_norm': 0.3290019929409027, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.771, 'grad_norm': 0.320248007774353, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.883, 'grad_norm': 0.3240400552749634, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8658, 'grad_norm': 0.3640105128288269, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8648, 'grad_norm': 0.3070066571235657, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7845, 'grad_norm': 0.3596435785293579, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8324, 'grad_norm': 0.30248793959617615, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6447, 'grad_norm': 0.3452083468437195, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7226, 'grad_norm': 0.3048035502433777, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6741, 'grad_norm': 0.3311581313610077, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7535, 'grad_norm': 0.33844393491744995, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6934, 'grad_norm': 0.2992381751537323, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7102, 'grad_norm': 0.3232366144657135, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6673, 'grad_norm': 0.34503433108329773, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 5.6363, 'grad_norm': 0.3414338529109955, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 5.6695, 'grad_norm': 0.3340323865413666, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 5.5686, 'grad_norm': 0.2953379154205322, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 5.6312, 'grad_norm': 0.33869630098342896, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 5.6431, 'grad_norm': 0.31590256094932556, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 5.5586, 'grad_norm': 0.34281596541404724, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 5.5648, 'grad_norm': 0.42304888367652893, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 5.6195, 'grad_norm': 0.3659343719482422, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 5.5448, 'grad_norm': 0.2783743441104889, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 5.5707, 'grad_norm': 0.3218107521533966, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 5.5673, 'grad_norm': 0.31465089321136475, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 5.5025, 'grad_norm': 0.32628804445266724, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 5.5503, 'grad_norm': 0.34743961691856384, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 5.4926, 'grad_norm': 0.3055354356765747, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 5.4479, 'grad_norm': 0.5234678387641907, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 5.4685, 'grad_norm': 0.32617372274398804, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 5.4509, 'grad_norm': 0.3556486964225769, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 5.4252, 'grad_norm': 0.3654335141181946, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 5.4582, 'grad_norm': 0.31860828399658203, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 5.404, 'grad_norm': 0.3205486536026001, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 5.4522, 'grad_norm': 0.39203187823295593, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 5.4721, 'grad_norm': 0.30983930826187134, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "  4%|‚ñé         | 568/15157 [29:58<12:48:54,  3.16s/it]2025-11-03 18:43:21,225 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.84 c\n",
      "{'train_runtime': 1801.851, 'train_samples_per_second': 1076.706, 'train_steps_per_second': 8.412, 'train_loss': 6.516830628701799, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 569/15157 [30:01<12:49:55,  3.17s/it]\n",
      "2025-11-03 18:43:21,374 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:43:21,391 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.40it/s]\n",
      "2025-11-03 18:43:45,263 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:43:46,312 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:43:46,313 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Ç–∞–∫.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –õ–æ–Ω–¥–æ–Ω–µ –≤ —Å–µ–ª–µ –≤ –≥–æ—Ä–æ–¥–µ –®–∞–Ω—å–Ω–æ-–í.¬†–í.¬†–ê.¬†–í.¬†–§.¬†–í.¬†–®.¬†–í.¬†–õ.¬†–°.¬†–õ.: –ù–∞—É–∫–∞.¬†‚Äî –ú.: –ù–∞—É–∫–∞, 2007.\n",
      "\n",
      "–í 1961¬†–≥\n",
      "2025-11-03 18:43:46,313 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:43:46,313 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.851, 'train_samples_per_second': 1076.706, 'train_steps_per_second': 8.412, 'total_flos': 1.919588427230085e+17, 'train_loss': 6.516830628701799, 'epoch': 0.037541029574282084}, 'eval': {'eval_loss': 6.077337265014648, 'eval_runtime': 23.8686, 'eval_samples_per_second': 209.48, 'eval_steps_per_second': 26.185, 'epoch': 0.037541029574282084}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Ç–∞–∫.\\n\\n–£—á–∏–ª—Å—è –≤ –õ–æ–Ω–¥–æ–Ω–µ –≤ —Å–µ–ª–µ –≤ –≥–æ—Ä–æ–¥–µ –®–∞–Ω—å–Ω–æ-–í.\\xa0–í.\\xa0–ê.\\xa0–í.\\xa0–§.\\xa0–í.\\xa0–®.\\xa0–í.\\xa0–õ.\\xa0–°.\\xa0–õ.: –ù–∞—É–∫–∞.\\xa0‚Äî –ú.: –ù–∞—É–∫–∞, 2007.\\n\\n–í 1961\\xa0–≥'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_DeepSpeed_2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/nft9o7va\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_181312-nft9o7va/logs\u001b[0m\n",
      "[rank0]:[W1103 18:43:49.059038118 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_2'  # 33135 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29504 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 2 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b4dda",
   "metadata": {},
   "source": [
    "___\n",
    "### DeepSpeed_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: World size: 2\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 3)\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 18:16:11,755 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:16:12,151 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:16:14,255 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:16:14,266 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:16:14,270 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_181615-0hzzdpbr\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_3\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/0hzzdpbr\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 18:16:16,194 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "Parameter Offload - Persistent parameters statistics: param_count = 49, numel = 54272\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6923, 'grad_norm': 3.6686655097746415, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5164, 'grad_norm': 5.038104472096909, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8408, 'grad_norm': 3.378780287138547, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4639, 'grad_norm': 1.138786406505714, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1786, 'grad_norm': 1.0461821231771407, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.007, 'grad_norm': 0.9544622024922441, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8161, 'grad_norm': 0.9355534419367514, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.728, 'grad_norm': 0.7604584328022364, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6889, 'grad_norm': 0.7006783320900247, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5826, 'grad_norm': 0.6495457724862668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4363, 'grad_norm': 0.9687593987575245, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3419, 'grad_norm': 0.7123596091597003, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2132, 'grad_norm': 0.7397988119023997, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1458, 'grad_norm': 1.088446608287305, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.116, 'grad_norm': 0.8647698004273836, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0382, 'grad_norm': 1.0740004714847242, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.952, 'grad_norm': 0.9312154623967168, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9419, 'grad_norm': 1.1001762148230307, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8198, 'grad_norm': 0.9295160526464471, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7263, 'grad_norm': 0.8783628941699118, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6418, 'grad_norm': 0.9664533908613936, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6425, 'grad_norm': 0.8065055679690432, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5477, 'grad_norm': 0.7515727800765956, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5766, 'grad_norm': 0.9016929483563586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3917, 'grad_norm': 0.7796490862862352, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3243, 'grad_norm': 0.9225948472436843, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4116, 'grad_norm': 1.1022671598518248, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3312, 'grad_norm': 0.9677615658309514, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2364, 'grad_norm': 0.8328573449091918, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2315, 'grad_norm': 0.8477178330803297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1168, 'grad_norm': 0.8728834707958798, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1838, 'grad_norm': 1.0399749854554272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0719, 'grad_norm': 0.9345049994762274, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9732, 'grad_norm': 1.024096602301595, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0478, 'grad_norm': 0.8582576046140278, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9849, 'grad_norm': 1.0546145345417732, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8939, 'grad_norm': 0.9674367821552126, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8142, 'grad_norm': 0.9956267563839062, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8465, 'grad_norm': 0.9813303691153088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9012, 'grad_norm': 0.8758007376970978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8739, 'grad_norm': 1.047776960308627, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6729, 'grad_norm': 0.888957986901388, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6816, 'grad_norm': 0.8077809607986471, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6385, 'grad_norm': 0.8591134463352412, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6942, 'grad_norm': 0.8552180473896958, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5446, 'grad_norm': 1.103853596144282, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6477, 'grad_norm': 1.1310697328127202, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6061, 'grad_norm': 0.8815733764256164, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5574, 'grad_norm': 0.9620597983115597, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5087, 'grad_norm': 0.8803177870977738, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4417, 'grad_norm': 0.9158506692991487, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5153, 'grad_norm': 0.8421436456404593, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.463, 'grad_norm': 0.8840385190928453, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4559, 'grad_norm': 0.8569972150135463, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3761, 'grad_norm': 0.9986714373766857, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3982, 'grad_norm': 0.8958509139875254, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2589, 'grad_norm': 0.9334281461796646, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3191, 'grad_norm': 0.9009156778348731, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4164, 'grad_norm': 0.8025089916963281, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2904, 'grad_norm': 0.8604925855242838, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3195, 'grad_norm': 0.8239474578921805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3157, 'grad_norm': 0.8516390445330702, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2892, 'grad_norm': 0.8554077219739088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.218, 'grad_norm': 0.988212880196313, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1706, 'grad_norm': 1.0378519718728358, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2431, 'grad_norm': 1.0498656526032168, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.107, 'grad_norm': 0.9113654133371218, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1213, 'grad_norm': 0.8693178016038134, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0669, 'grad_norm': 0.9882475266367782, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.791020866135482, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1198, 'grad_norm': 0.9475528529429813, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0848, 'grad_norm': 0.8394621446566618, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0778, 'grad_norm': 1.2157962400382296, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0782, 'grad_norm': 0.9460464905704643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0908, 'grad_norm': 0.8458119416301931, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9888, 'grad_norm': 0.8814883198404059, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.976, 'grad_norm': 0.8389241872167115, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9305, 'grad_norm': 0.8234012540963013, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.902, 'grad_norm': 0.9771300512741184, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9888, 'grad_norm': 0.9083272175955754, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9672, 'grad_norm': 0.8471878221796395, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9864, 'grad_norm': 0.9118531826054335, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9003, 'grad_norm': 0.8833711115644334, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9919, 'grad_norm': 0.9209230367573716, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7493, 'grad_norm': 0.9165489825921537, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8432, 'grad_norm': 0.8108080042634097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8001, 'grad_norm': 0.953483909747508, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8624, 'grad_norm': 0.9328252594698135, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8135, 'grad_norm': 0.8324237243810227, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8379, 'grad_norm': 0.8161158230424689, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8202, 'grad_norm': 0.985186735442075, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.77, 'grad_norm': 0.8712857288047461, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8026, 'grad_norm': 0.9713614836531296, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6929, 'grad_norm': 0.8701215727170363, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7682, 'grad_norm': 0.8754376020560871, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7899, 'grad_norm': 0.9908918244459293, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7026, 'grad_norm': 0.8064597339009851, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7046, 'grad_norm': 0.8617681029261028, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7655, 'grad_norm': 0.9222676315711975, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.687, 'grad_norm': 0.92291603691172, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8547991685504982, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6945, 'grad_norm': 0.8927314469155044, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.6321, 'grad_norm': 0.9685155719602763, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6756, 'grad_norm': 0.9371410125214955, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6229, 'grad_norm': 0.974017673529815, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5818, 'grad_norm': 0.8895976507428887, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5938, 'grad_norm': 1.0040532489741272, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5842, 'grad_norm': 0.819692972334174, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5757, 'grad_norm': 0.94733784263099, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5747, 'grad_norm': 0.9469483594603054, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5398, 'grad_norm': 0.8729129401284417, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.585, 'grad_norm': 0.9024696003732986, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6161, 'grad_norm': 0.8335436558588407, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5473, 'grad_norm': 0.9068393629982003, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5314, 'grad_norm': 0.7916696649508305, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5415, 'grad_norm': 0.872044788459042, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5237, 'grad_norm': 0.8648677430085027, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4893, 'grad_norm': 0.871713866103787, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4721, 'grad_norm': 0.8625541217363673, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.4671, 'grad_norm': 0.9813430683324117, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5525, 'grad_norm': 0.9507654019307644, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.9444311432434342, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4404, 'grad_norm': 0.9265397991502312, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5143, 'grad_norm': 0.9223589790102357, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4857, 'grad_norm': 0.9743350066698281, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.435, 'grad_norm': 1.0534058105454671, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4633, 'grad_norm': 1.1147617491416895, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4788, 'grad_norm': 0.8708640169199661, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4592, 'grad_norm': 0.8551973653014545, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4928, 'grad_norm': 0.9669475442821185, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4651, 'grad_norm': 0.9095261171735732, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4295, 'grad_norm': 0.8809266148557976, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.3781, 'grad_norm': 0.9085483165964268, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.471, 'grad_norm': 0.9326344480202532, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.338, 'grad_norm': 0.8033044348506737, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3407, 'grad_norm': 0.8048520569741922, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3813, 'grad_norm': 0.8083669630896226, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4304, 'grad_norm': 0.8497074244953333, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 690/15157 [29:58<10:25:52,  2.60s/it]2025-11-03 18:46:22,402 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.23 c\n",
      "{'train_runtime': 1801.2486, 'train_samples_per_second': 1077.066, 'train_steps_per_second': 8.415, 'train_loss': 5.504332236029132, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 691/15157 [30:01<10:28:28,  2.61s/it]\n",
      "2025-11-03 18:46:22,535 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:46:22,535 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:37<00:00, 16.63it/s]\n",
      "2025-11-03 18:47:00,508 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_3'  # 36829 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29505 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 3 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --stage3-prefetch-bucket-size 500000000 \\\n",
    "    --stage3-param-persistence-threshold 100000 \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e7ac5",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:17:51,879 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:17:52,134 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:17:52,134 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:17:52,136 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f2236deaf00>]}\n",
      "/app/lib/training.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 19:17:53,846 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:17:54,380 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:17:54,468 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:17:54,468 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:17:54,470 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f17094fe1b0>]}\n",
      "/app/lib/training.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_191757-gnt85jw8\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_FSDP_full_shard\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/gnt85jw8\n",
      "2025-11-03 19:17:58,167 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:17:58,168 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:17:58,168 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:17:58,168 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6921, 'grad_norm': 3.6685993671417236, 'learning_rate': 4.998680477667085e-05, 'epoch': 0.0}\n",
      "{'loss': 9.516, 'grad_norm': 5.000145435333252, 'learning_rate': 4.99703107475094e-05, 'epoch': 0.0}\n",
      "{'loss': 8.8406, 'grad_norm': 3.3742730617523193, 'learning_rate': 4.995381671834796e-05, 'epoch': 0.0}\n",
      "{'loss': 8.4638, 'grad_norm': 1.1423261165618896, 'learning_rate': 4.9937322689186515e-05, 'epoch': 0.0}\n",
      "{'loss': 8.1791, 'grad_norm': 1.057668685913086, 'learning_rate': 4.992082866002507e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0078, 'grad_norm': 0.9447437524795532, 'learning_rate': 4.990433463086363e-05, 'epoch': 0.0}\n",
      "{'loss': 7.8169, 'grad_norm': 0.9370400309562683, 'learning_rate': 4.988784060170218e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7288, 'grad_norm': 0.7628574371337891, 'learning_rate': 4.9871346572540744e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6897, 'grad_norm': 0.7042715549468994, 'learning_rate': 4.98548525433793e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5835, 'grad_norm': 0.6564656496047974, 'learning_rate': 4.983835851421785e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4374, 'grad_norm': 0.9480372071266174, 'learning_rate': 4.982186448505641e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3434, 'grad_norm': 0.7316735982894897, 'learning_rate': 4.9805370455894966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.215, 'grad_norm': 0.7324602007865906, 'learning_rate': 4.9788876426733526e-05, 'epoch': 0.0}\n",
      "{'loss': 7.148, 'grad_norm': 1.0855151414871216, 'learning_rate': 4.977238239757208e-05, 'epoch': 0.0}\n",
      "{'loss': 7.1185, 'grad_norm': 0.8853920102119446, 'learning_rate': 4.9755888368410634e-05, 'epoch': 0.0}\n",
      "{'loss': 7.0408, 'grad_norm': 1.0450620651245117, 'learning_rate': 4.9739394339249195e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9544, 'grad_norm': 0.9343412518501282, 'learning_rate': 4.972290031008775e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9447, 'grad_norm': 1.0575039386749268, 'learning_rate': 4.970640628092631e-05, 'epoch': 0.01}\n",
      "{'loss': 6.8225, 'grad_norm': 0.9116678237915039, 'learning_rate': 4.968991225176486e-05, 'epoch': 0.01}\n",
      "{'loss': 6.7294, 'grad_norm': 0.8690633773803711, 'learning_rate': 4.967341822260342e-05, 'epoch': 0.01}\n",
      "{'loss': 6.6447, 'grad_norm': 0.9283594489097595, 'learning_rate': 4.965692419344198e-05, 'epoch': 0.01}\n",
      "{'loss': 6.6454, 'grad_norm': 0.7795161008834839, 'learning_rate': 4.964043016428053e-05, 'epoch': 0.01}\n",
      "{'loss': 6.5507, 'grad_norm': 0.7706556916236877, 'learning_rate': 4.9623936135119085e-05, 'epoch': 0.01}\n",
      "{'loss': 6.5793, 'grad_norm': 0.8847196698188782, 'learning_rate': 4.9607442105957645e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3949, 'grad_norm': 0.7742242217063904, 'learning_rate': 4.95909480767962e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3275, 'grad_norm': 0.8789457678794861, 'learning_rate': 4.957445404763476e-05, 'epoch': 0.01}\n",
      "{'loss': 6.415, 'grad_norm': 1.0843669176101685, 'learning_rate': 4.9557960018473314e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3351, 'grad_norm': 0.9792466163635254, 'learning_rate': 4.954146598931187e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2407, 'grad_norm': 0.868530809879303, 'learning_rate': 4.952497196015043e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2357, 'grad_norm': 0.8396028280258179, 'learning_rate': 4.950847793098898e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1203, 'grad_norm': 0.8297266364097595, 'learning_rate': 4.949198390182754e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1877, 'grad_norm': 1.020311713218689, 'learning_rate': 4.9475489872666096e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0762, 'grad_norm': 0.9547456502914429, 'learning_rate': 4.945899584350465e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9777, 'grad_norm': 1.015638828277588, 'learning_rate': 4.944250181434321e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0532, 'grad_norm': 0.8578469753265381, 'learning_rate': 4.9426007785181764e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9901, 'grad_norm': 1.0535486936569214, 'learning_rate': 4.940951375602032e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8993, 'grad_norm': 0.9839456677436829, 'learning_rate': 4.939301972685888e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8188, 'grad_norm': 0.9774653315544128, 'learning_rate': 4.937652569769743e-05, 'epoch': 0.01}\n",
      "{'loss': 5.852, 'grad_norm': 0.9903869032859802, 'learning_rate': 4.936003166853599e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9061, 'grad_norm': 0.8830069899559021, 'learning_rate': 4.934353763937455e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8785, 'grad_norm': 1.028009295463562, 'learning_rate': 4.93270436102131e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6784, 'grad_norm': 0.927141547203064, 'learning_rate': 4.931054958105166e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6874, 'grad_norm': 0.8230565190315247, 'learning_rate': 4.9294055551890215e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6446, 'grad_norm': 0.8691125512123108, 'learning_rate': 4.9277561522728776e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6999, 'grad_norm': 0.8522950410842896, 'learning_rate': 4.926106749356733e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5506, 'grad_norm': 1.1313395500183105, 'learning_rate': 4.9244573464405884e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6533, 'grad_norm': 1.1135324239730835, 'learning_rate': 4.9228079435244444e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6115, 'grad_norm': 0.8765648007392883, 'learning_rate': 4.9211585406083e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5629, 'grad_norm': 0.9576177597045898, 'learning_rate': 4.919509137692156e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5143, 'grad_norm': 0.8940756916999817, 'learning_rate': 4.917859734776011e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4477, 'grad_norm': 0.9207929372787476, 'learning_rate': 4.9162103318598666e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5214, 'grad_norm': 0.8377256393432617, 'learning_rate': 4.914560928943723e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4691, 'grad_norm': 0.879462480545044, 'learning_rate': 4.912911526027578e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4624, 'grad_norm': 0.8720620274543762, 'learning_rate': 4.9112621231114334e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3831, 'grad_norm': 1.0094987154006958, 'learning_rate': 4.9096127201952895e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4051, 'grad_norm': 0.9029775261878967, 'learning_rate': 4.907963317279145e-05, 'epoch': 0.02}\n",
      "{'loss': 5.266, 'grad_norm': 0.9360216856002808, 'learning_rate': 4.906313914363001e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3264, 'grad_norm': 0.9177511930465698, 'learning_rate': 4.904664511446856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4237, 'grad_norm': 0.8080633282661438, 'learning_rate': 4.903015108530712e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2978, 'grad_norm': 0.8596243858337402, 'learning_rate': 4.901365705614568e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3269, 'grad_norm': 0.8231866359710693, 'learning_rate': 4.899716302698423e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3235, 'grad_norm': 0.8587458729743958, 'learning_rate': 4.898066899782279e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2969, 'grad_norm': 0.8600620031356812, 'learning_rate': 4.8964174968661346e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2262, 'grad_norm': 0.9906424283981323, 'learning_rate': 4.89476809394999e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1788, 'grad_norm': 1.0400859117507935, 'learning_rate': 4.893118691033846e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2516, 'grad_norm': 1.06809401512146, 'learning_rate': 4.8914692881177014e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1154, 'grad_norm': 0.9251386523246765, 'learning_rate': 4.8898198852015575e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1295, 'grad_norm': 0.864880383014679, 'learning_rate': 4.888170482285413e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0754, 'grad_norm': 0.9806000590324402, 'learning_rate': 4.886521079369268e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1088, 'grad_norm': 0.8003785014152527, 'learning_rate': 4.884871676453124e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1286, 'grad_norm': 0.941691517829895, 'learning_rate': 4.8832222735369797e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0937, 'grad_norm': 0.8454050421714783, 'learning_rate': 4.881572870620835e-05, 'epoch': 0.02}\n",
      "{'loss': 5.087, 'grad_norm': 1.2191836833953857, 'learning_rate': 4.879923467704691e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0873, 'grad_norm': 0.9526812434196472, 'learning_rate': 4.8782740647885465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.8473213911056519, 'learning_rate': 4.8766246618724025e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9983, 'grad_norm': 0.8750153183937073, 'learning_rate': 4.874975258956258e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9857, 'grad_norm': 0.8498088717460632, 'learning_rate': 4.873325856040113e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9406, 'grad_norm': 0.8415212631225586, 'learning_rate': 4.8716764531239694e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9118, 'grad_norm': 0.9755232334136963, 'learning_rate': 4.870027050207825e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9991, 'grad_norm': 0.9088501334190369, 'learning_rate': 4.868377647291681e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9774, 'grad_norm': 0.8449509143829346, 'learning_rate': 4.866728244375536e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9967, 'grad_norm': 0.8987208008766174, 'learning_rate': 4.8650788414593916e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9108, 'grad_norm': 0.8797411322593689, 'learning_rate': 4.8634294385432476e-05, 'epoch': 0.03}\n",
      "{'loss': 5.0022, 'grad_norm': 0.91493159532547, 'learning_rate': 4.861780035627103e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7599, 'grad_norm': 0.9096333980560303, 'learning_rate': 4.860130632710959e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8538, 'grad_norm': 0.8110061883926392, 'learning_rate': 4.8584812297948144e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8107, 'grad_norm': 0.9565772414207458, 'learning_rate': 4.85683182687867e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8736, 'grad_norm': 0.93027263879776, 'learning_rate': 4.855182423962526e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8245, 'grad_norm': 0.8339365124702454, 'learning_rate': 4.853533021046381e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8491, 'grad_norm': 0.8148435950279236, 'learning_rate': 4.8518836181302366e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8312, 'grad_norm': 0.9840726852416992, 'learning_rate': 4.850234215214093e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7811, 'grad_norm': 0.8608699440956116, 'learning_rate': 4.848584812297948e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8139, 'grad_norm': 0.9694945812225342, 'learning_rate': 4.846935409381804e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8743197917938232, 'learning_rate': 4.8452860064656595e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7797, 'grad_norm': 0.8785078525543213, 'learning_rate': 4.843636603549515e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8017, 'grad_norm': 1.0118496417999268, 'learning_rate': 4.841987200633371e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7144, 'grad_norm': 0.811053454875946, 'learning_rate': 4.8403377977172263e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7164, 'grad_norm': 0.8681556582450867, 'learning_rate': 4.8386883948010824e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7772, 'grad_norm': 0.9140840172767639, 'learning_rate': 4.837038991884938e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6992, 'grad_norm': 0.922348141670227, 'learning_rate': 4.835389588968793e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7163, 'grad_norm': 0.847205400466919, 'learning_rate': 4.833740186052649e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7066, 'grad_norm': 0.8906052708625793, 'learning_rate': 4.8320907831365046e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6443, 'grad_norm': 0.9738845825195312, 'learning_rate': 4.83044138022036e-05, 'epoch': 0.03}\n",
      "{'loss': 4.688, 'grad_norm': 0.9475910067558289, 'learning_rate': 4.828791977304216e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6353, 'grad_norm': 0.98466557264328, 'learning_rate': 4.8271425743880714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5942, 'grad_norm': 0.8803571462631226, 'learning_rate': 4.8254931714719275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6065, 'grad_norm': 0.9985359907150269, 'learning_rate': 4.823843768555783e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5969, 'grad_norm': 0.8307783603668213, 'learning_rate': 4.822194365639638e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5884, 'grad_norm': 0.959983766078949, 'learning_rate': 4.820544962723494e-05, 'epoch': 0.04}\n",
      "{'loss': 4.588, 'grad_norm': 0.9481823444366455, 'learning_rate': 4.81889555980735e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5532, 'grad_norm': 0.877038300037384, 'learning_rate': 4.817246156891206e-05, 'epoch': 0.04}\n",
      "{'loss': 4.598, 'grad_norm': 0.9016987085342407, 'learning_rate': 4.815596753975061e-05, 'epoch': 0.04}\n",
      "{'loss': 4.629, 'grad_norm': 0.8373340368270874, 'learning_rate': 4.8139473510589165e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5602, 'grad_norm': 0.9142613410949707, 'learning_rate': 4.8122979481427726e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5448, 'grad_norm': 0.7920125126838684, 'learning_rate': 4.810648545226628e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5553, 'grad_norm': 0.8880935907363892, 'learning_rate': 4.808999142310484e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5371, 'grad_norm': 0.8799552917480469, 'learning_rate': 4.8073497393943394e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5029, 'grad_norm': 0.8831263780593872, 'learning_rate': 4.805700336478195e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4856, 'grad_norm': 0.8571293950080872, 'learning_rate': 4.804050933562051e-05, 'epoch': 0.04}\n",
      "{'loss': 4.481, 'grad_norm': 0.9903404712677002, 'learning_rate': 4.802401530645906e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5663, 'grad_norm': 0.9656529426574707, 'learning_rate': 4.8007521277297616e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4727, 'grad_norm': 0.9243872761726379, 'learning_rate': 4.7991027248136176e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4545, 'grad_norm': 0.930270791053772, 'learning_rate': 4.797453321897473e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5288, 'grad_norm': 0.9271360635757446, 'learning_rate': 4.795803918981329e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5002, 'grad_norm': 0.9885192513465881, 'learning_rate': 4.7941545160651845e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4493, 'grad_norm': 1.0475068092346191, 'learning_rate': 4.79250511314904e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4769, 'grad_norm': 1.1011706590652466, 'learning_rate': 4.790855710232896e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.8717222809791565, 'learning_rate': 4.789206307316751e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4734, 'grad_norm': 0.8510183095932007, 'learning_rate': 4.7875569044006074e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5071, 'grad_norm': 0.9741666913032532, 'learning_rate': 4.785907501484463e-05, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 653/15157 [29:59<11:05:30,  2.75s/it]2025-11-03 19:48:02,003 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1802.23 c\n",
      "{'train_runtime': 1802.2373, 'train_samples_per_second': 1076.475, 'train_steps_per_second': 8.41, 'train_loss': 5.574778247681597, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 654/15157 [30:02<11:06:06,  2.76s/it]\n",
      "2025-11-03 19:48:02,157 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 19:48:02,186 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:45<00:00,  5.94it/s]\n",
      "2025-11-03 19:49:47,745 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard'  # 30117 MiB\n",
    "# FULL_SHARD | SHARD_GRAD_OP | NO_SHARD | HYBRID_SHARD | HYBRID_SHARD_ZERO2\n",
    "# \"full_shard\", \"shard_grad_op\", \"no_shard\", \"hybrid_shard\"\n",
    "    # --fsdp-cpu-offload \\ –≤—ã–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ CPU (–£–º–µ–Ω—å—à–∞–µ—Ç –ø–∞–º—è—Ç—å GPU, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ)\n",
    "    # --fsdp-activation-checkpointing \\  –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç (~30% –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö) (–ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34dea",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bd3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:37:23,190 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:37:59,475 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:37:59,734 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:37:59,734 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:37:59,735 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fc728012390>, <lib.callbacks.InspectCallback object at 0x7fc728012480>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 19:38:01,332 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:38:02,160 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:38:02,252 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:38:02,252 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:38:02,253 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f7f7c4b5a60>, <lib.callbacks.InspectCallback object at 0x7f7f7bcd2d20>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_193804-pgrtmpg4\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga2_FSDP_full_shard_v2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/pgrtmpg4\n",
      "2025-11-03 19:38:05,776 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:38:05,776 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 2, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:38:05,777 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:38:05,777 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "{'loss': 10.6444, 'grad_norm': 6.5313920974731445, 'learning_rate': 4.999340238833543e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6673, 'grad_norm': 3.2273755073547363, 'learning_rate': 4.9985155373754705e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9237, 'grad_norm': 3.645207405090332, 'learning_rate': 4.997690835917398e-05, 'epoch': 0.0}\n",
      "{'loss': 8.5601, 'grad_norm': 1.3648924827575684, 'learning_rate': 4.996866134459326e-05, 'epoch': 0.0}\n",
      "{'loss': 8.2867, 'grad_norm': 1.3722723722457886, 'learning_rate': 4.996041433001254e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0122, 'grad_norm': 1.1596969366073608, 'learning_rate': 4.995216731543181e-05, 'epoch': 0.0}\n",
      "{'loss': 7.9121, 'grad_norm': 0.9208484292030334, 'learning_rate': 4.9943920300851096e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7859, 'grad_norm': 0.8623735904693604, 'learning_rate': 4.993567328627037e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7536, 'grad_norm': 1.0223972797393799, 'learning_rate': 4.992742627168966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5907, 'grad_norm': 0.9671335220336914, 'learning_rate': 4.991917925710893e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6455, 'grad_norm': 1.3890206813812256, 'learning_rate': 4.991093224252821e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5677, 'grad_norm': 0.8959805369377136, 'learning_rate': 4.990268522794749e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4675, 'grad_norm': 1.2158596515655518, 'learning_rate': 4.9894438213366764e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3385, 'grad_norm': 1.021163821220398, 'learning_rate': 4.988619119878604e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3313, 'grad_norm': 1.2728992700576782, 'learning_rate': 4.9877944184205325e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2466, 'grad_norm': 1.184386968612671, 'learning_rate': 4.9869697169624595e-05, 'epoch': 0.0}\n",
      "{'loss': 7.226, 'grad_norm': 1.343267560005188, 'learning_rate': 4.986145015504388e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2143, 'grad_norm': 1.0885170698165894, 'learning_rate': 4.9853203140463156e-05, 'epoch': 0.0}\n",
      "{'loss': 7.186, 'grad_norm': 1.4330761432647705, 'learning_rate': 4.984495612588243e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9693, 'grad_norm': 1.0004644393920898, 'learning_rate': 4.983670911130171e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8406, 'grad_norm': 0.9873746633529663, 'learning_rate': 4.982846209672099e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9043, 'grad_norm': 0.9993258714675903, 'learning_rate': 4.982021508214027e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7424, 'grad_norm': 1.2514643669128418, 'learning_rate': 4.981196806755955e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8038, 'grad_norm': 1.064741611480713, 'learning_rate': 4.9803721052978824e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5962, 'grad_norm': 1.0138590335845947, 'learning_rate': 4.979547403839811e-05, 'epoch': 0.0}\n",
      "{'loss': 6.6334, 'grad_norm': 1.0394448041915894, 'learning_rate': 4.978722702381738e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5559, 'grad_norm': 1.0663433074951172, 'learning_rate': 4.977898000923666e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5233, 'grad_norm': 0.9639054536819458, 'learning_rate': 4.977073299465594e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5537, 'grad_norm': 0.9138987064361572, 'learning_rate': 4.9762485980075215e-05, 'epoch': 0.0}\n",
      "{'loss': 6.4483, 'grad_norm': 1.0875293016433716, 'learning_rate': 4.975423896549449e-05, 'epoch': 0.0}\n",
      "{'loss': 6.3772, 'grad_norm': 0.9539157748222351, 'learning_rate': 4.9745991950913776e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4714, 'grad_norm': 1.08989417552948, 'learning_rate': 4.9737744936333046e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3867, 'grad_norm': 0.9997318387031555, 'learning_rate': 4.972949792175233e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2735, 'grad_norm': 1.0559349060058594, 'learning_rate': 4.9721250907171607e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4121, 'grad_norm': 1.0875474214553833, 'learning_rate': 4.971300389259089e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2265, 'grad_norm': 1.2501388788223267, 'learning_rate': 4.970475687801016e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2732, 'grad_norm': 1.1039116382598877, 'learning_rate': 4.9696509863429444e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1098, 'grad_norm': 1.0326392650604248, 'learning_rate': 4.968826284884872e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0915, 'grad_norm': 1.060094952583313, 'learning_rate': 4.9680015834268e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1395, 'grad_norm': 1.1035743951797485, 'learning_rate': 4.9671768819687275e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0362, 'grad_norm': 1.1139506101608276, 'learning_rate': 4.966352180510656e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0501, 'grad_norm': 1.361629843711853, 'learning_rate': 4.965527479052583e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0638, 'grad_norm': 1.1630226373672485, 'learning_rate': 4.964702777594511e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0308, 'grad_norm': 1.0884668827056885, 'learning_rate': 4.963878076136439e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9067, 'grad_norm': 1.0826722383499146, 'learning_rate': 4.9630533746783666e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0173, 'grad_norm': 1.010643720626831, 'learning_rate': 4.962228673220294e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0348, 'grad_norm': 1.0483556985855103, 'learning_rate': 4.961403971762223e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9791, 'grad_norm': 1.2650882005691528, 'learning_rate': 4.9605792703041504e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9043, 'grad_norm': 1.1566051244735718, 'learning_rate': 4.959754568846078e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7435, 'grad_norm': 0.9544485211372375, 'learning_rate': 4.958929867388006e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8208, 'grad_norm': 1.2155009508132935, 'learning_rate': 4.958105165929934e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7167, 'grad_norm': 1.0473588705062866, 'learning_rate': 4.957280464471861e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9486, 'grad_norm': 1.1100026369094849, 'learning_rate': 4.9564557630137895e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7813, 'grad_norm': 1.1373603343963623, 'learning_rate': 4.955631061555717e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7994, 'grad_norm': 1.47096586227417, 'learning_rate': 4.954806360097645e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7695, 'grad_norm': 1.0950521230697632, 'learning_rate': 4.9539816586395726e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6905, 'grad_norm': 1.1174144744873047, 'learning_rate': 4.953156957181501e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6856, 'grad_norm': 1.051835298538208, 'learning_rate': 4.9523322557234286e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6396, 'grad_norm': 1.021802544593811, 'learning_rate': 4.951507554265356e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7389, 'grad_norm': 0.9070085287094116, 'learning_rate': 4.950682852807284e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6989, 'grad_norm': 1.0314044952392578, 'learning_rate': 4.9498581513492124e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4755, 'grad_norm': 0.9903817176818848, 'learning_rate': 4.9490334498911394e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6814, 'grad_norm': 1.0982643365859985, 'learning_rate': 4.948208748433068e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6682, 'grad_norm': 1.0954315662384033, 'learning_rate': 4.9473840469749954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5227, 'grad_norm': 1.4312175512313843, 'learning_rate': 4.946559345516923e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5659, 'grad_norm': 1.120503306388855, 'learning_rate': 4.945734644058851e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4352, 'grad_norm': 1.1130056381225586, 'learning_rate': 4.944909942600779e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4847, 'grad_norm': 1.056164026260376, 'learning_rate': 4.944085241142706e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5386, 'grad_norm': 1.1242332458496094, 'learning_rate': 4.9432605396846346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5591, 'grad_norm': 1.064380168914795, 'learning_rate': 4.942435838226562e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5723, 'grad_norm': 0.9790740013122559, 'learning_rate': 4.9416111367684906e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3953, 'grad_norm': 1.0622392892837524, 'learning_rate': 4.9407864353104176e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4267, 'grad_norm': 1.0991655588150024, 'learning_rate': 4.939961733852346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.382, 'grad_norm': 1.1173521280288696, 'learning_rate': 4.939137032394274e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3039, 'grad_norm': 1.047497034072876, 'learning_rate': 4.9383123309362014e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3389, 'grad_norm': 1.1278008222579956, 'learning_rate': 4.937487629478129e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3674, 'grad_norm': 0.9965013265609741, 'learning_rate': 4.9366629280200574e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3137, 'grad_norm': 1.1301568746566772, 'learning_rate': 4.9358382265619845e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5109, 'grad_norm': 0.9397605061531067, 'learning_rate': 4.935013525103913e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3577, 'grad_norm': 1.076525092124939, 'learning_rate': 4.9341888236458405e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4144, 'grad_norm': 1.0290720462799072, 'learning_rate': 4.933364122187768e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3834, 'grad_norm': 1.0764391422271729, 'learning_rate': 4.932539420729696e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2228, 'grad_norm': 1.1614902019500732, 'learning_rate': 4.931714719271624e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1843, 'grad_norm': 1.090602159500122, 'learning_rate': 4.930890017813552e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2014, 'grad_norm': 1.1497918367385864, 'learning_rate': 4.9300653163554796e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2625, 'grad_norm': 1.3094556331634521, 'learning_rate': 4.9292406148974073e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1546, 'grad_norm': 1.208540916442871, 'learning_rate': 4.928415913439336e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1954, 'grad_norm': 1.0286390781402588, 'learning_rate': 4.927591211981263e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2857, 'grad_norm': 1.0963047742843628, 'learning_rate': 4.926766510523191e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1918, 'grad_norm': 0.9852927923202515, 'learning_rate': 4.925941809065119e-05, 'epoch': 0.01}\n",
      "{'loss': 5.0279, 'grad_norm': 0.9711804389953613, 'learning_rate': 4.9251171076070465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1245, 'grad_norm': 1.0416266918182373, 'learning_rate': 4.924292406148974e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2345, 'grad_norm': 1.1587598323822021, 'learning_rate': 4.9234677046909025e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1673, 'grad_norm': 1.0791149139404297, 'learning_rate': 4.9226430032328295e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0438, 'grad_norm': 1.1286945343017578, 'learning_rate': 4.921818301774758e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2545, 'grad_norm': 1.1003646850585938, 'learning_rate': 4.9209936003166856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2145, 'grad_norm': 0.9384528994560242, 'learning_rate': 4.920168898858614e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9995, 'grad_norm': 0.9224302768707275, 'learning_rate': 4.919344197400541e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1453, 'grad_norm': 1.0209921598434448, 'learning_rate': 4.9185194959424694e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9835, 'grad_norm': 1.0382697582244873, 'learning_rate': 4.917694794484397e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0404, 'grad_norm': 1.004152536392212, 'learning_rate': 4.916870093026325e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9612, 'grad_norm': 1.1325302124023438, 'learning_rate': 4.9160453915682524e-05, 'epoch': 0.02}\n",
      "{'loss': 5.077, 'grad_norm': 0.9699444770812988, 'learning_rate': 4.915220690110181e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0896, 'grad_norm': 1.0638997554779053, 'learning_rate': 4.914395988652108e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9973, 'grad_norm': 1.1551052331924438, 'learning_rate': 4.913571287194036e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0661, 'grad_norm': 0.996765673160553, 'learning_rate': 4.912746585735964e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0973, 'grad_norm': 1.092122197151184, 'learning_rate': 4.911921884277892e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9443, 'grad_norm': 1.0544276237487793, 'learning_rate': 4.911097182819819e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8622, 'grad_norm': 1.2231124639511108, 'learning_rate': 4.9102724813617476e-05, 'epoch': 0.02}\n",
      "{'loss': 5.024, 'grad_norm': 1.041122317314148, 'learning_rate': 4.909447779903675e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9775, 'grad_norm': 1.089453101158142, 'learning_rate': 4.908623078445603e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9634, 'grad_norm': 1.0052860975265503, 'learning_rate': 4.907798376987531e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8447, 'grad_norm': 1.0101381540298462, 'learning_rate': 4.906973675529459e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8017, 'grad_norm': 1.0388035774230957, 'learning_rate': 4.906148974071386e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9357, 'grad_norm': 0.9926142692565918, 'learning_rate': 4.9053242726133144e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8476, 'grad_norm': 1.0412185192108154, 'learning_rate': 4.904499571155242e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0609, 'grad_norm': 1.0392355918884277, 'learning_rate': 4.90367486969717e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9396, 'grad_norm': 0.9341914653778076, 'learning_rate': 4.9028501682390975e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8953, 'grad_norm': 1.0489909648895264, 'learning_rate': 4.902025466781026e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8604, 'grad_norm': 1.180835485458374, 'learning_rate': 4.9012007653229536e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9404, 'grad_norm': 0.9948098063468933, 'learning_rate': 4.900376063864881e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8669, 'grad_norm': 1.0536701679229736, 'learning_rate': 4.899551362406809e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8946, 'grad_norm': 0.9966706037521362, 'learning_rate': 4.898726660948737e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8983, 'grad_norm': 1.0302015542984009, 'learning_rate': 4.897901959490664e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8993, 'grad_norm': 1.0842093229293823, 'learning_rate': 4.897077258032593e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8642, 'grad_norm': 1.1616989374160767, 'learning_rate': 4.8962525565745204e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8386, 'grad_norm': 0.9798850417137146, 'learning_rate': 4.895427855116448e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7661, 'grad_norm': 1.0196082592010498, 'learning_rate': 4.894603153658376e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7272, 'grad_norm': 1.090603232383728, 'learning_rate': 4.893778452200304e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7994, 'grad_norm': 1.0505355596542358, 'learning_rate': 4.892953750742231e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8151, 'grad_norm': 0.9707903861999512, 'learning_rate': 4.8921290492841595e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8455, 'grad_norm': 1.0490247011184692, 'learning_rate': 4.891304347826087e-05, 'epoch': 0.02}\n",
      "{'loss': 4.858, 'grad_norm': 0.9563772082328796, 'learning_rate': 4.8904796463680156e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5422, 'grad_norm': 1.062784194946289, 'learning_rate': 4.8896549449099426e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7993, 'grad_norm': 1.0139859914779663, 'learning_rate': 4.888830243451871e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6396, 'grad_norm': 1.046374797821045, 'learning_rate': 4.8880055419937986e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6918, 'grad_norm': 1.0056557655334473, 'learning_rate': 4.887180840535726e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6401, 'grad_norm': 1.157609224319458, 'learning_rate': 4.886356139077654e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6794, 'grad_norm': 1.0043175220489502, 'learning_rate': 4.8855314376195824e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7256, 'grad_norm': 1.1832711696624756, 'learning_rate': 4.8847067361615094e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7387, 'grad_norm': 0.9323859810829163, 'learning_rate': 4.883882034703438e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6963, 'grad_norm': 1.1491827964782715, 'learning_rate': 4.8830573332453655e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6466, 'grad_norm': 0.9812183976173401, 'learning_rate': 4.882232631787294e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7297, 'grad_norm': 0.992109477519989, 'learning_rate': 4.881407930329221e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6663, 'grad_norm': 1.2287836074829102, 'learning_rate': 4.880583228871149e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7133, 'grad_norm': 1.0841212272644043, 'learning_rate': 4.879758527413077e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6149, 'grad_norm': 0.9832476377487183, 'learning_rate': 4.8789338259550046e-05, 'epoch': 0.02}\n",
      "{'loss': 4.76, 'grad_norm': 0.9344678521156311, 'learning_rate': 4.878109124496932e-05, 'epoch': 0.02}\n",
      "{'loss': 4.704, 'grad_norm': 1.0734599828720093, 'learning_rate': 4.8772844230388607e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6975, 'grad_norm': 1.0407136678695679, 'learning_rate': 4.876459721580788e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6185, 'grad_norm': 1.1576107740402222, 'learning_rate': 4.875635020122716e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5762, 'grad_norm': 0.9878236651420593, 'learning_rate': 4.874810318664644e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5029, 'grad_norm': 1.0747416019439697, 'learning_rate': 4.8739856172065714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6683, 'grad_norm': 1.0084227323532104, 'learning_rate': 4.873160915748499e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6394, 'grad_norm': 1.0914009809494019, 'learning_rate': 4.8723362142904275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4652, 'grad_norm': 0.9519321322441101, 'learning_rate': 4.871511512832355e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5509, 'grad_norm': 1.2421261072158813, 'learning_rate': 4.870686811374283e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5087, 'grad_norm': 1.012853741645813, 'learning_rate': 4.8698621099162105e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5223, 'grad_norm': 0.998762845993042, 'learning_rate': 4.869037408458139e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6848, 'grad_norm': 1.1022077798843384, 'learning_rate': 4.868212707000066e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6026, 'grad_norm': 1.0309686660766602, 'learning_rate': 4.867388005541994e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5715, 'grad_norm': 1.0592811107635498, 'learning_rate': 4.866563304083922e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6757, 'grad_norm': 0.9432093501091003, 'learning_rate': 4.86573860262585e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5393, 'grad_norm': 1.0088406801223755, 'learning_rate': 4.8649139011677774e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5431, 'grad_norm': 0.9762718081474304, 'learning_rate': 4.864089199709706e-05, 'epoch': 0.03}\n",
      "{'loss': 4.505, 'grad_norm': 0.9821547269821167, 'learning_rate': 4.863264498251633e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5873, 'grad_norm': 1.0166200399398804, 'learning_rate': 4.862439796793561e-05, 'epoch': 0.03}\n",
      "{'loss': 4.674, 'grad_norm': 1.0620497465133667, 'learning_rate': 4.861615095335489e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3706, 'grad_norm': 1.0072818994522095, 'learning_rate': 4.860790393877417e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4114, 'grad_norm': 1.0058624744415283, 'learning_rate': 4.859965692419344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4734, 'grad_norm': 1.0765188932418823, 'learning_rate': 4.8591409909612726e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4653, 'grad_norm': 1.0471206903457642, 'learning_rate': 4.8583162895032e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4349, 'grad_norm': 1.1005496978759766, 'learning_rate': 4.857491588045128e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4313, 'grad_norm': 0.9662685990333557, 'learning_rate': 4.8566668865870556e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4832, 'grad_norm': 0.9477719068527222, 'learning_rate': 4.855842185128984e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5019, 'grad_norm': 1.0253028869628906, 'learning_rate': 4.855017483670911e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4575, 'grad_norm': 0.9929068684577942, 'learning_rate': 4.8541927822128394e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4436, 'grad_norm': 0.9480090141296387, 'learning_rate': 4.853368080754767e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4527, 'grad_norm': 0.9162320494651794, 'learning_rate': 4.852543379296695e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4905, 'grad_norm': 0.9413788914680481, 'learning_rate': 4.8517186778386225e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5195, 'grad_norm': 0.9664589762687683, 'learning_rate': 4.850893976380551e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4227, 'grad_norm': 0.9785865545272827, 'learning_rate': 4.8500692749224785e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4685, 'grad_norm': 1.0251414775848389, 'learning_rate': 4.849244573464406e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3794, 'grad_norm': 1.114363670349121, 'learning_rate': 4.848419872006334e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4262, 'grad_norm': 1.0935510396957397, 'learning_rate': 4.847595170548262e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4759, 'grad_norm': 0.9929882884025574, 'learning_rate': 4.846770469090189e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3377, 'grad_norm': 1.0329346656799316, 'learning_rate': 4.8459457676321176e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3363, 'grad_norm': 1.0098180770874023, 'learning_rate': 4.845121066174045e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4679, 'grad_norm': 1.0334932804107666, 'learning_rate': 4.844296364715973e-05, 'epoch': 0.03}\n",
      "{'loss': 4.374, 'grad_norm': 1.0937193632125854, 'learning_rate': 4.843471663257901e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4582, 'grad_norm': 1.0132368803024292, 'learning_rate': 4.842646961799829e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4351, 'grad_norm': 0.9898399114608765, 'learning_rate': 4.841822260341757e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2785, 'grad_norm': 0.9562243819236755, 'learning_rate': 4.8409975588836845e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4475, 'grad_norm': 0.9146280288696289, 'learning_rate': 4.840172857425612e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3932, 'grad_norm': 0.9829525351524353, 'learning_rate': 4.8393481559675405e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3242, 'grad_norm': 1.034329891204834, 'learning_rate': 4.8385234545094675e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3949, 'grad_norm': 0.9865344166755676, 'learning_rate': 4.837698753051396e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4569, 'grad_norm': 1.062667727470398, 'learning_rate': 4.8368740515933236e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2508, 'grad_norm': 1.0316967964172363, 'learning_rate': 4.836049350135251e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4239, 'grad_norm': 0.9955689907073975, 'learning_rate': 4.835224648677179e-05, 'epoch': 0.03}\n",
      "{'loss': 4.273, 'grad_norm': 1.014431357383728, 'learning_rate': 4.8343999472191073e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4695, 'grad_norm': 1.0195432901382446, 'learning_rate': 4.8335752457610344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3675, 'grad_norm': 1.0507800579071045, 'learning_rate': 4.832750544302963e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3484, 'grad_norm': 1.0973351001739502, 'learning_rate': 4.8319258428448904e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2405, 'grad_norm': 1.0506153106689453, 'learning_rate': 4.831101141386819e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3367, 'grad_norm': 1.0019389390945435, 'learning_rate': 4.830276439928746e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2619, 'grad_norm': 0.9787523150444031, 'learning_rate': 4.829451738470674e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4012, 'grad_norm': 1.0298384428024292, 'learning_rate': 4.828627037012602e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2744, 'grad_norm': 1.1903619766235352, 'learning_rate': 4.8278023355545295e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2926, 'grad_norm': 0.9904480576515198, 'learning_rate': 4.826977634096457e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2618, 'grad_norm': 0.9850509762763977, 'learning_rate': 4.8261529326383856e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2295, 'grad_norm': 0.9893596172332764, 'learning_rate': 4.8253282311803126e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2349, 'grad_norm': 1.012485146522522, 'learning_rate': 4.824503529722241e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2829, 'grad_norm': 1.0510729551315308, 'learning_rate': 4.823678828264169e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2644, 'grad_norm': 1.0484381914138794, 'learning_rate': 4.8228541268060964e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2433, 'grad_norm': 0.9664254188537598, 'learning_rate': 4.822029425348024e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2256, 'grad_norm': 0.985930860042572, 'learning_rate': 4.8212047238899524e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2789, 'grad_norm': 1.0419929027557373, 'learning_rate': 4.82038002243188e-05, 'epoch': 0.04}\n",
      "{'loss': 4.17, 'grad_norm': 0.9965826272964478, 'learning_rate': 4.819555320973808e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3219, 'grad_norm': 1.008651852607727, 'learning_rate': 4.8187306195157355e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3211, 'grad_norm': 1.0295435190200806, 'learning_rate': 4.817905918057664e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0926, 'grad_norm': 0.9688014388084412, 'learning_rate': 4.817081216599591e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2809, 'grad_norm': 0.9813153743743896, 'learning_rate': 4.816256515141519e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2389, 'grad_norm': 1.0399940013885498, 'learning_rate': 4.815431813683447e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3175, 'grad_norm': 0.9841344356536865, 'learning_rate': 4.8146071122253746e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2693, 'grad_norm': 1.0478426218032837, 'learning_rate': 4.813782410767302e-05, 'epoch': 0.04}\n",
      "{'loss': 4.245, 'grad_norm': 1.0579465627670288, 'learning_rate': 4.812957709309231e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2138, 'grad_norm': 0.9716281294822693, 'learning_rate': 4.812133007851158e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2554, 'grad_norm': 0.9244222640991211, 'learning_rate': 4.811308306393086e-05, 'epoch': 0.04}\n",
      "{'loss': 4.166, 'grad_norm': 0.9379757642745972, 'learning_rate': 4.810483604935014e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2282, 'grad_norm': 1.0778900384902954, 'learning_rate': 4.809658903476942e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2285, 'grad_norm': 0.9654243588447571, 'learning_rate': 4.808834202018869e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2957, 'grad_norm': 0.9799751043319702, 'learning_rate': 4.8080095005607975e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1134, 'grad_norm': 0.9998027682304382, 'learning_rate': 4.807184799102725e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1624, 'grad_norm': 0.9859493374824524, 'learning_rate': 4.806360097644653e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1954, 'grad_norm': 1.05072820186615, 'learning_rate': 4.8055353961865806e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2451, 'grad_norm': 1.0829925537109375, 'learning_rate': 4.804710694728509e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0853, 'grad_norm': 1.0532948970794678, 'learning_rate': 4.803885993270436e-05, 'epoch': 0.04}\n",
      "{'loss': 4.125, 'grad_norm': 0.9371650815010071, 'learning_rate': 4.803061291812364e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1703, 'grad_norm': 1.0054192543029785, 'learning_rate': 4.802236590354292e-05, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 1204/30314 [29:58<12:04:16,  1.49s/it]2025-11-03 20:08:07,778 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.44 c\n",
      "{'loss': 4.1741, 'grad_norm': 0.9618086218833923, 'learning_rate': 4.8014118888962204e-05, 'epoch': 0.04}\n",
      "{'train_runtime': 1800.4499, 'train_samples_per_second': 1077.543, 'train_steps_per_second': 16.837, 'train_loss': 5.22179103867147, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 1205/30314 [30:00<12:04:53,  1.49s/it]\n",
      "2025-11-03 20:08:07,901 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 20:08:07,930 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:44<00:00,  5.96it/s]\n",
      "2025-11-03 20:09:53,219 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v2'  # 30117 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29502 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 2 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72e493",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:52:13,847 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:52:13,938 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:52:13,938 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:52:13,939 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f94718b2270>, <lib.callbacks.InspectCallback object at 0x7f94718b2000>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "2025-11-03 19:52:16,567 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:52:16,835 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:52:16,835 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:52:16,837 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fd4f167e720>, <lib.callbacks.InspectCallback object at 0x7fd4f16c7c20>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_195216-o0r41zhk\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs8_ga4_FSDP_full_shard_v3\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/o0r41zhk\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:52:17,413 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:52:18,459 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "{'loss': 10.6419, 'grad_norm': 6.161344528198242, 'learning_rate': 4.999340238833543e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6551, 'grad_norm': 3.477644443511963, 'learning_rate': 4.9985155373754705e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9179, 'grad_norm': 3.2947916984558105, 'learning_rate': 4.997690835917398e-05, 'epoch': 0.0}\n",
      "{'loss': 8.5555, 'grad_norm': 1.495105266571045, 'learning_rate': 4.996866134459326e-05, 'epoch': 0.0}\n",
      "{'loss': 8.279, 'grad_norm': 1.198621392250061, 'learning_rate': 4.996041433001254e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0085, 'grad_norm': 1.3304463624954224, 'learning_rate': 4.995216731543181e-05, 'epoch': 0.0}\n",
      "{'loss': 7.908, 'grad_norm': 0.8468947410583496, 'learning_rate': 4.9943920300851096e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7852, 'grad_norm': 0.9712560176849365, 'learning_rate': 4.993567328627037e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7532, 'grad_norm': 0.8447855114936829, 'learning_rate': 4.992742627168966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5855, 'grad_norm': 1.0049594640731812, 'learning_rate': 4.991917925710893e-05, 'epoch': 0.0}\n",
      "{'loss': 7.633, 'grad_norm': 1.3147387504577637, 'learning_rate': 4.991093224252821e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5566, 'grad_norm': 1.072090983390808, 'learning_rate': 4.990268522794749e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4553, 'grad_norm': 1.2696805000305176, 'learning_rate': 4.9894438213366764e-05, 'epoch': 0.0}\n",
      "{'loss': 7.326, 'grad_norm': 0.961532473564148, 'learning_rate': 4.988619119878604e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3225, 'grad_norm': 1.1015723943710327, 'learning_rate': 4.9877944184205325e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2337, 'grad_norm': 1.3451743125915527, 'learning_rate': 4.9869697169624595e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2041, 'grad_norm': 1.4968640804290771, 'learning_rate': 4.986145015504388e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2032, 'grad_norm': 1.1569275856018066, 'learning_rate': 4.9853203140463156e-05, 'epoch': 0.0}\n",
      "{'loss': 7.1694, 'grad_norm': 1.3323944807052612, 'learning_rate': 4.984495612588243e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9561, 'grad_norm': 0.9584651589393616, 'learning_rate': 4.983670911130171e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8203, 'grad_norm': 0.9726276993751526, 'learning_rate': 4.982846209672099e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8941, 'grad_norm': 0.995424211025238, 'learning_rate': 4.982021508214027e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7298, 'grad_norm': 1.227007269859314, 'learning_rate': 4.981196806755955e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7842, 'grad_norm': 1.026127576828003, 'learning_rate': 4.9803721052978824e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5935, 'grad_norm': 0.9543346762657166, 'learning_rate': 4.979547403839811e-05, 'epoch': 0.0}\n",
      "{'loss': 6.6222, 'grad_norm': 1.0552695989608765, 'learning_rate': 4.978722702381738e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5549, 'grad_norm': 1.1391561031341553, 'learning_rate': 4.977898000923666e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5098, 'grad_norm': 0.9919469952583313, 'learning_rate': 4.977073299465594e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5447, 'grad_norm': 0.9068463444709778, 'learning_rate': 4.9762485980075215e-05, 'epoch': 0.0}\n",
      "{'loss': 6.4389, 'grad_norm': 1.1114656925201416, 'learning_rate': 4.975423896549449e-05, 'epoch': 0.0}\n",
      "{'loss': 6.3631, 'grad_norm': 0.9163604974746704, 'learning_rate': 4.9745991950913776e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4611, 'grad_norm': 1.1020753383636475, 'learning_rate': 4.9737744936333046e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3807, 'grad_norm': 1.000560998916626, 'learning_rate': 4.972949792175233e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2759, 'grad_norm': 1.0252231359481812, 'learning_rate': 4.9721250907171607e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4022, 'grad_norm': 1.074475646018982, 'learning_rate': 4.971300389259089e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2225, 'grad_norm': 1.1453893184661865, 'learning_rate': 4.970475687801016e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2648, 'grad_norm': 1.1735650300979614, 'learning_rate': 4.9696509863429444e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1069, 'grad_norm': 1.0566627979278564, 'learning_rate': 4.968826284884872e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0896, 'grad_norm': 1.074960470199585, 'learning_rate': 4.9680015834268e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1333, 'grad_norm': 1.1017637252807617, 'learning_rate': 4.9671768819687275e-05, 'epoch': 0.01}\n",
      "{'loss': 6.03, 'grad_norm': 1.1273071765899658, 'learning_rate': 4.966352180510656e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0433, 'grad_norm': 1.3091951608657837, 'learning_rate': 4.965527479052583e-05, 'epoch': 0.01}\n",
      "{'loss': 6.057, 'grad_norm': 1.1595767736434937, 'learning_rate': 4.964702777594511e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0316, 'grad_norm': 1.063482642173767, 'learning_rate': 4.963878076136439e-05, 'epoch': 0.01}\n",
      "{'loss': 5.901, 'grad_norm': 1.0704915523529053, 'learning_rate': 4.9630533746783666e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0156, 'grad_norm': 1.0088330507278442, 'learning_rate': 4.962228673220294e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0152, 'grad_norm': 1.058014154434204, 'learning_rate': 4.961403971762223e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9663, 'grad_norm': 1.2822837829589844, 'learning_rate': 4.9605792703041504e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9029, 'grad_norm': 1.131990909576416, 'learning_rate': 4.959754568846078e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7286, 'grad_norm': 0.9675118327140808, 'learning_rate': 4.958929867388006e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8197, 'grad_norm': 1.2069330215454102, 'learning_rate': 4.958105165929934e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7134, 'grad_norm': 1.0483969449996948, 'learning_rate': 4.957280464471861e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9452, 'grad_norm': 1.1181797981262207, 'learning_rate': 4.9564557630137895e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7756, 'grad_norm': 1.1422513723373413, 'learning_rate': 4.955631061555717e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8032, 'grad_norm': 1.4966661930084229, 'learning_rate': 4.954806360097645e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7696, 'grad_norm': 1.0849645137786865, 'learning_rate': 4.9539816586395726e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6892, 'grad_norm': 1.1216474771499634, 'learning_rate': 4.953156957181501e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6887, 'grad_norm': 1.0435843467712402, 'learning_rate': 4.9523322557234286e-05, 'epoch': 0.01}\n",
      "{'loss': 5.648, 'grad_norm': 1.041558027267456, 'learning_rate': 4.951507554265356e-05, 'epoch': 0.01}\n",
      "{'loss': 5.745, 'grad_norm': 0.9225491881370544, 'learning_rate': 4.950682852807284e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6975, 'grad_norm': 1.0287679433822632, 'learning_rate': 4.9498581513492124e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4825, 'grad_norm': 0.9838809967041016, 'learning_rate': 4.9490334498911394e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6692, 'grad_norm': 1.0883960723876953, 'learning_rate': 4.948208748433068e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6684, 'grad_norm': 1.1099557876586914, 'learning_rate': 4.9473840469749954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5461, 'grad_norm': 1.398963212966919, 'learning_rate': 4.946559345516923e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5691, 'grad_norm': 1.1353247165679932, 'learning_rate': 4.945734644058851e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4419, 'grad_norm': 1.103093147277832, 'learning_rate': 4.944909942600779e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4895, 'grad_norm': 1.0738216638565063, 'learning_rate': 4.944085241142706e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5304, 'grad_norm': 1.124693751335144, 'learning_rate': 4.9432605396846346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5634, 'grad_norm': 1.0684622526168823, 'learning_rate': 4.942435838226562e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5733, 'grad_norm': 0.9761757254600525, 'learning_rate': 4.9416111367684906e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3908, 'grad_norm': 1.0903525352478027, 'learning_rate': 4.9407864353104176e-05, 'epoch': 0.01}\n",
      "{'loss': 5.431, 'grad_norm': 1.1220921277999878, 'learning_rate': 4.939961733852346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3783, 'grad_norm': 1.1272591352462769, 'learning_rate': 4.939137032394274e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3062, 'grad_norm': 1.0519825220108032, 'learning_rate': 4.9383123309362014e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3408, 'grad_norm': 1.1458935737609863, 'learning_rate': 4.937487629478129e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3559, 'grad_norm': 1.0003204345703125, 'learning_rate': 4.9366629280200574e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3134, 'grad_norm': 1.1891167163848877, 'learning_rate': 4.9358382265619845e-05, 'epoch': 0.01}\n",
      "{'loss': 5.513, 'grad_norm': 0.9363975524902344, 'learning_rate': 4.935013525103913e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3568, 'grad_norm': 1.0879229307174683, 'learning_rate': 4.9341888236458405e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4212, 'grad_norm': 1.0583535432815552, 'learning_rate': 4.933364122187768e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3783, 'grad_norm': 1.1007628440856934, 'learning_rate': 4.932539420729696e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2266, 'grad_norm': 1.1876333951950073, 'learning_rate': 4.931714719271624e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1794, 'grad_norm': 1.1003211736679077, 'learning_rate': 4.930890017813552e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2109, 'grad_norm': 1.1900625228881836, 'learning_rate': 4.9300653163554796e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2586, 'grad_norm': 1.3015501499176025, 'learning_rate': 4.9292406148974073e-05, 'epoch': 0.01}\n",
      "{'loss': 5.151, 'grad_norm': 1.2091796398162842, 'learning_rate': 4.928415913439336e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1944, 'grad_norm': 1.0244771242141724, 'learning_rate': 4.927591211981263e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2759, 'grad_norm': 1.1053807735443115, 'learning_rate': 4.926766510523191e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1877, 'grad_norm': 0.9803803563117981, 'learning_rate': 4.925941809065119e-05, 'epoch': 0.01}\n",
      "{'loss': 5.03, 'grad_norm': 1.0061402320861816, 'learning_rate': 4.9251171076070465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1323, 'grad_norm': 1.0581709146499634, 'learning_rate': 4.924292406148974e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2329, 'grad_norm': 1.1692291498184204, 'learning_rate': 4.9234677046909025e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1678, 'grad_norm': 1.0997494459152222, 'learning_rate': 4.9226430032328295e-05, 'epoch': 0.02}\n",
      "{'loss': 5.053, 'grad_norm': 1.1355148553848267, 'learning_rate': 4.921818301774758e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2467, 'grad_norm': 1.0697600841522217, 'learning_rate': 4.9209936003166856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2164, 'grad_norm': 0.947257936000824, 'learning_rate': 4.920168898858614e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0045, 'grad_norm': 0.9384860992431641, 'learning_rate': 4.919344197400541e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1391, 'grad_norm': 1.0149863958358765, 'learning_rate': 4.9185194959424694e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9973, 'grad_norm': 1.0488492250442505, 'learning_rate': 4.917694794484397e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0235, 'grad_norm': 1.0280767679214478, 'learning_rate': 4.916870093026325e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9668, 'grad_norm': 1.1263558864593506, 'learning_rate': 4.9160453915682524e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0763, 'grad_norm': 0.9636012315750122, 'learning_rate': 4.915220690110181e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0829, 'grad_norm': 1.0556645393371582, 'learning_rate': 4.914395988652108e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0027, 'grad_norm': 1.1828125715255737, 'learning_rate': 4.913571287194036e-05, 'epoch': 0.02}\n",
      "{'loss': 5.07, 'grad_norm': 1.0107133388519287, 'learning_rate': 4.912746585735964e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0952, 'grad_norm': 1.0630176067352295, 'learning_rate': 4.911921884277892e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9512, 'grad_norm': 1.0486544370651245, 'learning_rate': 4.911097182819819e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8643, 'grad_norm': 1.1907598972320557, 'learning_rate': 4.9102724813617476e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0203, 'grad_norm': 1.0592553615570068, 'learning_rate': 4.909447779903675e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9779, 'grad_norm': 1.0902537107467651, 'learning_rate': 4.908623078445603e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9587, 'grad_norm': 1.0077029466629028, 'learning_rate': 4.907798376987531e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8344, 'grad_norm': 1.0105639696121216, 'learning_rate': 4.906973675529459e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8012, 'grad_norm': 1.0242735147476196, 'learning_rate': 4.906148974071386e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9231, 'grad_norm': 0.9816271662712097, 'learning_rate': 4.9053242726133144e-05, 'epoch': 0.02}\n",
      "{'loss': 4.835, 'grad_norm': 1.0263639688491821, 'learning_rate': 4.904499571155242e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0517, 'grad_norm': 1.0459864139556885, 'learning_rate': 4.90367486969717e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9445, 'grad_norm': 0.9311509132385254, 'learning_rate': 4.9028501682390975e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8924, 'grad_norm': 1.0497759580612183, 'learning_rate': 4.902025466781026e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8646, 'grad_norm': 1.2211323976516724, 'learning_rate': 4.9012007653229536e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9415, 'grad_norm': 1.0044968128204346, 'learning_rate': 4.900376063864881e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8729, 'grad_norm': 1.0624114274978638, 'learning_rate': 4.899551362406809e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8974, 'grad_norm': 1.0089800357818604, 'learning_rate': 4.898726660948737e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8938, 'grad_norm': 1.0398837327957153, 'learning_rate': 4.897901959490664e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9006, 'grad_norm': 1.0911967754364014, 'learning_rate': 4.897077258032593e-05, 'epoch': 0.02}\n",
      "{'loss': 4.849, 'grad_norm': 1.138630747795105, 'learning_rate': 4.8962525565745204e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8407, 'grad_norm': 0.9779117703437805, 'learning_rate': 4.895427855116448e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7744, 'grad_norm': 1.0078010559082031, 'learning_rate': 4.894603153658376e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7173, 'grad_norm': 1.0831915140151978, 'learning_rate': 4.893778452200304e-05, 'epoch': 0.02}\n",
      "{'loss': 4.797, 'grad_norm': 1.0523992776870728, 'learning_rate': 4.892953750742231e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8212, 'grad_norm': 0.9763866662979126, 'learning_rate': 4.8921290492841595e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8439, 'grad_norm': 1.0647414922714233, 'learning_rate': 4.891304347826087e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8632, 'grad_norm': 0.9521463513374329, 'learning_rate': 4.8904796463680156e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5346, 'grad_norm': 1.0839732885360718, 'learning_rate': 4.8896549449099426e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7985, 'grad_norm': 1.0150030851364136, 'learning_rate': 4.888830243451871e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6397, 'grad_norm': 1.0470433235168457, 'learning_rate': 4.8880055419937986e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6956, 'grad_norm': 0.9974631667137146, 'learning_rate': 4.887180840535726e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6435, 'grad_norm': 1.139997959136963, 'learning_rate': 4.886356139077654e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6754, 'grad_norm': 1.0230571031570435, 'learning_rate': 4.8855314376195824e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7319, 'grad_norm': 1.1717771291732788, 'learning_rate': 4.8847067361615094e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7383, 'grad_norm': 0.9462061524391174, 'learning_rate': 4.883882034703438e-05, 'epoch': 0.02}\n",
      "{'loss': 4.703, 'grad_norm': 1.1449016332626343, 'learning_rate': 4.8830573332453655e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6395, 'grad_norm': 0.9956967234611511, 'learning_rate': 4.882232631787294e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7279, 'grad_norm': 0.99331134557724, 'learning_rate': 4.881407930329221e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6618, 'grad_norm': 1.2324771881103516, 'learning_rate': 4.880583228871149e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7081, 'grad_norm': 1.0728100538253784, 'learning_rate': 4.879758527413077e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6113, 'grad_norm': 0.9720578193664551, 'learning_rate': 4.8789338259550046e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7627, 'grad_norm': 0.9374616146087646, 'learning_rate': 4.878109124496932e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7048, 'grad_norm': 1.047967791557312, 'learning_rate': 4.8772844230388607e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6989, 'grad_norm': 1.0282737016677856, 'learning_rate': 4.876459721580788e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6158, 'grad_norm': 1.1517817974090576, 'learning_rate': 4.875635020122716e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5737, 'grad_norm': 0.9916568994522095, 'learning_rate': 4.874810318664644e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5124, 'grad_norm': 1.0642269849777222, 'learning_rate': 4.8739856172065714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6655, 'grad_norm': 0.9978151917457581, 'learning_rate': 4.873160915748499e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6361, 'grad_norm': 1.1204804182052612, 'learning_rate': 4.8723362142904275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4643, 'grad_norm': 0.9572274684906006, 'learning_rate': 4.871511512832355e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5537, 'grad_norm': 1.232858657836914, 'learning_rate': 4.870686811374283e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5102, 'grad_norm': 1.0248379707336426, 'learning_rate': 4.8698621099162105e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5215, 'grad_norm': 1.0052692890167236, 'learning_rate': 4.869037408458139e-05, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 795/30314 [29:59<18:39:46,  2.28s/it]2025-11-03 20:22:21,437 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.56 c\n",
      "{'train_runtime': 1801.5717, 'train_samples_per_second': 1076.872, 'train_steps_per_second': 16.826, 'train_loss': 5.665090173932176, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 796/30314 [30:01<18:33:27,  2.26s/it]\n",
      "2025-11-03 20:22:21,563 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 20:22:21,578 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:45<00:00,  5.95it/s]\n",
      "2025-11-03 20:24:07,061 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 20:24:07,061 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.5717, 'train_samples_per_second': 1076.872, 'train_steps_per_second': 16.826, 'total_flos': 6.713499068871475e+16, 'train_loss': 5.665090173932176, 'epoch': 0.026258927540534747}, 'eval': {'eval_loss': 5.252961158752441, 'eval_runtime': 105.4808, 'eval_samples_per_second': 47.402, 'eval_steps_per_second': 5.925, 'epoch': 0.026258927540534747}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs8_ga4_FSDP_full_shard_v3\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/o0r41zhk\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_195216-o0r41zhk/logs\u001b[0m\n",
      "[rank0]:[W1103 20:24:10.707600155 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v3'  # 20817 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 8 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4100d43",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f29672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 20:30:35,497 [INFO] __main__: ============================================================\n",
      "2025-11-03 20:30:35,497 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 20:30:35,497 [INFO] __main__: ============================================================\n",
      "2025-11-03 20:30:35,804 [INFO] __main__: ============================================================\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: ============================================================\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: World size: 2\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 20:30:35,805 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   cpu_offload: True\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   activation_checkpointing: True\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 20:30:35,805 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 20:31:10,468 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 20:31:10,731 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap offload\n",
      "2025-11-03 20:31:10,731 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True, 'fsdp_activation_checkpointing': True}\n",
      "2025-11-03 20:31:10,733 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'activation_checkpointing': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=128,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fe89f45b050>, <lib.callbacks.InspectCallback object at 0x7fe89f4e6e70>]}\n",
      "2025-11-03 20:31:10,733 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 20:31:11,213 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 20:31:11,300 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap offload\n",
      "2025-11-03 20:31:11,300 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True, 'fsdp_activation_checkpointing': True}\n",
      "2025-11-03 20:31:11,301 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'activation_checkpointing': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=128,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f58c0632ba0>, <lib.callbacks.InspectCallback object at 0x7f58c0632a80>]}\n",
      "2025-11-03 20:31:11,301 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 20:31:12,277 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-03 20:31:12,281 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 20:31:12,879 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_203113-t0e927eq\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs128_ga1_FSDP_full_shard_v4\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/t0e927eq\n",
      "2025-11-03 20:31:14,917 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 20:31:14,917 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 128, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 1, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 20:31:14,918 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 20:31:14,918 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 20:31:18,103 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 20:31:18,103 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 20:31:18,103 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 20:31:18,103 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-03 20:31:18,140 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 20:31:18,140 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 20:31:18,140 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 20:31:18,141 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "{'loss': 10.7196, 'grad_norm': 5.8811354637146, 'learning_rate': 4.9973611294366015e-05, 'epoch': 0.0}\n",
      "{'loss': 9.5826, 'grad_norm': 5.411014080047607, 'learning_rate': 4.9940625412323525e-05, 'epoch': 0.0}\n",
      "{'loss': 8.8754, 'grad_norm': 2.0323328971862793, 'learning_rate': 4.990763953028104e-05, 'epoch': 0.0}\n",
      "{'loss': 8.4714, 'grad_norm': 2.5439395904541016, 'learning_rate': 4.987465364823855e-05, 'epoch': 0.0}\n",
      "{'loss': 8.2294, 'grad_norm': 1.0855417251586914, 'learning_rate': 4.984166776619607e-05, 'epoch': 0.0}\n",
      "{'loss': 7.987, 'grad_norm': 1.5545531511306763, 'learning_rate': 4.980868188415359e-05, 'epoch': 0.0}\n",
      "{'loss': 7.8322, 'grad_norm': 1.0781769752502441, 'learning_rate': 4.97756960021111e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7822, 'grad_norm': 2.1946916580200195, 'learning_rate': 4.9742710120068616e-05, 'epoch': 0.01}\n",
      "{'loss': 7.7635, 'grad_norm': 0.7909306883811951, 'learning_rate': 4.9709724238026126e-05, 'epoch': 0.01}\n",
      "{'loss': 7.6925, 'grad_norm': 1.2087503671646118, 'learning_rate': 4.9676738355983644e-05, 'epoch': 0.01}\n",
      "{'loss': 7.6273, 'grad_norm': 1.3507664203643799, 'learning_rate': 4.964375247394116e-05, 'epoch': 0.01}\n",
      "{'loss': 7.6002, 'grad_norm': 1.1137768030166626, 'learning_rate': 4.961076659189867e-05, 'epoch': 0.01}\n",
      "{'loss': 7.4865, 'grad_norm': 1.064537763595581, 'learning_rate': 4.957778070985619e-05, 'epoch': 0.01}\n",
      "{'loss': 7.5039, 'grad_norm': 1.2147811651229858, 'learning_rate': 4.95447948278137e-05, 'epoch': 0.01}\n",
      "{'loss': 7.4284, 'grad_norm': 1.06865394115448, 'learning_rate': 4.951180894577121e-05, 'epoch': 0.01}\n",
      "{'loss': 7.3549, 'grad_norm': 0.8905453681945801, 'learning_rate': 4.947882306372873e-05, 'epoch': 0.01}\n",
      "{'loss': 7.2773, 'grad_norm': 0.9353383779525757, 'learning_rate': 4.944583718168624e-05, 'epoch': 0.01}\n",
      "{'loss': 7.2682, 'grad_norm': 0.9451626539230347, 'learning_rate': 4.9412851299643755e-05, 'epoch': 0.01}\n",
      "{'loss': 7.1409, 'grad_norm': 1.0059936046600342, 'learning_rate': 4.9379865417601265e-05, 'epoch': 0.01}\n",
      "{'loss': 7.1488, 'grad_norm': 1.2008447647094727, 'learning_rate': 4.934687953555878e-05, 'epoch': 0.01}\n",
      "{'loss': 7.0717, 'grad_norm': 1.00874662399292, 'learning_rate': 4.931389365351629e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9854, 'grad_norm': 1.070094347000122, 'learning_rate': 4.928090777147381e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9475, 'grad_norm': 1.374938726425171, 'learning_rate': 4.924792188943133e-05, 'epoch': 0.02}\n",
      "{'loss': 6.9463, 'grad_norm': 1.0770304203033447, 'learning_rate': 4.921493600738884e-05, 'epoch': 0.02}\n",
      "{'loss': 6.8789, 'grad_norm': 1.8278350830078125, 'learning_rate': 4.9181950125346356e-05, 'epoch': 0.02}\n",
      "{'loss': 6.8479, 'grad_norm': 1.338284969329834, 'learning_rate': 4.9148964243303866e-05, 'epoch': 0.02}\n",
      "{'loss': 6.8072, 'grad_norm': 1.2964975833892822, 'learning_rate': 4.9115978361261383e-05, 'epoch': 0.02}\n",
      "{'loss': 6.7516, 'grad_norm': 1.342871069908142, 'learning_rate': 4.90829924792189e-05, 'epoch': 0.02}\n",
      "{'loss': 6.6718, 'grad_norm': 1.4925458431243896, 'learning_rate': 4.905000659717641e-05, 'epoch': 0.02}\n",
      "{'loss': 6.7061, 'grad_norm': 1.6354202032089233, 'learning_rate': 4.901702071513393e-05, 'epoch': 0.02}\n",
      "{'loss': 6.6724, 'grad_norm': 1.4518709182739258, 'learning_rate': 4.898403483309144e-05, 'epoch': 0.02}\n",
      "{'loss': 6.6281, 'grad_norm': 1.7858538627624512, 'learning_rate': 4.8951048951048956e-05, 'epoch': 0.02}\n",
      "{'loss': 6.5892, 'grad_norm': 1.9080266952514648, 'learning_rate': 4.891806306900647e-05, 'epoch': 0.02}\n",
      "{'loss': 6.5206, 'grad_norm': 2.0387465953826904, 'learning_rate': 4.8885077186963984e-05, 'epoch': 0.02}\n",
      "{'loss': 6.4855, 'grad_norm': 3.4442427158355713, 'learning_rate': 4.8852091304921495e-05, 'epoch': 0.02}\n",
      "{'loss': 6.5099, 'grad_norm': 1.9530606269836426, 'learning_rate': 4.8819105422879005e-05, 'epoch': 0.02}\n",
      "{'loss': 6.4729, 'grad_norm': 2.4410507678985596, 'learning_rate': 4.878611954083652e-05, 'epoch': 0.02}\n",
      "{'loss': 6.4496, 'grad_norm': 1.5659639835357666, 'learning_rate': 4.875313365879404e-05, 'epoch': 0.03}\n",
      "{'loss': 6.4333, 'grad_norm': 6.11972713470459, 'learning_rate': 4.872014777675155e-05, 'epoch': 0.03}\n",
      "{'loss': 6.4182, 'grad_norm': 1.8951749801635742, 'learning_rate': 4.868716189470907e-05, 'epoch': 0.03}\n",
      "{'loss': 6.4539, 'grad_norm': 3.457775354385376, 'learning_rate': 4.865417601266658e-05, 'epoch': 0.03}\n",
      "{'loss': 6.4371, 'grad_norm': 4.580352783203125, 'learning_rate': 4.8621190130624096e-05, 'epoch': 0.03}\n",
      "{'loss': 6.3682, 'grad_norm': 2.840243101119995, 'learning_rate': 4.8588204248581606e-05, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 218/7579 [29:56<16:40:45,  8.16s/it]2025-11-03 21:01:22,854 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1804.71 c\n",
      "{'train_runtime': 1804.7246, 'train_samples_per_second': 1074.991, 'train_steps_per_second': 4.2, 'train_loss': 7.234573294582977, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 219/7579 [30:04<16:50:51,  8.24s/it]\n",
      "2025-11-03 21:01:23,162 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 21:01:23,213 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [05:06<00:00,  2.04it/s]\n",
      "2025-11-03 21:06:30,959 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 21:06:30,960 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1804.7246, 'train_samples_per_second': 1074.991, 'train_steps_per_second': 4.2, 'total_flos': 7.388222593381171e+16, 'train_loss': 7.234573294582977, 'epoch': 0.028895632669217575}, 'eval': {'eval_loss': 6.852046966552734, 'eval_runtime': 307.7438, 'eval_samples_per_second': 16.247, 'eval_steps_per_second': 2.031, 'epoch': 0.028895632669217575}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs128_ga1_FSDP_full_shard_v4\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/t0e927eq\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_203113-t0e927eq/logs\u001b[0m\n",
      "[rank0]:[W1103 21:06:33.660917211 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v4'  # 61781 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29502 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --fsdp-cpu-offload \\\n",
    "    --fsdp-activation-checkpointing \\\n",
    "    --bf16 \\\n",
    "    --batch-size 128 \\\n",
    "    --grad-accum 1 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc69d4e",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard_v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91097c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-04 11:46:58,474 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,474 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 11:46:58,474 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,475 [INFO] __main__: World size: 4\n",
      "2025-11-04 11:46:58,475 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-04 11:46:58,475 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-04 11:46:58,477 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-04 11:46:58,477 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "2025-11-04 11:46:58,649 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,649 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 11:46:58,649 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,734 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,734 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 11:46:58,734 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,759 [INFO] __main__: ============================================================\n",
      "2025-11-04 11:46:58,759 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 11:46:58,759 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-04 11:47:34,538 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 11:47:34,631 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 11:47:34,631 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 11:47:34,632 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f62acd24e00>, <lib.callbacks.InspectCallback object at 0x7f62ada94470>]}\n",
      "2025-11-04 11:47:34,632 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-04 11:47:34,786 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-04 11:47:34,853 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 11:47:35,144 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 11:47:35,144 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 11:47:35,145 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=2,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f5d65afaf30>, <lib.callbacks.InspectCallback object at 0x7f5d65afac00>]}\n",
      "2025-11-04 11:47:35,145 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 11:47:35,192 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 11:47:35,192 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 11:47:35,193 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f02088f7950>, <lib.callbacks.InspectCallback object at 0x7f02088d2b70>]}\n",
      "2025-11-04 11:47:35,193 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-04 11:47:36,343 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 11:47:36,628 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-04 11:47:36,882 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 11:47:36,885 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 11:47:36,886 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 11:47:36,887 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 11:47:36,888 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=3,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f7c51b66ff0>, <lib.callbacks.InspectCallback object at 0x7f7c51b67050>]}\n",
      "2025-11-04 11:47:36,888 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "2025-11-04 11:47:37,012 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 11:47:37,018 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251104_114737-ghstkogd\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_FSDP_full_shard_v5\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ghstkogd\n",
      "2025-11-04 11:47:38,340 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-04 11:47:38,341 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': True, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-04 11:47:38,341 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-04 11:47:38,341 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-04 11:47:38,552 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 11:47:38,555 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-04 11:47:40,133 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 11:47:40,133 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 11:47:40,133 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 11:47:40,133 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "2025-11-04 11:47:40,134 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 11:47:40,134 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 11:47:40,134 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 11:47:40,134 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-04 11:47:40,135 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 11:47:40,135 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 11:47:40,136 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 11:47:40,136 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "2025-11-04 11:47:40,142 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 11:47:40,142 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 11:47:40,142 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 11:47:40,143 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "  0%|          | 0/7579 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "{'loss': 10.8027, 'grad_norm': 4.586697101593018, 'learning_rate': 4.9973611294366015e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6163, 'grad_norm': 6.306763172149658, 'learning_rate': 4.9940625412323525e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9183, 'grad_norm': 5.24174690246582, 'learning_rate': 4.990763953028104e-05, 'epoch': 0.0}\n",
      "{'loss': 8.4913, 'grad_norm': 1.1347461938858032, 'learning_rate': 4.987465364823855e-05, 'epoch': 0.0}\n",
      "{'loss': 8.2258, 'grad_norm': 1.0515797138214111, 'learning_rate': 4.984166776619607e-05, 'epoch': 0.0}\n",
      "{'loss': 7.9461, 'grad_norm': 0.7152884602546692, 'learning_rate': 4.980868188415359e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7576, 'grad_norm': 0.5936956405639648, 'learning_rate': 4.97756960021111e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6796, 'grad_norm': 0.5643001794815063, 'learning_rate': 4.9742710120068616e-05, 'epoch': 0.01}\n",
      "{'loss': 7.6208, 'grad_norm': 1.2450945377349854, 'learning_rate': 4.9709724238026126e-05, 'epoch': 0.01}\n",
      "{'loss': 7.5114, 'grad_norm': 1.365601658821106, 'learning_rate': 4.9676738355983644e-05, 'epoch': 0.01}\n",
      "{'loss': 7.4076, 'grad_norm': 1.1779508590698242, 'learning_rate': 4.964375247394116e-05, 'epoch': 0.01}\n",
      "{'loss': 7.3456, 'grad_norm': 0.8417869806289673, 'learning_rate': 4.961076659189867e-05, 'epoch': 0.01}\n",
      "{'loss': 7.1598, 'grad_norm': 0.7008211612701416, 'learning_rate': 4.957778070985619e-05, 'epoch': 0.01}\n",
      "{'loss': 7.1688, 'grad_norm': 0.6059408783912659, 'learning_rate': 4.95447948278137e-05, 'epoch': 0.01}\n",
      "{'loss': 7.043, 'grad_norm': 0.6711352467536926, 'learning_rate': 4.951180894577121e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9435, 'grad_norm': 0.6640428900718689, 'learning_rate': 4.947882306372873e-05, 'epoch': 0.01}\n",
      "{'loss': 6.8175, 'grad_norm': 0.6854327321052551, 'learning_rate': 4.944583718168624e-05, 'epoch': 0.01}\n",
      "{'loss': 6.8025, 'grad_norm': 0.8130162954330444, 'learning_rate': 4.9412851299643755e-05, 'epoch': 0.01}\n",
      "{'loss': 6.631, 'grad_norm': 0.6197493672370911, 'learning_rate': 4.9379865417601265e-05, 'epoch': 0.01}\n",
      "{'loss': 6.6165, 'grad_norm': 0.6432830095291138, 'learning_rate': 4.934687953555878e-05, 'epoch': 0.01}\n",
      "{'loss': 6.5116, 'grad_norm': 0.8761367797851562, 'learning_rate': 4.931389365351629e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3925, 'grad_norm': 0.885035514831543, 'learning_rate': 4.928090777147381e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3334, 'grad_norm': 0.7607801556587219, 'learning_rate': 4.924792188943133e-05, 'epoch': 0.02}\n",
      "{'loss': 6.3173, 'grad_norm': 0.9142106175422668, 'learning_rate': 4.921493600738884e-05, 'epoch': 0.02}\n",
      "{'loss': 6.2145, 'grad_norm': 1.0596824884414673, 'learning_rate': 4.9181950125346356e-05, 'epoch': 0.02}\n",
      "{'loss': 6.1553, 'grad_norm': 0.8018634915351868, 'learning_rate': 4.9148964243303866e-05, 'epoch': 0.02}\n",
      "{'loss': 6.1139, 'grad_norm': 0.8045076131820679, 'learning_rate': 4.9115978361261383e-05, 'epoch': 0.02}\n",
      "{'loss': 6.0358, 'grad_norm': 0.7795698046684265, 'learning_rate': 4.90829924792189e-05, 'epoch': 0.02}\n",
      "{'loss': 5.9301, 'grad_norm': 0.6823263764381409, 'learning_rate': 4.905000659717641e-05, 'epoch': 0.02}\n",
      "{'loss': 5.9706, 'grad_norm': 0.8017715811729431, 'learning_rate': 4.901702071513393e-05, 'epoch': 0.02}\n",
      "{'loss': 5.9313, 'grad_norm': 0.7702471017837524, 'learning_rate': 4.898403483309144e-05, 'epoch': 0.02}\n",
      "{'loss': 5.862, 'grad_norm': 0.7888495922088623, 'learning_rate': 4.8951048951048956e-05, 'epoch': 0.02}\n",
      "{'loss': 5.8029, 'grad_norm': 0.9060655832290649, 'learning_rate': 4.891806306900647e-05, 'epoch': 0.02}\n",
      "{'loss': 5.7116, 'grad_norm': 0.8022943139076233, 'learning_rate': 4.8885077186963984e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6732, 'grad_norm': 0.8389177918434143, 'learning_rate': 4.8852091304921495e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6844, 'grad_norm': 0.790310263633728, 'learning_rate': 4.8819105422879005e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6441, 'grad_norm': 0.9391998648643494, 'learning_rate': 4.878611954083652e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6078, 'grad_norm': 0.7220750451087952, 'learning_rate': 4.875313365879404e-05, 'epoch': 0.03}\n",
      "{'loss': 5.5362, 'grad_norm': 0.8749591112136841, 'learning_rate': 4.872014777675155e-05, 'epoch': 0.03}\n",
      "{'loss': 5.5083, 'grad_norm': 0.8379343152046204, 'learning_rate': 4.868716189470907e-05, 'epoch': 0.03}\n",
      "{'loss': 5.5394, 'grad_norm': 0.9940804243087769, 'learning_rate': 4.865417601266658e-05, 'epoch': 0.03}\n",
      "{'loss': 5.4969, 'grad_norm': 0.7818601727485657, 'learning_rate': 4.8621190130624096e-05, 'epoch': 0.03}\n",
      "{'loss': 5.3507, 'grad_norm': 0.7568018436431885, 'learning_rate': 4.8588204248581606e-05, 'epoch': 0.03}\n",
      "{'loss': 5.3798, 'grad_norm': 1.097009301185608, 'learning_rate': 4.8555218366539123e-05, 'epoch': 0.03}\n",
      "{'loss': 5.3686, 'grad_norm': 0.7768962383270264, 'learning_rate': 4.852223248449664e-05, 'epoch': 0.03}\n",
      "{'loss': 5.3271, 'grad_norm': 0.8364092111587524, 'learning_rate': 4.848924660245415e-05, 'epoch': 0.03}\n",
      "{'loss': 5.2829, 'grad_norm': 0.8149890899658203, 'learning_rate': 4.845626072041167e-05, 'epoch': 0.03}\n",
      "{'loss': 5.3014, 'grad_norm': 0.920372486114502, 'learning_rate': 4.842327483836918e-05, 'epoch': 0.03}\n",
      "{'loss': 5.2265, 'grad_norm': 0.8908597826957703, 'learning_rate': 4.8390288956326696e-05, 'epoch': 0.03}\n",
      "{'loss': 5.2467, 'grad_norm': 0.6933526396751404, 'learning_rate': 4.8357303074284214e-05, 'epoch': 0.03}\n",
      "{'loss': 5.2184, 'grad_norm': 0.7761008143424988, 'learning_rate': 4.8324317192241724e-05, 'epoch': 0.03}\n",
      "{'loss': 5.1723, 'grad_norm': 0.7778246998786926, 'learning_rate': 4.829133131019924e-05, 'epoch': 0.03}\n",
      "{'loss': 5.1216, 'grad_norm': 0.7657158970832825, 'learning_rate': 4.825834542815675e-05, 'epoch': 0.03}\n",
      "{'loss': 5.1018, 'grad_norm': 0.7672592401504517, 'learning_rate': 4.822535954611426e-05, 'epoch': 0.04}\n",
      "{'loss': 5.0823, 'grad_norm': 0.7466782331466675, 'learning_rate': 4.819237366407178e-05, 'epoch': 0.04}\n",
      "{'loss': 5.0624, 'grad_norm': 0.8196761608123779, 'learning_rate': 4.815938778202929e-05, 'epoch': 0.04}\n",
      "{'loss': 5.0765, 'grad_norm': 0.8104023337364197, 'learning_rate': 4.812640189998681e-05, 'epoch': 0.04}\n",
      "{'loss': 5.0284, 'grad_norm': 0.7986648678779602, 'learning_rate': 4.809341601794432e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9949, 'grad_norm': 0.933010458946228, 'learning_rate': 4.8060430135901836e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9568, 'grad_norm': 0.8212350606918335, 'learning_rate': 4.8027444253859346e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9933, 'grad_norm': 0.941548764705658, 'learning_rate': 4.799445837181686e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9629, 'grad_norm': 1.1764225959777832, 'learning_rate': 4.796147248977438e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9448, 'grad_norm': 0.8721638917922974, 'learning_rate': 4.792848660773189e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9428, 'grad_norm': 0.8254676461219788, 'learning_rate': 4.789550072568941e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9559, 'grad_norm': 0.7755318284034729, 'learning_rate': 4.786251484364692e-05, 'epoch': 0.04}\n",
      "{'loss': 4.9159, 'grad_norm': 0.7912365794181824, 'learning_rate': 4.7829528961604436e-05, 'epoch': 0.04}\n",
      "{'loss': 4.8944, 'grad_norm': 0.7777007818222046, 'learning_rate': 4.7796543079561954e-05, 'epoch': 0.04}\n",
      "{'loss': 4.8021, 'grad_norm': 0.797046959400177, 'learning_rate': 4.7763557197519464e-05, 'epoch': 0.04}\n",
      "{'loss': 4.879, 'grad_norm': 0.8685548901557922, 'learning_rate': 4.773057131547698e-05, 'epoch': 0.05}\n",
      "{'loss': 4.8531, 'grad_norm': 0.9361977577209473, 'learning_rate': 4.769758543343449e-05, 'epoch': 0.05}\n",
      "{'loss': 4.8807, 'grad_norm': 0.9164804220199585, 'learning_rate': 4.766459955139201e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7962, 'grad_norm': 0.8144111633300781, 'learning_rate': 4.763161366934952e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7292, 'grad_norm': 0.8306987881660461, 'learning_rate': 4.759862778730704e-05, 'epoch': 0.05}\n",
      "{'loss': 4.785, 'grad_norm': 0.8401136994361877, 'learning_rate': 4.756564190526455e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7089, 'grad_norm': 0.8052535057067871, 'learning_rate': 4.753265602322206e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7585, 'grad_norm': 0.9728232026100159, 'learning_rate': 4.7499670141179575e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7343, 'grad_norm': 0.8734872937202454, 'learning_rate': 4.746668425913709e-05, 'epoch': 0.05}\n",
      "{'loss': 4.679, 'grad_norm': 0.7148551940917969, 'learning_rate': 4.74336983770946e-05, 'epoch': 0.05}\n",
      "{'loss': 4.6792, 'grad_norm': 0.8742380738258362, 'learning_rate': 4.740071249505212e-05, 'epoch': 0.05}\n",
      "{'loss': 4.6745, 'grad_norm': 0.8268457055091858, 'learning_rate': 4.736772661300963e-05, 'epoch': 0.05}\n",
      "{'loss': 4.6184, 'grad_norm': 0.7354267835617065, 'learning_rate': 4.733474073096715e-05, 'epoch': 0.05}\n",
      "{'loss': 4.6631, 'grad_norm': 1.017383098602295, 'learning_rate': 4.730175484892466e-05, 'epoch': 0.05}\n",
      "{'loss': 4.7099, 'grad_norm': 1.0424304008483887, 'learning_rate': 4.7268768966882176e-05, 'epoch': 0.05}\n",
      "{'loss': 4.6057, 'grad_norm': 1.0217599868774414, 'learning_rate': 4.7235783084839694e-05, 'epoch': 0.06}\n",
      "{'loss': 4.6321, 'grad_norm': 0.7887975573539734, 'learning_rate': 4.7202797202797204e-05, 'epoch': 0.06}\n",
      "{'loss': 4.603, 'grad_norm': 0.7483492493629456, 'learning_rate': 4.716981132075472e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5732, 'grad_norm': 0.8815250992774963, 'learning_rate': 4.713682543871223e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5634, 'grad_norm': 0.8224893808364868, 'learning_rate': 4.710383955666975e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5401, 'grad_norm': 0.8361169099807739, 'learning_rate': 4.7070853674627267e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5525, 'grad_norm': 0.7538508176803589, 'learning_rate': 4.703786779258478e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5896, 'grad_norm': 0.8097637295722961, 'learning_rate': 4.7004881910542294e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5248, 'grad_norm': 0.8233209252357483, 'learning_rate': 4.6971896028499805e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5515, 'grad_norm': 0.7345536351203918, 'learning_rate': 4.693891014645732e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4931, 'grad_norm': 0.6971938610076904, 'learning_rate': 4.690592426441483e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5092, 'grad_norm': 0.8435736298561096, 'learning_rate': 4.687293838237234e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4431, 'grad_norm': 1.0269577503204346, 'learning_rate': 4.683995250032986e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4922, 'grad_norm': 0.9984848499298096, 'learning_rate': 4.680696661828737e-05, 'epoch': 0.06}\n",
      "{'loss': 4.5192, 'grad_norm': 0.8703465461730957, 'learning_rate': 4.677398073624489e-05, 'epoch': 0.06}\n",
      "{'loss': 4.4828, 'grad_norm': 0.8682764172554016, 'learning_rate': 4.6740994854202406e-05, 'epoch': 0.07}\n",
      "{'loss': 4.4781, 'grad_norm': 0.8108975291252136, 'learning_rate': 4.6708008972159916e-05, 'epoch': 0.07}\n",
      "{'loss': 4.4139, 'grad_norm': 0.7722007036209106, 'learning_rate': 4.6675023090117434e-05, 'epoch': 0.07}\n",
      "{'loss': 4.4017, 'grad_norm': 0.8667240738868713, 'learning_rate': 4.6642037208074944e-05, 'epoch': 0.07}\n",
      "{'loss': 4.4068, 'grad_norm': 0.9011518955230713, 'learning_rate': 4.660905132603246e-05, 'epoch': 0.07}\n",
      "{'loss': 4.402, 'grad_norm': 0.8163943290710449, 'learning_rate': 4.657606544398997e-05, 'epoch': 0.07}\n",
      "{'loss': 4.3737, 'grad_norm': 0.9523839354515076, 'learning_rate': 4.654307956194749e-05, 'epoch': 0.07}\n",
      "{'loss': 4.3149, 'grad_norm': 1.0666433572769165, 'learning_rate': 4.6510093679905007e-05, 'epoch': 0.07}\n",
      "{'loss': 4.4022, 'grad_norm': 0.8346560597419739, 'learning_rate': 4.647710779786252e-05, 'epoch': 0.07}\n",
      "{'loss': 4.3627, 'grad_norm': 0.7362038493156433, 'learning_rate': 4.6444121915820034e-05, 'epoch': 0.07}\n",
      "{'loss': 4.3524, 'grad_norm': 0.8186846375465393, 'learning_rate': 4.6411136033777545e-05, 'epoch': 0.07}\n",
      "{'loss': 4.367, 'grad_norm': 0.7627508044242859, 'learning_rate': 4.637815015173506e-05, 'epoch': 0.07}\n",
      "{'loss': 4.3736, 'grad_norm': 0.7365829944610596, 'learning_rate': 4.634516426969257e-05, 'epoch': 0.07}\n",
      "{'loss': 4.296, 'grad_norm': 0.7423951625823975, 'learning_rate': 4.631217838765009e-05, 'epoch': 0.07}\n",
      "{'loss': 4.2858, 'grad_norm': 0.811992347240448, 'learning_rate': 4.627919250560761e-05, 'epoch': 0.07}\n",
      "{'loss': 4.2951, 'grad_norm': 0.7100083231925964, 'learning_rate': 4.624620662356511e-05, 'epoch': 0.08}\n",
      "{'loss': 4.2696, 'grad_norm': 0.8507101535797119, 'learning_rate': 4.621322074152263e-05, 'epoch': 0.08}\n",
      "{'loss': 4.3686, 'grad_norm': 0.8241246342658997, 'learning_rate': 4.6180234859480146e-05, 'epoch': 0.08}\n",
      "{'loss': 4.2828, 'grad_norm': 0.865728497505188, 'learning_rate': 4.6147248977437656e-05, 'epoch': 0.08}\n",
      "{'loss': 4.3112, 'grad_norm': 0.933724582195282, 'learning_rate': 4.6114263095395173e-05, 'epoch': 0.08}\n",
      "  8%|‚ñä         | 593/7579 [29:59<5:52:34,  3.03s/it]2025-11-04 12:17:42,647 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1802.50 c\n",
      "{'train_runtime': 1802.5154, 'train_samples_per_second': 1076.309, 'train_steps_per_second': 4.205, 'train_loss': 5.4477788527003845, 'epoch': 0.08}\n",
      "  8%|‚ñä         | 594/7579 [30:02<5:53:16,  3.03s/it]\n",
      "2025-11-04 12:17:42,786 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 12:17:42,796 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 12:17:42,806 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 12:17:42,811 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:54<00:00,  5.78it/s]\n",
      "2025-11-04 12:18:37,325 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-04 12:18:37,326 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1802.5154, 'train_samples_per_second': 1076.309, 'train_steps_per_second': 4.205, 'total_flos': 1.0019644338968986e+17, 'train_loss': 5.4477788527003845, 'epoch': 0.07837962657517979}, 'eval': {'eval_loss': 4.954087734222412, 'eval_runtime': 54.5119, 'eval_samples_per_second': 91.723, 'eval_steps_per_second': 5.742, 'epoch': 0.07837962657517979}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_FSDP_full_shard_v5\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ghstkogd\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251104_114737-ghstkogd/logs\u001b[0m\n",
      "[rank0]:[W1104 12:18:41.923784738 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v5'  # 14379 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "  --num_processes 4 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d7eaa",
   "metadata": {},
   "source": [
    "___\n",
    "### FSDP_full_shard_v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529caa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-04 12:29:36,010 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,010 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 12:29:36,010 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,014 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,014 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 12:29:36,014 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,044 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,044 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 12:29:36,044 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: ============================================================\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: World size: 4\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1,2,3\n",
      "2025-11-04 12:29:36,208 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-04 12:29:36,209 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-04 12:29:36,209 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-04 12:30:11,997 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-04 12:30:12,236 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-04 12:30:12,277 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 12:30:12,278 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 12:30:12,278 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 12:30:12,280 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=2,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fc2304d8410>, <lib.callbacks.InspectCallback object at 0x7fc2301d6990>]}\n",
      "2025-11-04 12:30:12,280 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-04 12:30:12,365 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 12:30:12,672 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 12:30:12,672 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 12:30:12,673 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f391dab01d0>, <lib.callbacks.InspectCallback object at 0x7f391cc2acc0>]}\n",
      "2025-11-04 12:30:12,673 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 12:30:12,728 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 12:30:12,728 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 12:30:12,729 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fb63dbf3a40>, <lib.callbacks.InspectCallback object at 0x7fb63ed909b0>]}\n",
      "2025-11-04 12:30:12,729 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\n",
      "2025-11-04 12:30:12,755 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-04 12:30:12,755 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-04 12:30:12,757 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=3,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f5e70ac0530>, <lib.callbacks.InspectCallback object at 0x7f5e70ab6ed0>]}\n",
      "2025-11-04 12:30:12,757 [INFO] lib.training: –î–æ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "/app/lib/training.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-04 12:30:14,154 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 12:30:14,160 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-04 12:30:14,310 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 12:30:14,321 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 12:30:14,324 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-04 12:30:14,359 [INFO] lib.training: –ü–æ—Å–ª–µ Trainer: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: cpu\n",
      "2025-11-04 12:30:14,362 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: creating run\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251104_123015-bjw91zkb\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs8_ga2_FSDP_full_shard_v6\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/bjw91zkb\n",
      "2025-11-04 12:30:16,638 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-04 12:30:16,639 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': True, 'gradient_accumulation_steps': 2, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-04 12:30:16,639 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-04 12:30:16,639 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-04 12:30:18,335 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 12:30:18,335 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 12:30:18,335 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 12:30:18,335 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "2025-11-04 12:30:18,336 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 12:30:18,336 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 12:30:18,336 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 12:30:18,336 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "2025-11-04 12:30:18,337 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 12:30:18,337 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 12:30:18,337 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 12:30:18,337 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-04 12:30:18,344 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-04 12:30:18,344 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-04 12:30:18,344 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-04 12:30:18,345 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "  0%|          | 0/30314 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "{'loss': 10.6439, 'grad_norm': 6.458316326141357, 'learning_rate': 4.999340238833543e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6597, 'grad_norm': 3.241887331008911, 'learning_rate': 4.9985155373754705e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9178, 'grad_norm': 3.397484302520752, 'learning_rate': 4.997690835917398e-05, 'epoch': 0.0}\n",
      "{'loss': 8.5531, 'grad_norm': 1.613102912902832, 'learning_rate': 4.996866134459326e-05, 'epoch': 0.0}\n",
      "{'loss': 8.2779, 'grad_norm': 1.0446356534957886, 'learning_rate': 4.996041433001254e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0094, 'grad_norm': 1.4306232929229736, 'learning_rate': 4.995216731543181e-05, 'epoch': 0.0}\n",
      "{'loss': 7.9125, 'grad_norm': 0.7213961482048035, 'learning_rate': 4.9943920300851096e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7831, 'grad_norm': 0.9182899594306946, 'learning_rate': 4.993567328627037e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7512, 'grad_norm': 1.0703787803649902, 'learning_rate': 4.992742627168966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5853, 'grad_norm': 1.080506443977356, 'learning_rate': 4.991917925710893e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6394, 'grad_norm': 1.3484574556350708, 'learning_rate': 4.991093224252821e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5607, 'grad_norm': 0.9114267230033875, 'learning_rate': 4.990268522794749e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4573, 'grad_norm': 1.3313578367233276, 'learning_rate': 4.9894438213366764e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3304, 'grad_norm': 1.0596277713775635, 'learning_rate': 4.988619119878604e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3283, 'grad_norm': 1.3270317316055298, 'learning_rate': 4.9877944184205325e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2447, 'grad_norm': 1.1802128553390503, 'learning_rate': 4.9869697169624595e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2093, 'grad_norm': 1.2014983892440796, 'learning_rate': 4.986145015504388e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2068, 'grad_norm': 1.0775504112243652, 'learning_rate': 4.9853203140463156e-05, 'epoch': 0.0}\n",
      "{'loss': 7.1733, 'grad_norm': 1.4725592136383057, 'learning_rate': 4.984495612588243e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9652, 'grad_norm': 0.9999507069587708, 'learning_rate': 4.983670911130171e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8336, 'grad_norm': 1.000173807144165, 'learning_rate': 4.982846209672099e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8995, 'grad_norm': 1.0262740850448608, 'learning_rate': 4.982021508214027e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7421, 'grad_norm': 1.2042407989501953, 'learning_rate': 4.981196806755955e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7872, 'grad_norm': 1.0130268335342407, 'learning_rate': 4.9803721052978824e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5963, 'grad_norm': 1.024614691734314, 'learning_rate': 4.979547403839811e-05, 'epoch': 0.0}\n",
      "{'loss': 6.6283, 'grad_norm': 1.0481328964233398, 'learning_rate': 4.978722702381738e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5431, 'grad_norm': 1.0859501361846924, 'learning_rate': 4.977898000923666e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5172, 'grad_norm': 0.9854033589363098, 'learning_rate': 4.977073299465594e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5375, 'grad_norm': 0.9006024599075317, 'learning_rate': 4.9762485980075215e-05, 'epoch': 0.0}\n",
      "{'loss': 6.4429, 'grad_norm': 1.1057180166244507, 'learning_rate': 4.975423896549449e-05, 'epoch': 0.0}\n",
      "{'loss': 6.3709, 'grad_norm': 0.9321805834770203, 'learning_rate': 4.9745991950913776e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4575, 'grad_norm': 1.0673446655273438, 'learning_rate': 4.9737744936333046e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3733, 'grad_norm': 0.9898919463157654, 'learning_rate': 4.972949792175233e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2806, 'grad_norm': 1.0633585453033447, 'learning_rate': 4.9721250907171607e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4108, 'grad_norm': 1.1100636720657349, 'learning_rate': 4.971300389259089e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2241, 'grad_norm': 1.210543155670166, 'learning_rate': 4.970475687801016e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2679, 'grad_norm': 1.1122329235076904, 'learning_rate': 4.9696509863429444e-05, 'epoch': 0.01}\n",
      "{'loss': 6.104, 'grad_norm': 1.0624637603759766, 'learning_rate': 4.968826284884872e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0919, 'grad_norm': 1.0433952808380127, 'learning_rate': 4.9680015834268e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1392, 'grad_norm': 1.1030027866363525, 'learning_rate': 4.9671768819687275e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0292, 'grad_norm': 1.1219254732131958, 'learning_rate': 4.966352180510656e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0417, 'grad_norm': 1.305834412574768, 'learning_rate': 4.965527479052583e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0578, 'grad_norm': 1.1608456373214722, 'learning_rate': 4.964702777594511e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0192, 'grad_norm': 1.0645568370819092, 'learning_rate': 4.963878076136439e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8987, 'grad_norm': 1.0774075984954834, 'learning_rate': 4.9630533746783666e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0163, 'grad_norm': 1.0091707706451416, 'learning_rate': 4.962228673220294e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0256, 'grad_norm': 1.074485421180725, 'learning_rate': 4.961403971762223e-05, 'epoch': 0.01}\n",
      "{'loss': 5.968, 'grad_norm': 1.2936793565750122, 'learning_rate': 4.9605792703041504e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9052, 'grad_norm': 1.166038155555725, 'learning_rate': 4.959754568846078e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7246, 'grad_norm': 0.9824703931808472, 'learning_rate': 4.958929867388006e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8195, 'grad_norm': 1.219422459602356, 'learning_rate': 4.958105165929934e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7078, 'grad_norm': 1.0520551204681396, 'learning_rate': 4.957280464471861e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9407, 'grad_norm': 1.0961637496948242, 'learning_rate': 4.9564557630137895e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7827, 'grad_norm': 1.1436833143234253, 'learning_rate': 4.955631061555717e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7935, 'grad_norm': 1.3994472026824951, 'learning_rate': 4.954806360097645e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7666, 'grad_norm': 1.082916259765625, 'learning_rate': 4.9539816586395726e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6885, 'grad_norm': 1.1046674251556396, 'learning_rate': 4.953156957181501e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6822, 'grad_norm': 1.0432751178741455, 'learning_rate': 4.9523322557234286e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6365, 'grad_norm': 1.0410057306289673, 'learning_rate': 4.951507554265356e-05, 'epoch': 0.01}\n",
      "{'loss': 5.731, 'grad_norm': 0.9310414791107178, 'learning_rate': 4.950682852807284e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6975, 'grad_norm': 1.0641359090805054, 'learning_rate': 4.9498581513492124e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4778, 'grad_norm': 0.9945716857910156, 'learning_rate': 4.9490334498911394e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6677, 'grad_norm': 1.1093130111694336, 'learning_rate': 4.948208748433068e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6696, 'grad_norm': 1.1100331544876099, 'learning_rate': 4.9473840469749954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.527, 'grad_norm': 1.3635979890823364, 'learning_rate': 4.946559345516923e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5649, 'grad_norm': 1.1410698890686035, 'learning_rate': 4.945734644058851e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4356, 'grad_norm': 1.1233996152877808, 'learning_rate': 4.944909942600779e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4871, 'grad_norm': 1.05753755569458, 'learning_rate': 4.944085241142706e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5265, 'grad_norm': 1.1593701839447021, 'learning_rate': 4.9432605396846346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5572, 'grad_norm': 1.0512769222259521, 'learning_rate': 4.942435838226562e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5748, 'grad_norm': 0.9926838874816895, 'learning_rate': 4.9416111367684906e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3856, 'grad_norm': 1.0660077333450317, 'learning_rate': 4.9407864353104176e-05, 'epoch': 0.01}\n",
      "{'loss': 5.432, 'grad_norm': 1.116496205329895, 'learning_rate': 4.939961733852346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.37, 'grad_norm': 1.1212438344955444, 'learning_rate': 4.939137032394274e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2839, 'grad_norm': 1.0623458623886108, 'learning_rate': 4.9383123309362014e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3394, 'grad_norm': 1.130013346672058, 'learning_rate': 4.937487629478129e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3645, 'grad_norm': 0.9941450953483582, 'learning_rate': 4.9366629280200574e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3088, 'grad_norm': 1.1218013763427734, 'learning_rate': 4.9358382265619845e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5087, 'grad_norm': 0.9336419105529785, 'learning_rate': 4.935013525103913e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3581, 'grad_norm': 1.1124532222747803, 'learning_rate': 4.9341888236458405e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4074, 'grad_norm': 1.0634063482284546, 'learning_rate': 4.933364122187768e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3768, 'grad_norm': 1.115071177482605, 'learning_rate': 4.932539420729696e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2254, 'grad_norm': 1.189546823501587, 'learning_rate': 4.931714719271624e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1833, 'grad_norm': 1.0977352857589722, 'learning_rate': 4.930890017813552e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2067, 'grad_norm': 1.2070547342300415, 'learning_rate': 4.9300653163554796e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2589, 'grad_norm': 1.2811695337295532, 'learning_rate': 4.9292406148974073e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1503, 'grad_norm': 1.206850528717041, 'learning_rate': 4.928415913439336e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1833, 'grad_norm': 1.0330220460891724, 'learning_rate': 4.927591211981263e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2784, 'grad_norm': 1.130678653717041, 'learning_rate': 4.926766510523191e-05, 'epoch': 0.01}\n",
      "{'loss': 5.19, 'grad_norm': 0.9795604348182678, 'learning_rate': 4.925941809065119e-05, 'epoch': 0.01}\n",
      "{'loss': 5.0255, 'grad_norm': 1.0229912996292114, 'learning_rate': 4.9251171076070465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1247, 'grad_norm': 1.0619897842407227, 'learning_rate': 4.924292406148974e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2352, 'grad_norm': 1.1626255512237549, 'learning_rate': 4.9234677046909025e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1637, 'grad_norm': 1.1207691431045532, 'learning_rate': 4.9226430032328295e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0408, 'grad_norm': 1.1325979232788086, 'learning_rate': 4.921818301774758e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2434, 'grad_norm': 1.0844296216964722, 'learning_rate': 4.9209936003166856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2151, 'grad_norm': 0.9366000890731812, 'learning_rate': 4.920168898858614e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0015, 'grad_norm': 0.9347882270812988, 'learning_rate': 4.919344197400541e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1398, 'grad_norm': 1.0198497772216797, 'learning_rate': 4.9185194959424694e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9876, 'grad_norm': 1.030051350593567, 'learning_rate': 4.917694794484397e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0163, 'grad_norm': 1.0358672142028809, 'learning_rate': 4.916870093026325e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9584, 'grad_norm': 1.1211410760879517, 'learning_rate': 4.9160453915682524e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0705, 'grad_norm': 0.9659106135368347, 'learning_rate': 4.915220690110181e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0808, 'grad_norm': 1.0819895267486572, 'learning_rate': 4.914395988652108e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9975, 'grad_norm': 1.2211557626724243, 'learning_rate': 4.913571287194036e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0697, 'grad_norm': 1.0313388109207153, 'learning_rate': 4.912746585735964e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0811, 'grad_norm': 1.0634551048278809, 'learning_rate': 4.911921884277892e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9435, 'grad_norm': 1.0722153186798096, 'learning_rate': 4.911097182819819e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8574, 'grad_norm': 1.1871531009674072, 'learning_rate': 4.9102724813617476e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0217, 'grad_norm': 1.0644242763519287, 'learning_rate': 4.909447779903675e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9754, 'grad_norm': 1.0883758068084717, 'learning_rate': 4.908623078445603e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9584, 'grad_norm': 1.017386555671692, 'learning_rate': 4.907798376987531e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8289, 'grad_norm': 1.0164598226547241, 'learning_rate': 4.906973675529459e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7949, 'grad_norm': 1.037180781364441, 'learning_rate': 4.906148974071386e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9251, 'grad_norm': 0.9872786998748779, 'learning_rate': 4.9053242726133144e-05, 'epoch': 0.02}\n",
      "{'loss': 4.832, 'grad_norm': 1.034256935119629, 'learning_rate': 4.904499571155242e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0472, 'grad_norm': 1.031738519668579, 'learning_rate': 4.90367486969717e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9462, 'grad_norm': 0.9541729688644409, 'learning_rate': 4.9028501682390975e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8957, 'grad_norm': 1.0629583597183228, 'learning_rate': 4.902025466781026e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8604, 'grad_norm': 1.1984162330627441, 'learning_rate': 4.9012007653229536e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9408, 'grad_norm': 0.9985387325286865, 'learning_rate': 4.900376063864881e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8649, 'grad_norm': 1.060473084449768, 'learning_rate': 4.899551362406809e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8935, 'grad_norm': 1.024479627609253, 'learning_rate': 4.898726660948737e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8884, 'grad_norm': 1.0384938716888428, 'learning_rate': 4.897901959490664e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8993, 'grad_norm': 1.106911063194275, 'learning_rate': 4.897077258032593e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8509, 'grad_norm': 1.1830105781555176, 'learning_rate': 4.8962525565745204e-05, 'epoch': 0.02}\n",
      "{'loss': 4.84, 'grad_norm': 0.9832803010940552, 'learning_rate': 4.895427855116448e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7636, 'grad_norm': 1.0051518678665161, 'learning_rate': 4.894603153658376e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7189, 'grad_norm': 1.0752488374710083, 'learning_rate': 4.893778452200304e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7917, 'grad_norm': 1.0492606163024902, 'learning_rate': 4.892953750742231e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8164, 'grad_norm': 0.9654434323310852, 'learning_rate': 4.8921290492841595e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8404, 'grad_norm': 1.0700021982192993, 'learning_rate': 4.891304347826087e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8679, 'grad_norm': 0.9594531059265137, 'learning_rate': 4.8904796463680156e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5351, 'grad_norm': 1.1030610799789429, 'learning_rate': 4.8896549449099426e-05, 'epoch': 0.02}\n",
      "{'loss': 4.798, 'grad_norm': 1.0137369632720947, 'learning_rate': 4.888830243451871e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6434, 'grad_norm': 1.0488154888153076, 'learning_rate': 4.8880055419937986e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6943, 'grad_norm': 0.9942074418067932, 'learning_rate': 4.887180840535726e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6346, 'grad_norm': 1.1520899534225464, 'learning_rate': 4.886356139077654e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6831, 'grad_norm': 1.017580270767212, 'learning_rate': 4.8855314376195824e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7278, 'grad_norm': 1.203478455543518, 'learning_rate': 4.8847067361615094e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7419, 'grad_norm': 0.9255306720733643, 'learning_rate': 4.883882034703438e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6987, 'grad_norm': 1.1593281030654907, 'learning_rate': 4.8830573332453655e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6424, 'grad_norm': 0.988456130027771, 'learning_rate': 4.882232631787294e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7238, 'grad_norm': 0.9725451469421387, 'learning_rate': 4.881407930329221e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6607, 'grad_norm': 1.221150517463684, 'learning_rate': 4.880583228871149e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7109, 'grad_norm': 1.083613395690918, 'learning_rate': 4.879758527413077e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6122, 'grad_norm': 0.976732611656189, 'learning_rate': 4.8789338259550046e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7665, 'grad_norm': 0.9422478675842285, 'learning_rate': 4.878109124496932e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7043, 'grad_norm': 1.03690505027771, 'learning_rate': 4.8772844230388607e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6982, 'grad_norm': 1.0370216369628906, 'learning_rate': 4.876459721580788e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6221, 'grad_norm': 1.1299493312835693, 'learning_rate': 4.875635020122716e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5755, 'grad_norm': 0.9984933733940125, 'learning_rate': 4.874810318664644e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5012, 'grad_norm': 1.0652681589126587, 'learning_rate': 4.8739856172065714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6641, 'grad_norm': 1.0079700946807861, 'learning_rate': 4.873160915748499e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6276, 'grad_norm': 1.1033085584640503, 'learning_rate': 4.8723362142904275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4563, 'grad_norm': 0.960887610912323, 'learning_rate': 4.871511512832355e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5591, 'grad_norm': 1.2716799974441528, 'learning_rate': 4.870686811374283e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5017, 'grad_norm': 1.022857904434204, 'learning_rate': 4.8698621099162105e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5172, 'grad_norm': 0.9953296184539795, 'learning_rate': 4.869037408458139e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6833, 'grad_norm': 1.1139194965362549, 'learning_rate': 4.868212707000066e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5968, 'grad_norm': 1.0685745477676392, 'learning_rate': 4.867388005541994e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5718, 'grad_norm': 1.057360053062439, 'learning_rate': 4.866563304083922e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6714, 'grad_norm': 0.9759106040000916, 'learning_rate': 4.86573860262585e-05, 'epoch': 0.03}\n",
      "{'loss': 4.531, 'grad_norm': 1.0061135292053223, 'learning_rate': 4.8649139011677774e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5428, 'grad_norm': 1.0109604597091675, 'learning_rate': 4.864089199709706e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5035, 'grad_norm': 0.9984024167060852, 'learning_rate': 4.863264498251633e-05, 'epoch': 0.03}\n",
      "{'loss': 4.582, 'grad_norm': 1.0092178583145142, 'learning_rate': 4.862439796793561e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6759, 'grad_norm': 1.090242862701416, 'learning_rate': 4.861615095335489e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3746, 'grad_norm': 1.008749008178711, 'learning_rate': 4.860790393877417e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4086, 'grad_norm': 1.0129457712173462, 'learning_rate': 4.859965692419344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4739, 'grad_norm': 1.0595976114273071, 'learning_rate': 4.8591409909612726e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4552, 'grad_norm': 1.0144603252410889, 'learning_rate': 4.8583162895032e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4355, 'grad_norm': 1.0905755758285522, 'learning_rate': 4.857491588045128e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4323, 'grad_norm': 0.9876592755317688, 'learning_rate': 4.8566668865870556e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4869, 'grad_norm': 0.9501920342445374, 'learning_rate': 4.855842185128984e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4951, 'grad_norm': 1.0182583332061768, 'learning_rate': 4.855017483670911e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4575, 'grad_norm': 1.0192668437957764, 'learning_rate': 4.8541927822128394e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4477, 'grad_norm': 0.958482563495636, 'learning_rate': 4.853368080754767e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4368, 'grad_norm': 0.9202280044555664, 'learning_rate': 4.852543379296695e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4925, 'grad_norm': 0.9236849546432495, 'learning_rate': 4.8517186778386225e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5062, 'grad_norm': 0.9441831707954407, 'learning_rate': 4.850893976380551e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4049, 'grad_norm': 0.9588618278503418, 'learning_rate': 4.8500692749224785e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4572, 'grad_norm': 1.0182689428329468, 'learning_rate': 4.849244573464406e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3787, 'grad_norm': 1.1122883558273315, 'learning_rate': 4.848419872006334e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4293, 'grad_norm': 1.1027203798294067, 'learning_rate': 4.847595170548262e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4707, 'grad_norm': 0.992185115814209, 'learning_rate': 4.846770469090189e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3268, 'grad_norm': 1.0204057693481445, 'learning_rate': 4.8459457676321176e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3366, 'grad_norm': 1.0231059789657593, 'learning_rate': 4.845121066174045e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4489, 'grad_norm': 1.0466947555541992, 'learning_rate': 4.844296364715973e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3576, 'grad_norm': 1.0842663049697876, 'learning_rate': 4.843471663257901e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4632, 'grad_norm': 1.0090522766113281, 'learning_rate': 4.842646961799829e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4366, 'grad_norm': 1.0284156799316406, 'learning_rate': 4.841822260341757e-05, 'epoch': 0.03}\n",
      "{'loss': 4.279, 'grad_norm': 1.0130407810211182, 'learning_rate': 4.8409975588836845e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4433, 'grad_norm': 0.9178645014762878, 'learning_rate': 4.840172857425612e-05, 'epoch': 0.03}\n",
      "{'loss': 4.386, 'grad_norm': 0.9880272150039673, 'learning_rate': 4.8393481559675405e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3131, 'grad_norm': 1.046189546585083, 'learning_rate': 4.8385234545094675e-05, 'epoch': 0.03}\n",
      "{'loss': 4.377, 'grad_norm': 0.9994511604309082, 'learning_rate': 4.837698753051396e-05, 'epoch': 0.03}\n",
      "{'loss': 4.439, 'grad_norm': 1.0610724687576294, 'learning_rate': 4.8368740515933236e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2449, 'grad_norm': 1.0273733139038086, 'learning_rate': 4.836049350135251e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4121, 'grad_norm': 0.9814417958259583, 'learning_rate': 4.835224648677179e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2695, 'grad_norm': 1.0173139572143555, 'learning_rate': 4.8343999472191073e-05, 'epoch': 0.03}\n",
      "{'loss': 4.47, 'grad_norm': 1.0232692956924438, 'learning_rate': 4.8335752457610344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3753, 'grad_norm': 1.0316380262374878, 'learning_rate': 4.832750544302963e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3413, 'grad_norm': 1.0596213340759277, 'learning_rate': 4.8319258428448904e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2446, 'grad_norm': 1.044251799583435, 'learning_rate': 4.831101141386819e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3326, 'grad_norm': 0.9934329986572266, 'learning_rate': 4.830276439928746e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2593, 'grad_norm': 0.996052622795105, 'learning_rate': 4.829451738470674e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3987, 'grad_norm': 1.0439754724502563, 'learning_rate': 4.828627037012602e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2861, 'grad_norm': 1.1781795024871826, 'learning_rate': 4.8278023355545295e-05, 'epoch': 0.03}\n",
      "{'loss': 4.281, 'grad_norm': 0.9952004551887512, 'learning_rate': 4.826977634096457e-05, 'epoch': 0.03}\n",
      "{'loss': 4.247, 'grad_norm': 0.9913967251777649, 'learning_rate': 4.8261529326383856e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2212, 'grad_norm': 1.012749433517456, 'learning_rate': 4.8253282311803126e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2476, 'grad_norm': 1.0260835886001587, 'learning_rate': 4.824503529722241e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2785, 'grad_norm': 1.0642647743225098, 'learning_rate': 4.823678828264169e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2654, 'grad_norm': 1.0623470544815063, 'learning_rate': 4.8228541268060964e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2378, 'grad_norm': 0.9749417304992676, 'learning_rate': 4.822029425348024e-05, 'epoch': 0.04}\n",
      "{'loss': 4.215, 'grad_norm': 1.0023754835128784, 'learning_rate': 4.8212047238899524e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2633, 'grad_norm': 1.045582890510559, 'learning_rate': 4.82038002243188e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1673, 'grad_norm': 1.0083266496658325, 'learning_rate': 4.819555320973808e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3186, 'grad_norm': 1.0117809772491455, 'learning_rate': 4.8187306195157355e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3198, 'grad_norm': 1.0302633047103882, 'learning_rate': 4.817905918057664e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0915, 'grad_norm': 0.961218535900116, 'learning_rate': 4.817081216599591e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2802, 'grad_norm': 0.9849963188171387, 'learning_rate': 4.816256515141519e-05, 'epoch': 0.04}\n",
      "{'loss': 4.238, 'grad_norm': 1.0318959951400757, 'learning_rate': 4.815431813683447e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3173, 'grad_norm': 0.9770708680152893, 'learning_rate': 4.8146071122253746e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2537, 'grad_norm': 1.0362355709075928, 'learning_rate': 4.813782410767302e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2429, 'grad_norm': 1.0495834350585938, 'learning_rate': 4.812957709309231e-05, 'epoch': 0.04}\n",
      "{'loss': 4.203, 'grad_norm': 0.9644210338592529, 'learning_rate': 4.812133007851158e-05, 'epoch': 0.04}\n",
      "{'loss': 4.254, 'grad_norm': 0.9238255023956299, 'learning_rate': 4.811308306393086e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1461, 'grad_norm': 0.9433541893959045, 'learning_rate': 4.810483604935014e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2294, 'grad_norm': 1.0887831449508667, 'learning_rate': 4.809658903476942e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2306, 'grad_norm': 0.9505829215049744, 'learning_rate': 4.808834202018869e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2903, 'grad_norm': 0.9844001531600952, 'learning_rate': 4.8080095005607975e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1157, 'grad_norm': 0.9803200364112854, 'learning_rate': 4.807184799102725e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1647, 'grad_norm': 0.9831609725952148, 'learning_rate': 4.806360097644653e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1896, 'grad_norm': 1.0182881355285645, 'learning_rate': 4.8055353961865806e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2416, 'grad_norm': 1.0777803659439087, 'learning_rate': 4.804710694728509e-05, 'epoch': 0.04}\n",
      "{'loss': 4.083, 'grad_norm': 1.0364645719528198, 'learning_rate': 4.803885993270436e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1216, 'grad_norm': 0.932995080947876, 'learning_rate': 4.803061291812364e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1678, 'grad_norm': 0.9841821193695068, 'learning_rate': 4.802236590354292e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1778, 'grad_norm': 0.9776709079742432, 'learning_rate': 4.8014118888962204e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3079, 'grad_norm': 0.9603378176689148, 'learning_rate': 4.8005871874381474e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1282, 'grad_norm': 1.1504641771316528, 'learning_rate': 4.799762485980076e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1377, 'grad_norm': 1.0177407264709473, 'learning_rate': 4.7989377845220035e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0886, 'grad_norm': 1.0328336954116821, 'learning_rate': 4.798113083063931e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1472, 'grad_norm': 0.9734464287757874, 'learning_rate': 4.797288381605859e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1254, 'grad_norm': 1.1423653364181519, 'learning_rate': 4.796463680147787e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2664, 'grad_norm': 0.9187659025192261, 'learning_rate': 4.795638978689714e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2017, 'grad_norm': 0.9289751052856445, 'learning_rate': 4.7948142772316426e-05, 'epoch': 0.04}\n",
      "{'loss': 4.133, 'grad_norm': 0.9975016117095947, 'learning_rate': 4.79398957577357e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1422, 'grad_norm': 1.0357083082199097, 'learning_rate': 4.793164874315498e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1062, 'grad_norm': 1.0937340259552002, 'learning_rate': 4.7923401728574257e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1162, 'grad_norm': 1.023891806602478, 'learning_rate': 4.791515471399354e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1437, 'grad_norm': 1.6435133218765259, 'learning_rate': 4.790690769941282e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1224, 'grad_norm': 0.9883304238319397, 'learning_rate': 4.7898660684832094e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2056, 'grad_norm': 1.0657919645309448, 'learning_rate': 4.789041367025137e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1555, 'grad_norm': 1.0689369440078735, 'learning_rate': 4.7882166655670655e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1202, 'grad_norm': 0.9801862835884094, 'learning_rate': 4.7873919641089925e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2061, 'grad_norm': 0.9860517382621765, 'learning_rate': 4.786567262650921e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1659, 'grad_norm': 1.1761746406555176, 'learning_rate': 4.7857425611928485e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1272, 'grad_norm': 0.9198694825172424, 'learning_rate': 4.784917859734776e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1611, 'grad_norm': 0.9760756492614746, 'learning_rate': 4.784093158276704e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1228, 'grad_norm': 0.9552378058433533, 'learning_rate': 4.783268456818632e-05, 'epoch': 0.04}\n",
      "{'loss': 4.136, 'grad_norm': 1.0199437141418457, 'learning_rate': 4.782443755360559e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1751, 'grad_norm': 0.9647973775863647, 'learning_rate': 4.781619053902488e-05, 'epoch': 0.04}\n",
      "{'loss': 3.96, 'grad_norm': 0.9862890243530273, 'learning_rate': 4.7807943524444154e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1606, 'grad_norm': 0.9870089888572693, 'learning_rate': 4.779969650986344e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1977, 'grad_norm': 1.0133336782455444, 'learning_rate': 4.779144949528271e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1574, 'grad_norm': 0.9557496309280396, 'learning_rate': 4.778320248070199e-05, 'epoch': 0.04}\n",
      "{'loss': 3.905, 'grad_norm': 1.005793809890747, 'learning_rate': 4.777495546612127e-05, 'epoch': 0.04}\n",
      "{'loss': 3.9488, 'grad_norm': 0.9771710634231567, 'learning_rate': 4.7766708451540545e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1125, 'grad_norm': 0.916486382484436, 'learning_rate': 4.775846143695982e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1419, 'grad_norm': 1.0012753009796143, 'learning_rate': 4.7750214422379106e-05, 'epoch': 0.05}\n",
      "{'loss': 4.0235, 'grad_norm': 1.0037305355072021, 'learning_rate': 4.7741967407798376e-05, 'epoch': 0.05}\n",
      "{'loss': 4.081, 'grad_norm': 0.9143751263618469, 'learning_rate': 4.773372039321766e-05, 'epoch': 0.05}\n",
      "{'loss': 4.175, 'grad_norm': 0.9623749852180481, 'learning_rate': 4.7725473378636936e-05, 'epoch': 0.05}\n",
      "{'loss': 4.1896, 'grad_norm': 1.0156198740005493, 'learning_rate': 4.771722636405621e-05, 'epoch': 0.05}\n",
      "{'loss': 4.1297, 'grad_norm': 0.978550374507904, 'learning_rate': 4.770897934947549e-05, 'epoch': 0.05}\n",
      "{'loss': 4.0241, 'grad_norm': 0.9286726713180542, 'learning_rate': 4.7700732334894774e-05, 'epoch': 0.05}\n",
      "{'loss': 4.0577, 'grad_norm': 1.1395583152770996, 'learning_rate': 4.769248532031405e-05, 'epoch': 0.05}\n",
      "{'loss': 4.1074, 'grad_norm': 1.0419666767120361, 'learning_rate': 4.768423830573333e-05, 'epoch': 0.05}\n",
      "{'loss': 4.0504, 'grad_norm': 0.9306480288505554, 'learning_rate': 4.7675991291152604e-05, 'epoch': 0.05}\n",
      "{'loss': 4.1298, 'grad_norm': 0.9816692471504211, 'learning_rate': 4.766774427657189e-05, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 1416/30314 [29:59<10:11:47,  1.27s/it]2025-11-04 13:00:19,011 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.67 c\n",
      "{'train_runtime': 1800.6775, 'train_samples_per_second': 1077.407, 'train_steps_per_second': 16.835, 'train_loss': 5.054531334316386, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 1417/30314 [30:00<10:12:01,  1.27s/it]\n",
      "2025-11-04 13:00:19,129 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 13:00:19,140 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 13:00:19,149 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-04 13:00:19,165 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:53<00:00,  5.81it/s]\n",
      "2025-11-04 13:01:13,361 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-04 13:01:13,361 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.6775, 'train_samples_per_second': 1077.407, 'train_steps_per_second': 16.835, 'total_flos': 5.975520213938995e+16, 'train_loss': 5.054531334316386, 'epoch': 0.04674484965444439}, 'eval': {'eval_loss': 4.731584548950195, 'eval_runtime': 54.209, 'eval_samples_per_second': 92.236, 'eval_steps_per_second': 5.774, 'epoch': 0.04674484965444439}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs8_ga2_FSDP_full_shard_v6\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/bjw91zkb\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251104_123015-bjw91zkb/logs\u001b[0m\n",
      "[rank0]:[W1104 13:01:16.899107420 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v6'  # 11225 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch \\\n",
    "  --num_processes 4 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 8 \\\n",
    "    --grad-accum 2 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00baa4d",
   "metadata": {},
   "source": [
    "___\n",
    "# –û—Ç—á—ë—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1198ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>CUDA visible devices</th>\n",
       "      <th>World size</th>\n",
       "      <th>Parallel mode</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Per-device batch size</th>\n",
       "      <th>Gradient accumulation</th>\n",
       "      <th>Effective batch size</th>\n",
       "      <th>Iter/s</th>\n",
       "      <th>Samples/s</th>\n",
       "      <th>Total samples seen</th>\n",
       "      <th>Steps detected</th>\n",
       "      <th>Train runtime</th>\n",
       "      <th>Eval loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_GPU_v2.log</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>947</td>\n",
       "      <td>1800.49</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline_1_GPU.log</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1782</td>\n",
       "      <td>1800.48</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSpeed_1.log</td>\n",
       "      <td>2, 3</td>\n",
       "      <td>2</td>\n",
       "      <td>DeepSpeed (ZeRO-1)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>878</td>\n",
       "      <td>1801.23</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSpeed_2.log</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>2</td>\n",
       "      <td>DeepSpeed (ZeRO-2)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>570</td>\n",
       "      <td>1801.85</td>\n",
       "      <td>6.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSpeed_3.log</td>\n",
       "      <td>2, 3</td>\n",
       "      <td>2</td>\n",
       "      <td>DeepSpeed (ZeRO-3)</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>692</td>\n",
       "      <td>1801.25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FSDP_full_shard.log</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>2</td>\n",
       "      <td>FSDP full_shard auto_wrap</td>\n",
       "      <td>bf16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>46.55</td>\n",
       "      <td>83840.0</td>\n",
       "      <td>655</td>\n",
       "      <td>1802.24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSDP_full_shard_v2.log</td>\n",
       "      <td>2, 3</td>\n",
       "      <td>2</td>\n",
       "      <td>FSDP full_shard auto_wrap</td>\n",
       "      <td>bf16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.49</td>\n",
       "      <td>42.95</td>\n",
       "      <td>77312.0</td>\n",
       "      <td>1208</td>\n",
       "      <td>1800.45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FSDP_full_shard_v3.log</td>\n",
       "      <td>0, 1</td>\n",
       "      <td>2</td>\n",
       "      <td>FSDP full_shard auto_wrap</td>\n",
       "      <td>bf16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>28.32</td>\n",
       "      <td>51008.0</td>\n",
       "      <td>797</td>\n",
       "      <td>1801.57</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FSDP_full_shard_v4.log</td>\n",
       "      <td>2, 3</td>\n",
       "      <td>2</td>\n",
       "      <td>FSDP full_shard auto_wrap offload</td>\n",
       "      <td>bf16</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.20</td>\n",
       "      <td>31.22</td>\n",
       "      <td>56320.0</td>\n",
       "      <td>220</td>\n",
       "      <td>1804.72</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSDP_full_shard_v5.log</td>\n",
       "      <td>0, 1, 2, 3</td>\n",
       "      <td>4</td>\n",
       "      <td>FSDP full_shard auto_wrap</td>\n",
       "      <td>bf16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.03</td>\n",
       "      <td>84.49</td>\n",
       "      <td>152064.0</td>\n",
       "      <td>594</td>\n",
       "      <td>1802.52</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                File name CUDA visible devices  World size  \\\n",
       "0            1_GPU_v2.log                    0           1   \n",
       "1      Baseline_1_GPU.log                    1           1   \n",
       "2         DeepSpeed_1.log                 2, 3           2   \n",
       "3         DeepSpeed_2.log                 0, 1           2   \n",
       "4         DeepSpeed_3.log                 2, 3           2   \n",
       "5     FSDP_full_shard.log                 0, 1           2   \n",
       "6  FSDP_full_shard_v2.log                 2, 3           2   \n",
       "7  FSDP_full_shard_v3.log                 0, 1           2   \n",
       "8  FSDP_full_shard_v4.log                 2, 3           2   \n",
       "9  FSDP_full_shard_v5.log           0, 1, 2, 3           4   \n",
       "\n",
       "                       Parallel mode Precision  Per-device batch size  \\\n",
       "0                               None      None                    NaN   \n",
       "1                               None      None                    NaN   \n",
       "2                 DeepSpeed (ZeRO-1)      None                    NaN   \n",
       "3                 DeepSpeed (ZeRO-2)      None                    NaN   \n",
       "4                 DeepSpeed (ZeRO-3)      None                    NaN   \n",
       "5          FSDP full_shard auto_wrap      bf16                   16.0   \n",
       "6          FSDP full_shard auto_wrap      bf16                   16.0   \n",
       "7          FSDP full_shard auto_wrap      bf16                    8.0   \n",
       "8  FSDP full_shard auto_wrap offload      bf16                  128.0   \n",
       "9          FSDP full_shard auto_wrap      bf16                   16.0   \n",
       "\n",
       "   Gradient accumulation  Effective batch size  Iter/s  Samples/s  \\\n",
       "0                      1                   NaN    1.90        NaN   \n",
       "1                      1                   NaN    1.01        NaN   \n",
       "2                      1                   NaN    2.05        NaN   \n",
       "3                      1                   NaN    3.16        NaN   \n",
       "4                      1                   NaN    2.60        NaN   \n",
       "5                      4                 128.0    2.75      46.55   \n",
       "6                      2                  64.0    1.49      42.95   \n",
       "7                      4                  64.0    2.26      28.32   \n",
       "8                      1                 256.0    8.20      31.22   \n",
       "9                      4                 256.0    3.03      84.49   \n",
       "\n",
       "   Total samples seen  Steps detected  Train runtime  Eval loss  \n",
       "0                 NaN             947        1800.49       3.94  \n",
       "1                 NaN            1782        1800.48       3.78  \n",
       "2                 NaN             878        1801.23       4.83  \n",
       "3                 NaN             570        1801.85       6.08  \n",
       "4                 NaN             692        1801.25        NaN  \n",
       "5             83840.0             655        1802.24        NaN  \n",
       "6             77312.0            1208        1800.45        NaN  \n",
       "7             51008.0             797        1801.57       5.25  \n",
       "8             56320.0             220        1804.72       6.85  \n",
       "9            152064.0             594        1802.52       4.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib.utils.stash import parse_logs\n",
    "import pandas as pd\n",
    "\n",
    "log_path = \"/app/data/logs\" \n",
    "df = parse_logs(log_path)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(df[['File name', 'CUDA visible devices', 'World size', 'Parallel mode', 'Precision', 'Per-device batch size', 'Gradient accumulation', 'Effective batch size', 'Iter/s', 'Samples/s', 'Total samples seen', 'Steps detected', 'Train runtime', 'Eval loss']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac45df",
   "metadata": {},
   "source": [
    "–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU:\n",
    "- 21005 MiB ‚Äî Baseline_1_GPU\n",
    "- 34353 MiB ‚Äî¬†1_GPU_v2\n",
    "- 35523 MiB ‚Äî DeepSpeed_1\n",
    "- 33135 MiB ‚Äî DeepSpeed_2\n",
    "- 36829 MiB ‚Äî DeepSpeed_3\n",
    "- 30117 MiB ‚Äî¬†FSDP_full_shard\n",
    "- 30117 MiB ‚Äî¬†FSDP_full_shard_v2\n",
    "- 20817 MiB ‚Äî¬†FSDP_full_shard_v3\n",
    "- 61781 MiB ‚Äî¬†FSDP_full_shard_v4\n",
    "___\n",
    "–í–æ—Ç, —á—Ç–æ —è —Ç–∞–∫ –∏ –Ω–µ —Å–º–æ–≥ –ø–æ–Ω—è—Ç—å:\n",
    "\n",
    "- –ü–æ—á–µ–º—É –Ω–∞ 1 GPU —É –Ω–∞—Å –º–æ–¥–µ–ª—å–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞, —á–µ–º –æ–Ω–∞ –∂–µ, —Ä–∞–∑–±–∏—Ç–∞—è –Ω–∞ 2 GPU –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º DeepSpeed –∏–ª–∏ FSDP (–Ω–æ —Ç—É—Ç —á—É—Ç—å –ø–æ–ª—É—á—à–µ). –í–µ—Ä–æ—è—Ç–Ω–æ –¥–µ–ª–æ —Ç—É—Ç –≤ –∞–ª–ª–æ—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±—É—Ñ–µ—Ä–∞—Ö, —è –Ω–µ–º–Ω–æ–≥–æ —Å –Ω–∏–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª, –∏ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏. –û–¥–Ω–∞–∫–æ —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å–∫–∞ –≤ DeepSpeed –Ω–∞ 2 GPU –∑–∞–Ω–∏–º–∞–ª–∞ –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞ —á–µ–º –Ω–∞ 1 –º–Ω–µ –¥–æ–±–∏—Ç—å—Å—è —Ç–∞–∫ –∏ –Ω–µ —É–¥–∞–ª–æ—Å—å. –¢—É—Ç —è —É–∂–µ –Ω–∞—á–∞–ª –ø–æ–¥–æ–∑—Ä–µ–≤–∞—Ç—å, —á—Ç–æ –¥–µ–ª–∞—é —á—Ç–æ-—Ç–æ –≤–æ–æ–±—â–µ –Ω–µ —Ç–∞–∫ –∫–∞–∫ –Ω—É–∂–Ω–æ –∏ —É –º–µ–Ω—è –º–æ–¥–µ–ª—å–∫–∞ –ø—Ä–æ—Å—Ç–æ —Ç—É–ø–æ –∫–æ–ø–∏—Ä—É–µ—Ç—Å—è –º–µ–∂–¥—É –¥–µ–≤–∞–π—Å–∞–º–∏ –∏–ª–∏ –µ—â—ë —á—Ç–æ, –∏ –º–µ–Ω—è –ø–æ—Å–µ—Ç–∏–ª–∞ –º—ã—Å–ª—å –ø—Ä–æ—Å—Ç–æ –∑–∞–∫–∏–Ω—É—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 4 GPU, —Ç–æ—á–Ω–æ —Ç–æ –∂–µ —Å–∞–º–æ–µ —á—Ç–æ –∏ –≤ FSDP_full_shard, –Ω–æ –Ω–∞ 4. 30117 ‚Üí 14379 MiB –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É, –æ–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç! –ü—Ä–∞–≤–¥–∞ —Å–∫–æ—Ä–æ—Å—Ç—å —Å—Ç–∞–ª–∞ –µ—â—ë –º–µ–¥–ª–µ–Ω–Ω–µ–µ.\n",
    "\n",
    "- –ü–æ—á–µ–º—É —Ç–∞–∫ –∑–∞–º–µ–¥–ª—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ.. –ö–∞–∑–∞–ª–æ—Å—å –±—ã, —É –Ω–∞—Å –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å AFAB, –Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–∞–¥–∞–µ—Ç. –ú–æ–¥–µ–ª—å–∫–∞ –Ω–∞ 2 GPU —É—Å–ø–µ–≤–∞–µ—Ç —É–≤–∏–¥–µ—Ç—å –º–µ–Ω—å—à–µ —Å—ç–º–ø–ª–æ–≤, —á–µ–º –ø—Ä–∏ Baseline (FSDP_full_shard / FSDP_full_shard_v2 / Baseline :: 83840 / 77312 / 115200). –ù–æ —É—Å–ø–µ–≤–∞–µ—Ç —É–≤–∏–¥–µ—Ç—å –±–æ–ª—å—à–µ —Å—ç–º–ø–ª–æ–≤ –Ω–∞ 4 GPU (–¥–∞ –ø—Ä–æ—Å—Ç–∏—Ç –º–µ–Ω—è Devbox 11). –í–µ—Ä–æ—è—Ç–Ω–æ —Ç—É—Ç –¥–µ–ª–æ –≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è—Ö, —è –ø—Ä–æ–≤–µ—Ä–∏–ª `nvidia-smi topo -m`:\n",
    "\n",
    "||**GPU0**|**GPU1**|**GPU2**|**GPU3**|**CPU Affinity**|**NUMA Affinity**|**GPU NUMA ID**|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|**GPU0**|X|SYS|SYS|SYS|0‚Äì15, 32‚Äì47|0|N/A|\n",
    "|**GPU1**|SYS|X|SYS|SYS|0‚Äì15, 32‚Äì47|0|N/A|\n",
    "|**GPU2**|SYS|SYS|X|SYS|16‚Äì31, 48‚Äì63|1|N/A|\n",
    "|**GPU3**|SYS|SYS|SYS|X|16‚Äì31, 48‚Äì63|1|N/A|\n",
    "\n",
    "- NVLINK–æ–≤ –Ω–µ—Ç, –æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ CPU, –≤–µ—Ä–æ—è—Ç–Ω–æ –±–æ—Ç–ª–Ω–µ–∫ —Ç—É—Ç –∏–º–µ–Ω–Ω–æ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, –∏ –∫—Ä–∞—Å–∏–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –ø—Ä–æ AFAB (–µ—Å–ª–∏ —Ç–∞–º –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º –≤–æ–æ–±—â–µ –æ–Ω) —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ —Ç–∞–∫, —Ç–æ–Ω–∫–∏–µ —á—ë—Ä–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü–∏ –º–µ–∂–¥—É –º–∏–∫—Ä–æ–±–∞—Ç—á–∞–º–∏ –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –±–æ–ª—å—â—É—â–∏–µ (–ø–æ –º–µ—Ä–∫–∞–º GPU) –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏ (–∫—Å—Ç–∞—Ç–∏, –≤ –∫–∞—Ä—Ç–∏–Ω–∫–µ –µ—Å—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Å —Ç–µ–º, —á—Ç–æ —É –Ω–∞—Å –Ω–∞ —Å–ª–∞–π–¥–µ AFAB –≤ –ø—Ä–µ–∑–µ, –∫–∞—Ä—Ç–∏–Ω–∫–∞ —Å [medium](https://medium.com/better-programming/parallel-and-distributed-training-in-deep-learning-for-beginners-part-1-introduction-612a4534a117)):\n",
    "\n",
    "![](/app/data/imgs/AFAB.png)\n",
    "\n",
    "- –î–∞–ª—å—à–µ, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –º–∏–∫—Ä–æ–±–∞—Ç—á–∏. –£ –Ω–∞—Å –µ—Å—Ç—å world_size - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU, –ø–æ—Ç–æ–º gradient accumulation –∏ batch size. –ò –≤–æ–æ–±—â–µ DeepSpeed –Ω–∞—Å –≤—ã–Ω—É–∂–¥–∞–µ—Ç –¥–µ–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ: `kwargs.setdefault(\"train_micro_batch_size_per_gpu\", training_config[\"per_device_train_batch_size\"])` –∏–Ω–∞—á–µ –≥—Ä–æ–º–∫–æ —Ä—É–≥–∞–µ—Ç—Å—è. –¢–æ –µ—Å—Ç—å –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä `micro_batch_size`, –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ —Ä–∞–≤–µ–Ω –ø—Ä–æ—Å—Ç–æ –Ω–∞—à–µ–º—É –±–∞—Ç—á—É. –í–æ—Ç —Ç—É—Ç –∏ –≤–æ–ø—Ä–æ—Å - –≤ –∫–∞–∫–æ–π –º–æ–º–µ–Ω—Ç –≤–æ–æ–±—â–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –º–∏–∫—Ä–æ–±–∞—Ç—á–∏ –∏ –∏–∑ —á–µ–≥–æ (–∏ –∫–∞–∫ –ø–æ–Ω—è—Ç—å —á—Ç–æ —Ç–∞–º –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º —É —ç—Ç–æ–≥–æ DeepSpeed, Tensor parallel –∏–ª–∏ Pipeline parallel —Å –∫–∞–∫–æ–π-—Ç–æ –≤–∞—Ä–∏–∞—Ü–∏–µ–π AFAB)? –ü—Ä–æ—Å—Ç–æ –ø–æ –∏–¥–µ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ –¥–æ–ª–∂–Ω–æ –≤—Å—ë —Ä–∞–≤–Ω–æ —É—Å–∫–æ—Ä—è—Ç—å—Å—è (—Ö–æ—Ç—å —á—É—Ç—å-—á—É—Ç—å), –Ω–µ—É–∂–µ–ª–∏ –≤—Å—ë –¥–µ–ª–æ –≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è—Ö —á–µ—Ä–µ–∑ SYS, –Ω–µ –ø—Ä–æ—Å—Ç–æ –∂–µ —Ç–∞–∫ —É –Ω–∞—Å —Ç–∞–º –µ—Å—Ç—å `overlap_comm`?\n",
    "\n",
    "- –¢–∞–∫–∂–µ —è –∫–æ–≥–¥–∞ –∑–∞–ø—É—Å–∫–∞–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç `FSDP_full_shard_v4` —Å –æ–≥—Ä–æ–º–Ω—ã–º –±–∞—Ç—á–æ–º, —Ç–æ —É–≤–∏–¥–µ–ª –º–æ—â–Ω—ã–π (–ø–æ—Ä—è–¥–∫–∞ 2.5-3 —Å–µ–∫—É–Ω–¥) –ø—Ä–æ—Å—Ç–æ–π –æ–±–µ–∏—Ö GPU. –û–Ω–∏ –Ω–∞—á–∏–Ω–∞—é—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å, –ø–æ—Ç–æ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Ç—É–ø–∏—Ç—å. –ï—Å–ª–∏ —ç—Ç–æ –∏–∑-–∑–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π —Å –±–æ–ª—å—à–∏–º –±–∞—Ç—á–æ–º –∏ —Ç–∞–º AFAB, —Ç–æ –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –ø–æ –∏–¥–µ–µ —Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ-–æ—á–µ—Ä–µ–¥–∏ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º. –°–Ω–æ–≤–∞ —Å—Ç—Ä–∞–Ω–Ω–æ.\n",
    "\n",
    "___\n",
    "–í–æ–ø—Ä–æ—Å—ã –∏–∑ –∑–∞–¥–∞–Ω–∏—è:\n",
    "\n",
    "- DeepSpeed: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å batch_size –∑–∞ —Å—á–µ—Ç –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–∏. –ï—Å–ª–∏ –¥–∞, —Ç–æ –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å—Ç–∞–ª–æ –æ–±—É—á–µ–Ω–∏–µ?\n",
    "    - –ü–æ–ª—É—á–∞–µ—Ç—Å—è, –Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞ –±–æ–ª—å—à–µ–º —á–µ–º 2 –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ GPU (–∏ —á–µ–º –±–æ–ª—å—à–µ GPU —Ç–µ–º –º–µ–¥–ª–µ–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ)\n",
    "- –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ –¥–ª—è –æ—Ç—á—ë—Ç–∞ –ø–æ –î–ó ...\n",
    "    - –í—Å—ë –≤ —Ç–∞–±–ª–∏—á–∫–µ –≤—ã—à–µ –∏ –≤ [wandb](https://wandb.ai/ksorzz/llm_hw2-aylesnov?nw=nwuserksorzz) (–¥–æ–±–∞–≤–ª—è–ª –ª–æ–≥–∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –ø–æ—ç—Ç–æ–º—É –∫–æ–µ-–≥–¥–µ `None`)\n",
    "- –°—Ä–∞–≤–Ω–∏—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏. –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è –ª–∏ –ª–∏–Ω–µ–π–Ω—ã–π scaling —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è?\n",
    "    - `1 GPU (GBS = 64):  115200 / 115200 = 1    (Baseline, eval_loss: 3.78)`\n",
    "    - `2 GPU (GBS = 128): 112384 / 115200 = 0.98 (DeepSpeed_1, eval_loss: 4.83)`\n",
    "    - `2 GPU (GBS = 128): 83840  / 115200 = 0.73 (FSDP_full_shard, eval_loss: 5.1)`\n",
    "    - `4 GPU (GBS = 256): 152064 / 115200 = 1.32 (FSDP_full_shard_v5, eval_loss: 4.95)`\n",
    "    - –í –≤–æ–ø—Ä–æ—Å–µ r < 2 –∏–ª–∏ r > 2, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –≤–∏–¥–∏–º–æ, —á—Ç–æ –≤—Å—ë —Ç–∞–∫–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ—á—Ç–∏ x2 –±—É—Å—Ç –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ª–∏–±–æ –∂–µ —Ç–∞–º –æ—à–∏–±–∫–∞ –∏ –≤–æ–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã–ª —Å—Ç–æ—è—Ç—å –∫–∞–∫ r < 1 –∏–ª–∏ r > 1. –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ r=0.98 –Ω–∞ 2 GPU —Å stage 1 DeepSpeed - –ª—É—á—à–µ–µ, —á–µ–≥–æ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∏—Ç—å—Å—è (—Ö–æ—Ç—è —Å—Ö–æ–¥–∏–ª–æ—Å—å –æ–Ω–æ —Ö—É–∂–µ, –Ω–æ —ç—Ç–æ –∏–∑-–∑–∞ global batch size —Å—É–¥—è –ø–æ –≤—Å–µ–º—É, –ø–æ—Å–∫–æ–ª—å–∫–æ –≤ —Å–∏–Ω–≥–ª –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —è–≤–ª—è–µ—Ç—Å—è 64 –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª—å–∫–∏ –∏ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞). r —É –º–µ–Ω—è –Ω–∞ 2 GPU –ø–æ–ª—É—á–∏–ª–æ—Å—å __–º–µ–Ω—å—à–µ 1__, —á—Ç–æ –ª–∏–±–æ –º–æ—è –æ—à–∏–±–∫–∞ –ª–∏–±–æ —ç—Ç–æ –±–æ—Ç–ª–Ω–µ–∫ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –º–µ–∂–¥—É –∫–∞—Ä—Ç–∞–º–∏.\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥, –∫–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏:\n",
    "    - Single-GPU\n",
    "\n",
    "–í —Ü–µ–ª–æ–º –∫–∞–∫–∏–µ-—Ç–æ —Ç–∞–∫–∏–µ –º—ã—Å–ª–∏ –ø–æ –ø–æ–≤–æ–¥—É –∑–∞–¥–∞–Ω–∏—è, –≤–æ–æ–±—â–µ –≤—Å—ë –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ, –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–æ—Å—å —É–≤–∏–¥–µ—Ç—å –æ—â—É—Ç–∏–º–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ GPU utilization (–Ω–æ —Ç–æ–ª—å–∫–æ –Ω–∞ 4 –∫–∞—Ä—Ç–∞—Ö).\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37ab25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
