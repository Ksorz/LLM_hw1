# Распределённое обучение (HW_2)

- Результаты экспериментов, таблица и отчёт в [ноубуке](/hw2_parallel_pretrain/distributed_learning.ipynb) (в конце). [Wandb](https://wandb.ai/ksorzz/llm_hw2-aylesnov?nw=nwuserksorzz)

# Обучение языковой модели (HW_1)

## Эксперименты с гиперпараметрами

### Batch Size и Gradient Accumulation
- Проведено 15 экспериментов через `wandb.sweep`
- Лучший результат показало сочетание `per_device_train_batch_size=16` и `gradient_accumulation_steps=4`

### Learning Rate и Scheduler
- После 15 экспериментов через `wandb.sweep` я делал кастомные эксперименты руками
```python
SWEEP_CONFIG = {
    'method': 'bayes',
    'metric': {'name': 'final_eval_loss', 'goal': 'minimize'},
    'parameters': {
        'per_device_train_batch_size': {'values': [1, 2, 4, 6]},
        'gradient_accumulation_steps': {'values': [2, 4, 6]},
        'learning_rate': {'values': [5e-5, 1e-4, 2e-4, 5e-4]},
        'optim': {'values': ['adamw_torch', 'adamw_torch_fused', 'adafactor']},
        'lr_scheduler_type': {'values': ['linear', 'cosine', 'constant']},
        'torch_compile': {'values': [True, False]},
        'warmup_ratio': {'values': [0.07, 0.1, 0.2]},
    }
}
```
- При высоком lr (1e-4 или 5e-5) модель перестаёт стабильно сходиться при достижении loss ≈ 4

### Ручные эксперименты
- Добавлен кастомный scheduler, который снижает lr после N шагов
- Изначально использовался triangular scheduler, но он показался избыточным (хотя и дал лучший результат)
- Оставлено только снижение lr до определённого уровня

### Текущие результаты
- Снижение lr дало буст в обучении (результаты видны в wandb)
- Попытка снизить lr до ~5e-5 на 20-й минуте была не самой удачной
- Снижение lr помогает, но баланс сложно нащупать

### Что хочется попробовать
- Увеличить batch size и lr
- Сделать снижение lr более плавным
- Эксперимент: `per_device_train_batch_size=10` и `gradient_accumulation_steps=8` с большим, но более плавным lr показывает более медленную, но устойчивую сходимость
- На данный момент лучший показатель c `per_device_train_batch_size=16` и `gradient_accumulation_steps=4`. Пожалуй пока на этом остановлюсь

## Мониторинг
[Wandb Dashboard](https://wandb.ai/ksorzz/llm_hw1-aylesnov/workspace?nw=nwuserksorzz)
имена экспериментов:
```python
name = (
    f"bs{cfg['per_device_train_batch_size']}#"
    f"ga{cfg['gradient_accumulation_steps']}#"
    f"lr{cfg['learning_rate']}#"
    f"{cfg['optim']}#"
    f"{cfg.get('lr_scheduler_type','linear')}#"
    f"warmup_{cfg.get('warmup_ratio')}"
)
```


## Примеры генераций

**Промпт:** "В начале было слово"  
**Ограничение:** 64 токена

### Пример 1
```
В начале было слово, и слово было сделано в СССР и в Москве.

В культуре 

В романе М. И. Степанова принимал участие в съёмках фильма «Вторая история» (1957), и в «Пушкинском театре» (1957).

Примечания

Ссылки 

Дети кино
```
*Метрики: eval/loss=4.69, train/loss=4.09, lr=0.00049*

### Пример 2
```
В начале было слово, и слово было сделано от этого лица. В начале XIX века в XVIII веке на месте дома помещицы, в частности, «Афики» В. И. Гурова был убит под угрозой исследования в «Пушкинском государстве».

Примечания

Литература 

 Могии А
```
*Метрики: eval/loss=4.71, train/loss=3.65, lr=0.0001*

### Пример 3
```
В начале было слово, и слово было дано в честь святого Иоанна Иоанна Предтеи Ибн.

В честь святого Иоанна Дамасканцев, которое в честь святого Иоанна Крестителя Ибн Абу ибн Алир был снят в 1821 году и назван в честь великого визинина Ибн Мухаммеда, который был открыт в честь святого Петра I. В
```
*Метрики: eval/loss=4.20, train/loss=3.40, lr=0.0005*

### Пример 4
```
В начале было слово, и слово было сделано. В результате его пребывания в течение всего дня в «Крук-кэмк» (1958—1968).

Памятники 
В 1928 году в ходе Великой Отечественной войны в ходе операции «Кабло-Боттингем Форест» была расформирована.
```
*Метрики: eval/loss=4.73, train/loss=4.10, lr=0.0002*

### Пример 5
```
В начале было слово, и слово было дано от имени Ф. Г. Букша. В отличие от названия, в частности, «Новые» иероглифические «хождения» — буква «престига», в «грозу» обозначает буква «попа».

Изменность 
В некоторых источниках используется
```
*Метрики: eval/loss=3.91, train/loss=3.09, lr=5e-05*

### Пример 6 (Финальный лучший результат)
```
В начале было слово, и слово было дано от — «камень».

Впервые, в своей работе, в 1920-е годы (содержание в переводе «Памятники») был издан новый исторический тип издания (в переводе с английского — «Учение»).

В 1929 году в городе было создано отделение «Петровская
```
*Метрики: eval/loss=3.81, train/loss=3.17, lr=0.00017*