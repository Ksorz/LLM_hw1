{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab4ed44",
   "metadata": {},
   "source": [
    "# ZeRO –∏ FSDP: –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "## __[DeepSpeed ZeRO](https://www.deepspeed.ai/docs/config-json/):__\n",
    "\n",
    "–®–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä—ë—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å data-parallel:  \n",
    "- —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (optimizer states)  \n",
    "- –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (gradients)  \n",
    "- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (parameters)  \n",
    "\n",
    "–≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –ø—Ä–æ—Ü–µ—Å—Å/—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "__–£—Ä–æ–≤–Ω–∏ (stage):__\n",
    "\n",
    "| Stage | –ß—Ç–æ —à–∞—Ä–¥–∏—Ç—Å—è | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n",
    "|-------|-------------|-------------|\n",
    "| 0 | –Ω–∏—á–µ–≥–æ (—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π data-parallel, —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π) | –æ–±—ã—á–Ω—ã–π DDP |\n",
    "| 1 | optimizer states | –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Å—Ç–∞—é—Ç—Å—è —Ä–µ–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏. –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 0, –Ω–µ–±–æ–ª—å—à–æ–π –Ω–∞–∫–ª–∞–¥ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. |\n",
    "| 2 | optimizer states + gradients | –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—Å—ë –µ—â—ë —Ä–µ–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã. –ë–æ–ª—å—à–µ —ç–∫–æ–Ω–æ–º–∏—è, –±–æ–ª—å—à–µ –Ω–∞–∫–ª–∞–¥. |\n",
    "| 3 | optimizer states + gradients + parameters | –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ –∑–∞ —Å—á—ë—Ç –ø–æ–ª–Ω–æ–π —à–∞—Ä–¥–∏–∑–∞—Ü–∏–∏. –ù–æ —Å–∞–º–∞—è –±–æ–ª—å—à–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. |\n",
    "\n",
    "__–ê–∫—Ç—É–∞–ª—å–Ω—ã–π (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π) DeepSpeed config –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:__\n",
    "\n",
    "```python\n",
    "zero_optimization = {\n",
    "    \"stage\": 0,                                         # 0|1|2|3 ‚Äî —É—Ä–æ–≤–µ–Ω—å ZeRO, 0 = –æ–±—ã—á–Ω—ã–π DDP\n",
    "    \"allgather_partitions\": True,                       # —Å–æ–±–∏—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ —à–∞–≥–∞ (True ‚Äî –¥–µ—Ñ–æ–ª—Ç)\n",
    "    \"reduce_scatter\": True,                             # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å reduce_scatter –≤–º–µ—Å—Ç–æ allreduce\n",
    "    \"allgather_bucket_size\": 5e8,                       # —Ä–∞–∑–º–µ—Ä –±–∞–∫–µ—Ç–∞ (–≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö, –Ω–µ –±–∞–π—Ç–∞—Ö), –¥–µ—Ñ–æ–ª—Ç 500–ú\n",
    "    \"reduce_bucket_size\": 5e8,                          # —Ä–∞–∑–º–µ—Ä –±–∞–∫–µ—Ç–∞ –¥–ª—è reduce-scatter, –¥–µ—Ñ–æ–ª—Ç 500–ú\n",
    "    \"overlap_comm\": False,                              # –¥–µ—Ñ–æ–ª—Ç False; True –ø–µ—Ä–µ–∫—Ä—ã–≤–∞–µ—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏\n",
    "    \"contiguous_gradients\": True,                       # —Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –±–ª–æ–∫–µ –ø–∞–º—è—Ç–∏\n",
    "    \"offload_optimizer\": {                              # offload –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–∞–∫—Ç—É–∞–ª—å–Ω–æ –ø—Ä–∏ stage >= 2)\n",
    "        \"device\": \"none\",                               # –≤–∞—Ä–∏–∞–Ω—Ç—ã: \"none\" | \"cpu\" | \"nvme\"\n",
    "        \"pin_memory\": False\n",
    "    },\n",
    "    \"offload_param\": {                                  # offload –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∞–∫—Ç—É–∞–ª—å–Ω–æ –ø—Ä–∏ stage == 3)\n",
    "        \"device\": \"none\",                               # \"none\" | \"cpu\" | \"nvme\"\n",
    "        \"pin_memory\": False\n",
    "    },\n",
    "    \"sub_group_size\": 1e9,                              # –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã –ø—Ä–∏ stage3 (–±–∞–π—Ç—ã)\n",
    "    \"stage3_max_live_parameters\": 1e9,                  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏\n",
    "    \"stage3_max_reuse_distance\": 1e9,                   # –º–∞–∫—Å. —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    \"stage3_prefetch_bucket_size\": 5e8,                 # —Ä–∞–∑–º–µ—Ä –ø—Ä–µ—Ñ–µ—Ç—á-–±–∞–∫–µ—Ç–∞\n",
    "    \"stage3_param_persistence_threshold\": 1e5,          # –ø–æ—Ä–æ–≥ –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ–º—ã—Ö\n",
    "    \"stage3_gather_fp16_weights_on_model_save\": True,   # —Å–æ–±–∏—Ä–∞—Ç—å –≤–µ—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏\n",
    "}\n",
    "```\n",
    "___\n",
    "\n",
    "## [FSDP](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.fsdp) (Fully Sharded Data Parallel)\n",
    "\n",
    "–ú–µ—Ç–æ–¥ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤ PyTorch –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É Accelerate / Transformers. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —à–∞—Ä–¥–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏/–∏–ª–∏ optimizer-—Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ –Ω–∞ GPU.\n",
    "\n",
    "__Default FSDP config:__\n",
    "\n",
    "```python\n",
    "fsdp_config = {\n",
    "    \"fsdp_sharding_strategy\": \"FULL_SHARD\",             # FULL_SHARD | SHARD_GRAD_OP | NO_SHARD | HYBRID_SHARD | HYBRID_SHARD_ZERO2\n",
    "    \"fsdp_offload_params\": False,                       # False = –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è –Ω–∞ CPU\n",
    "    \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",  # TRANSFORMER_BASED_WRAP | SIZE_BASED_WRAP | NO_WRAP\n",
    "    \"fsdp_transformer_layer_cls_to_wrap\": None,         # —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ —Å–ª–æ—ë–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä [\"BertLayer\"]); None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "    \"fsdp_backward_prefetch_policy\": \"BACKWARD_POST\",   # BACKWARD_PRE | BACKWARD_POST | NO_PREFETCH\n",
    "    \"fsdp_forward_prefetch\": False,                     # False = –Ω–µ –ø—Ä–µ—Ñ–µ—Ç—á–∏—Ç—å –≤–ø–µ—Ä—ë–¥\n",
    "    \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",       # FULL_STATE_DICT | SHARDED_STATE_DICT | LOCAL_STATE_DICT (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ SHARDED)\n",
    "    \"fsdp_cpu_ram_efficient_loading\": True,             # True = –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ RAM\n",
    "    \"fsdp_min_num_params\": 1e8,                         # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è SIZE_BASED_WRAP (100 –º–ª–Ω)\n",
    "    \"fsdp_sync_module_states\": True,                    # —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    \"fsdp_use_orig_params\": True,                       # True = —Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "    \"fsdp_activation_checkpointing\": False,             # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ checkpointing –∞–∫—Ç–∏–≤–∞—Ü–∏–π (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "[link](https://huggingface.co/docs/accelerate/en/usage_guides/fsdp?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcc33a",
   "metadata": {},
   "source": [
    "### Baseline_1_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: baseline\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: World size: 1\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: CUDA_VISIBLE_DEVICES: 1\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:34:41,692 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ baseline setup (single GPU)\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:35:16,757 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_173519-sd6qsqe4\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_Baseline_1_GPU\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/sd6qsqe4\n",
      "2025-11-03 17:35:21,167 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:35:21,167 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:35:21,168 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:35:21,168 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/30314 [00:00<?, ?it/s]W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:21.966000 2629993 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 17:35:30.229000 2629993 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.8721, 'grad_norm': 40.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 10.3303, 'grad_norm': 3.21875, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.7224, 'grad_norm': 4.1875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.5379, 'grad_norm': 1.8203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.41, 'grad_norm': 1.375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2709, 'grad_norm': 1.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.215, 'grad_norm': 1.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.08, 'grad_norm': 1.1953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0479, 'grad_norm': 0.7578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8773, 'grad_norm': 0.703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8597, 'grad_norm': 0.8515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7418, 'grad_norm': 0.94140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5995, 'grad_norm': 1.109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4574, 'grad_norm': 1.2578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3744, 'grad_norm': 0.96484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2797, 'grad_norm': 0.8125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2186, 'grad_norm': 1.0859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.121, 'grad_norm': 0.75, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0157, 'grad_norm': 0.84375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.7949, 'grad_norm': 0.87890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6535, 'grad_norm': 0.7109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.6468, 'grad_norm': 0.75390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.455, 'grad_norm': 0.796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.4746, 'grad_norm': 0.76953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2777, 'grad_norm': 0.8515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.2574, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1485, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1064, 'grad_norm': 0.796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 6.1286, 'grad_norm': 0.6953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.9833, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 5.9077, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9811, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9083, 'grad_norm': 0.71875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7807, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8899, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7069, 'grad_norm': 0.68359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7366, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5773, 'grad_norm': 0.62890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5452, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5706, 'grad_norm': 0.5546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.455, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4878, 'grad_norm': 0.6796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4999, 'grad_norm': 0.69140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4484, 'grad_norm': 0.54296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3111, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4163, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4154, 'grad_norm': 0.640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3586, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.292, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1492, 'grad_norm': 0.52734375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.2227, 'grad_norm': 0.6875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0989, 'grad_norm': 0.5234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3193, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1377, 'grad_norm': 0.6015625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1691, 'grad_norm': 0.69140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1182, 'grad_norm': 0.56640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0525, 'grad_norm': 0.55859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0464, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.9923, 'grad_norm': 0.48046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0926, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.041, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8398, 'grad_norm': 0.484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0249, 'grad_norm': 0.65234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0166, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8451, 'grad_norm': 0.58984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8894, 'grad_norm': 0.498046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.772, 'grad_norm': 0.58984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8341, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8778, 'grad_norm': 0.5859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8943, 'grad_norm': 0.62890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8777, 'grad_norm': 0.484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7398, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7571, 'grad_norm': 0.6484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.721, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6558, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6991, 'grad_norm': 0.486328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6919, 'grad_norm': 0.4921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6391, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.8391, 'grad_norm': 0.458984375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7064, 'grad_norm': 0.4921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7305, 'grad_norm': 0.5, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.7011, 'grad_norm': 0.5078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5659, 'grad_norm': 0.5625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5317, 'grad_norm': 0.48828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5267, 'grad_norm': 0.474609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5892, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.4835, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.511, 'grad_norm': 0.5546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.6248, 'grad_norm': 0.49609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.5176, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.3683, 'grad_norm': 0.49609375, 'learning_rate': 0.000698374193548387, 'epoch': 0.02}\n",
      "{'loss': 4.4851, 'grad_norm': 0.55078125, 'learning_rate': 0.0006963419354838709, 'epoch': 0.02}\n",
      "{'loss': 4.5789, 'grad_norm': 0.4765625, 'learning_rate': 0.0006943096774193547, 'epoch': 0.02}\n",
      "{'loss': 4.5082, 'grad_norm': 0.5390625, 'learning_rate': 0.0006922774193548388, 'epoch': 0.02}\n",
      "{'loss': 4.383, 'grad_norm': 0.515625, 'learning_rate': 0.0006902451612903226, 'epoch': 0.02}\n",
      "{'loss': 4.5932, 'grad_norm': 0.4921875, 'learning_rate': 0.0006882129032258064, 'epoch': 0.02}\n",
      "{'loss': 4.548, 'grad_norm': 0.59375, 'learning_rate': 0.0006861806451612903, 'epoch': 0.02}\n",
      "{'loss': 4.3619, 'grad_norm': 0.51953125, 'learning_rate': 0.0006841483870967741, 'epoch': 0.02}\n",
      "{'loss': 4.4733, 'grad_norm': 0.453125, 'learning_rate': 0.0006821161290322579, 'epoch': 0.02}\n",
      "{'loss': 4.3207, 'grad_norm': 0.52734375, 'learning_rate': 0.0006800838709677419, 'epoch': 0.02}\n",
      "{'loss': 4.365, 'grad_norm': 0.466796875, 'learning_rate': 0.0006780516129032257, 'epoch': 0.02}\n",
      "{'loss': 4.3066, 'grad_norm': 0.515625, 'learning_rate': 0.0006760193548387097, 'epoch': 0.02}\n",
      "{'loss': 4.4256, 'grad_norm': 0.46484375, 'learning_rate': 0.0006739870967741935, 'epoch': 0.02}\n",
      "{'loss': 4.4189, 'grad_norm': 0.462890625, 'learning_rate': 0.0006719548387096773, 'epoch': 0.02}\n",
      "{'loss': 4.3297, 'grad_norm': 0.451171875, 'learning_rate': 0.0006699225806451613, 'epoch': 0.02}\n",
      "{'loss': 4.3721, 'grad_norm': 0.439453125, 'learning_rate': 0.0006678903225806451, 'epoch': 0.02}\n",
      "{'loss': 4.454, 'grad_norm': 0.462890625, 'learning_rate': 0.000665858064516129, 'epoch': 0.02}\n",
      "{'loss': 4.2769, 'grad_norm': 0.453125, 'learning_rate': 0.0006638258064516128, 'epoch': 0.02}\n",
      "{'loss': 4.2232, 'grad_norm': 0.5, 'learning_rate': 0.0006617935483870967, 'epoch': 0.02}\n",
      "{'loss': 4.3592, 'grad_norm': 0.4453125, 'learning_rate': 0.0006597612903225807, 'epoch': 0.02}\n",
      "{'loss': 4.3141, 'grad_norm': 0.5, 'learning_rate': 0.0006577290322580645, 'epoch': 0.02}\n",
      "{'loss': 4.3016, 'grad_norm': 0.458984375, 'learning_rate': 0.0006556967741935483, 'epoch': 0.02}\n",
      "{'loss': 4.1957, 'grad_norm': 0.47265625, 'learning_rate': 0.0006536645161290322, 'epoch': 0.02}\n",
      "{'loss': 4.1466, 'grad_norm': 0.439453125, 'learning_rate': 0.000651632258064516, 'epoch': 0.02}\n",
      "{'loss': 4.2662, 'grad_norm': 0.462890625, 'learning_rate': 0.0006495999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.2014, 'grad_norm': 0.4921875, 'learning_rate': 0.0006475677419354838, 'epoch': 0.02}\n",
      "{'loss': 4.3977, 'grad_norm': 0.458984375, 'learning_rate': 0.0006455354838709677, 'epoch': 0.02}\n",
      "{'loss': 4.2665, 'grad_norm': 0.46875, 'learning_rate': 0.0006435032258064516, 'epoch': 0.02}\n",
      "{'loss': 4.2421, 'grad_norm': 0.462890625, 'learning_rate': 0.0006414709677419354, 'epoch': 0.02}\n",
      "{'loss': 4.2164, 'grad_norm': 0.55078125, 'learning_rate': 0.0006394387096774192, 'epoch': 0.02}\n",
      "{'loss': 4.2853, 'grad_norm': 0.5390625, 'learning_rate': 0.0006374064516129032, 'epoch': 0.02}\n",
      "{'loss': 4.2224, 'grad_norm': 0.53125, 'learning_rate': 0.000635374193548387, 'epoch': 0.02}\n",
      "{'loss': 4.2244, 'grad_norm': 0.44921875, 'learning_rate': 0.0006333419354838709, 'epoch': 0.02}\n",
      "{'loss': 4.2336, 'grad_norm': 0.52734375, 'learning_rate': 0.0006313096774193548, 'epoch': 0.02}\n",
      "{'loss': 4.2468, 'grad_norm': 0.419921875, 'learning_rate': 0.0006292774193548386, 'epoch': 0.02}\n",
      "{'loss': 4.1884, 'grad_norm': 0.482421875, 'learning_rate': 0.0006272451612903226, 'epoch': 0.02}\n",
      "{'loss': 4.1778, 'grad_norm': 0.47265625, 'learning_rate': 0.0006252129032258064, 'epoch': 0.02}\n",
      "{'loss': 4.1299, 'grad_norm': 0.4375, 'learning_rate': 0.0006231806451612903, 'epoch': 0.02}\n",
      "{'loss': 4.0729, 'grad_norm': 0.5078125, 'learning_rate': 0.0006211483870967741, 'epoch': 0.02}\n",
      "{'loss': 4.1565, 'grad_norm': 0.419921875, 'learning_rate': 0.0006191161290322579, 'epoch': 0.02}\n",
      "{'loss': 4.167, 'grad_norm': 0.421875, 'learning_rate': 0.000617083870967742, 'epoch': 0.02}\n",
      "{'loss': 4.166, 'grad_norm': 0.439453125, 'learning_rate': 0.0006150516129032258, 'epoch': 0.02}\n",
      "{'loss': 4.2098, 'grad_norm': 0.431640625, 'learning_rate': 0.0006130193548387097, 'epoch': 0.02}\n",
      "{'loss': 3.9133, 'grad_norm': 0.482421875, 'learning_rate': 0.0006109870967741935, 'epoch': 0.02}\n",
      "{'loss': 4.1249, 'grad_norm': 0.470703125, 'learning_rate': 0.0006089548387096773, 'epoch': 0.02}\n",
      "{'loss': 3.9969, 'grad_norm': 0.470703125, 'learning_rate': 0.0006069225806451612, 'epoch': 0.02}\n",
      "{'loss': 4.033, 'grad_norm': 0.50390625, 'learning_rate': 0.0006048903225806451, 'epoch': 0.02}\n",
      "{'loss': 3.9884, 'grad_norm': 0.4140625, 'learning_rate': 0.0006028580645161289, 'epoch': 0.02}\n",
      "{'loss': 4.0232, 'grad_norm': 0.42578125, 'learning_rate': 0.0006008258064516129, 'epoch': 0.02}\n",
      "{'loss': 4.0789, 'grad_norm': 0.408203125, 'learning_rate': 0.0005987935483870967, 'epoch': 0.02}\n",
      "{'loss': 4.0805, 'grad_norm': 0.435546875, 'learning_rate': 0.0005967612903225807, 'epoch': 0.02}\n",
      "{'loss': 4.0462, 'grad_norm': 0.4140625, 'learning_rate': 0.0005947290322580645, 'epoch': 0.02}\n",
      "{'loss': 3.9913, 'grad_norm': 0.41796875, 'learning_rate': 0.0005926967741935483, 'epoch': 0.02}\n",
      "{'loss': 4.0652, 'grad_norm': 0.43359375, 'learning_rate': 0.0005906645161290322, 'epoch': 0.02}\n",
      "{'loss': 4.0017, 'grad_norm': 0.435546875, 'learning_rate': 0.000588632258064516, 'epoch': 0.02}\n",
      "{'loss': 4.0467, 'grad_norm': 0.4375, 'learning_rate': 0.0005866000000000001, 'epoch': 0.02}\n",
      "{'loss': 3.9504, 'grad_norm': 0.427734375, 'learning_rate': 0.0005845677419354839, 'epoch': 0.02}\n",
      "{'loss': 4.1199, 'grad_norm': 0.4453125, 'learning_rate': 0.0005825354838709677, 'epoch': 0.02}\n",
      "{'loss': 4.044, 'grad_norm': 0.4375, 'learning_rate': 0.0005805032258064516, 'epoch': 0.02}\n",
      "{'loss': 4.0121, 'grad_norm': 0.46484375, 'learning_rate': 0.0005784709677419354, 'epoch': 0.02}\n",
      "{'loss': 3.9407, 'grad_norm': 0.41796875, 'learning_rate': 0.0005764387096774192, 'epoch': 0.02}\n",
      "{'loss': 3.9057, 'grad_norm': 0.40234375, 'learning_rate': 0.0005744064516129033, 'epoch': 0.03}\n",
      "{'loss': 3.8537, 'grad_norm': 0.5234375, 'learning_rate': 0.000572374193548387, 'epoch': 0.03}\n",
      "{'loss': 3.9901, 'grad_norm': 0.44921875, 'learning_rate': 0.000570341935483871, 'epoch': 0.03}\n",
      "{'loss': 3.9606, 'grad_norm': 0.447265625, 'learning_rate': 0.0005683096774193548, 'epoch': 0.03}\n",
      "{'loss': 3.8117, 'grad_norm': 0.421875, 'learning_rate': 0.0005662774193548386, 'epoch': 0.03}\n",
      "{'loss': 3.872, 'grad_norm': 0.462890625, 'learning_rate': 0.0005642451612903226, 'epoch': 0.03}\n",
      "{'loss': 3.8629, 'grad_norm': 0.408203125, 'learning_rate': 0.0005622129032258064, 'epoch': 0.03}\n",
      "{'loss': 3.8546, 'grad_norm': 0.44140625, 'learning_rate': 0.0005601806451612902, 'epoch': 0.03}\n",
      "{'loss': 3.9864, 'grad_norm': 0.4296875, 'learning_rate': 0.0005581483870967742, 'epoch': 0.03}\n",
      "{'loss': 3.9252, 'grad_norm': 0.4453125, 'learning_rate': 0.000556116129032258, 'epoch': 0.03}\n",
      "{'loss': 3.9008, 'grad_norm': 0.466796875, 'learning_rate': 0.000554083870967742, 'epoch': 0.03}\n",
      "{'loss': 3.9787, 'grad_norm': 0.4453125, 'learning_rate': 0.0005520516129032258, 'epoch': 0.03}\n",
      "{'loss': 3.853, 'grad_norm': 0.412109375, 'learning_rate': 0.0005500193548387096, 'epoch': 0.03}\n",
      "{'loss': 3.8547, 'grad_norm': 0.4140625, 'learning_rate': 0.0005479870967741935, 'epoch': 0.03}\n",
      "{'loss': 3.834, 'grad_norm': 0.43359375, 'learning_rate': 0.0005459548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.8953, 'grad_norm': 0.4140625, 'learning_rate': 0.0005439225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.9946, 'grad_norm': 0.408203125, 'learning_rate': 0.0005418903225806451, 'epoch': 0.03}\n",
      "{'loss': 3.6884, 'grad_norm': 0.44140625, 'learning_rate': 0.0005398580645161289, 'epoch': 0.03}\n",
      "{'loss': 3.7325, 'grad_norm': 0.384765625, 'learning_rate': 0.0005378258064516129, 'epoch': 0.03}\n",
      "{'loss': 3.779, 'grad_norm': 0.38671875, 'learning_rate': 0.0005357935483870967, 'epoch': 0.03}\n",
      "{'loss': 3.7886, 'grad_norm': 0.400390625, 'learning_rate': 0.0005337612903225805, 'epoch': 0.03}\n",
      "{'loss': 3.7665, 'grad_norm': 0.4296875, 'learning_rate': 0.0005317290322580645, 'epoch': 0.03}\n",
      "{'loss': 3.7348, 'grad_norm': 0.431640625, 'learning_rate': 0.0005296967741935483, 'epoch': 0.03}\n",
      "{'loss': 3.7802, 'grad_norm': 0.416015625, 'learning_rate': 0.0005276645161290323, 'epoch': 0.03}\n",
      "{'loss': 3.797, 'grad_norm': 0.40234375, 'learning_rate': 0.0005256322580645161, 'epoch': 0.03}\n",
      "{'loss': 3.7704, 'grad_norm': 0.455078125, 'learning_rate': 0.0005235999999999999, 'epoch': 0.03}\n",
      "{'loss': 3.7399, 'grad_norm': 0.427734375, 'learning_rate': 0.0005215677419354839, 'epoch': 0.03}\n",
      "{'loss': 3.7339, 'grad_norm': 0.412109375, 'learning_rate': 0.0005195354838709677, 'epoch': 0.03}\n",
      "{'loss': 3.7947, 'grad_norm': 0.41015625, 'learning_rate': 0.0005175032258064515, 'epoch': 0.03}\n",
      "{'loss': 3.8254, 'grad_norm': 0.455078125, 'learning_rate': 0.0005154709677419354, 'epoch': 0.03}\n",
      "{'loss': 3.7258, 'grad_norm': 0.40234375, 'learning_rate': 0.0005134387096774193, 'epoch': 0.03}\n",
      "{'loss': 3.7509, 'grad_norm': 0.400390625, 'learning_rate': 0.0005114064516129032, 'epoch': 0.03}\n",
      "{'loss': 3.677, 'grad_norm': 0.42578125, 'learning_rate': 0.0005093741935483871, 'epoch': 0.03}\n",
      "{'loss': 3.7139, 'grad_norm': 0.376953125, 'learning_rate': 0.0005073419354838709, 'epoch': 0.03}\n",
      "{'loss': 3.7375, 'grad_norm': 0.41015625, 'learning_rate': 0.0005053096774193548, 'epoch': 0.03}\n",
      "{'loss': 3.6206, 'grad_norm': 0.42578125, 'learning_rate': 0.0005032774193548386, 'epoch': 0.03}\n",
      "{'loss': 3.6434, 'grad_norm': 0.384765625, 'learning_rate': 0.0005012451612903226, 'epoch': 0.03}\n",
      "{'loss': 3.7604, 'grad_norm': 0.416015625, 'learning_rate': 0.0004992129032258064, 'epoch': 0.03}\n",
      "{'loss': 3.659, 'grad_norm': 0.421875, 'learning_rate': 0.0004971806451612902, 'epoch': 0.03}\n",
      "{'loss': 3.7299, 'grad_norm': 0.416015625, 'learning_rate': 0.0004951483870967742, 'epoch': 0.03}\n",
      "{'loss': 3.7111, 'grad_norm': 0.41015625, 'learning_rate': 0.000493116129032258, 'epoch': 0.03}\n",
      "{'loss': 3.5974, 'grad_norm': 0.38671875, 'learning_rate': 0.000491083870967742, 'epoch': 0.03}\n",
      "{'loss': 3.7257, 'grad_norm': 0.37109375, 'learning_rate': 0.0004890516129032258, 'epoch': 0.03}\n",
      "{'loss': 3.6866, 'grad_norm': 0.37890625, 'learning_rate': 0.0004870193548387096, 'epoch': 0.03}\n",
      "{'loss': 3.6072, 'grad_norm': 0.423828125, 'learning_rate': 0.00048498709677419346, 'epoch': 0.03}\n",
      "{'loss': 3.6581, 'grad_norm': 0.40234375, 'learning_rate': 0.0004829548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.7355, 'grad_norm': 0.37890625, 'learning_rate': 0.0004809225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.5404, 'grad_norm': 0.38671875, 'learning_rate': 0.00047889032258064513, 'epoch': 0.03}\n",
      "{'loss': 3.6842, 'grad_norm': 0.419921875, 'learning_rate': 0.000476858064516129, 'epoch': 0.03}\n",
      "{'loss': 3.5375, 'grad_norm': 0.396484375, 'learning_rate': 0.00047482580645161285, 'epoch': 0.03}\n",
      "{'loss': 3.7285, 'grad_norm': 0.400390625, 'learning_rate': 0.0004727935483870967, 'epoch': 0.03}\n",
      "{'loss': 3.6084, 'grad_norm': 0.41015625, 'learning_rate': 0.0004707612903225806, 'epoch': 0.03}\n",
      "{'loss': 3.6129, 'grad_norm': 0.3984375, 'learning_rate': 0.0004687290322580644, 'epoch': 0.03}\n",
      "{'loss': 3.5092, 'grad_norm': 0.43359375, 'learning_rate': 0.0004666967741935484, 'epoch': 0.03}\n",
      "{'loss': 3.5876, 'grad_norm': 0.41015625, 'learning_rate': 0.00046466451612903225, 'epoch': 0.03}\n",
      "{'loss': 3.5284, 'grad_norm': 0.390625, 'learning_rate': 0.0004626322580645161, 'epoch': 0.03}\n",
      "{'loss': 3.638, 'grad_norm': 0.384765625, 'learning_rate': 0.0004606, 'epoch': 0.03}\n",
      "{'loss': 3.5263, 'grad_norm': 0.37890625, 'learning_rate': 0.0004585677419354838, 'epoch': 0.03}\n",
      "{'loss': 3.5353, 'grad_norm': 0.400390625, 'learning_rate': 0.0004565354838709677, 'epoch': 0.03}\n",
      "{'loss': 3.5116, 'grad_norm': 0.38671875, 'learning_rate': 0.0004545032258064516, 'epoch': 0.03}\n",
      "{'loss': 3.4871, 'grad_norm': 0.4375, 'learning_rate': 0.0004524709677419354, 'epoch': 0.03}\n",
      "{'loss': 3.4933, 'grad_norm': 0.365234375, 'learning_rate': 0.00045043870967741937, 'epoch': 0.04}\n",
      "{'loss': 3.5265, 'grad_norm': 0.40625, 'learning_rate': 0.0004484064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.5162, 'grad_norm': 0.396484375, 'learning_rate': 0.0004463741935483871, 'epoch': 0.04}\n",
      "{'loss': 3.5155, 'grad_norm': 0.388671875, 'learning_rate': 0.00044434193548387093, 'epoch': 0.04}\n",
      "{'loss': 3.4806, 'grad_norm': 0.359375, 'learning_rate': 0.00044230967741935477, 'epoch': 0.04}\n",
      "{'loss': 3.5072, 'grad_norm': 0.3828125, 'learning_rate': 0.0004402774193548387, 'epoch': 0.04}\n",
      "{'loss': 3.4144, 'grad_norm': 0.3671875, 'learning_rate': 0.00043824516129032254, 'epoch': 0.04}\n",
      "{'loss': 3.5585, 'grad_norm': 0.376953125, 'learning_rate': 0.0004362129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.5678, 'grad_norm': 0.3671875, 'learning_rate': 0.0004341806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.358, 'grad_norm': 0.427734375, 'learning_rate': 0.00043214838709677416, 'epoch': 0.04}\n",
      "{'loss': 3.5108, 'grad_norm': 0.419921875, 'learning_rate': 0.00043011612903225805, 'epoch': 0.04}\n",
      "{'loss': 3.4789, 'grad_norm': 0.4140625, 'learning_rate': 0.0004280838709677419, 'epoch': 0.04}\n",
      "{'loss': 3.558, 'grad_norm': 0.388671875, 'learning_rate': 0.0004260516129032258, 'epoch': 0.04}\n",
      "{'loss': 3.4933, 'grad_norm': 0.373046875, 'learning_rate': 0.00042401935483870966, 'epoch': 0.04}\n",
      "{'loss': 3.4828, 'grad_norm': 0.376953125, 'learning_rate': 0.0004219870967741935, 'epoch': 0.04}\n",
      "{'loss': 3.4499, 'grad_norm': 0.400390625, 'learning_rate': 0.00041995483870967734, 'epoch': 0.04}\n",
      "{'loss': 3.4912, 'grad_norm': 0.35546875, 'learning_rate': 0.0004179225806451613, 'epoch': 0.04}\n",
      "{'loss': 3.4443, 'grad_norm': 0.3671875, 'learning_rate': 0.0004158903225806451, 'epoch': 0.04}\n",
      "{'loss': 3.4784, 'grad_norm': 0.388671875, 'learning_rate': 0.000413858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.4652, 'grad_norm': 0.376953125, 'learning_rate': 0.0004118258064516129, 'epoch': 0.04}\n",
      "{'loss': 3.5129, 'grad_norm': 0.369140625, 'learning_rate': 0.00040979354838709673, 'epoch': 0.04}\n",
      "{'loss': 3.3576, 'grad_norm': 0.384765625, 'learning_rate': 0.0004077612903225806, 'epoch': 0.04}\n",
      "{'loss': 3.4001, 'grad_norm': 0.357421875, 'learning_rate': 0.00040572903225806446, 'epoch': 0.04}\n",
      "{'loss': 3.4312, 'grad_norm': 0.39453125, 'learning_rate': 0.00040369677419354835, 'epoch': 0.04}\n",
      "{'loss': 3.4591, 'grad_norm': 0.369140625, 'learning_rate': 0.00040166451612903224, 'epoch': 0.04}\n",
      "{'loss': 3.3329, 'grad_norm': 0.40625, 'learning_rate': 0.00039963225806451607, 'epoch': 0.04}\n",
      "{'loss': 3.3461, 'grad_norm': 0.3515625, 'learning_rate': 0.00039759999999999996, 'epoch': 0.04}\n",
      "{'loss': 3.3844, 'grad_norm': 0.412109375, 'learning_rate': 0.00039556774193548385, 'epoch': 0.04}\n",
      "{'loss': 3.4108, 'grad_norm': 0.369140625, 'learning_rate': 0.00039353548387096774, 'epoch': 0.04}\n",
      "{'loss': 3.5249, 'grad_norm': 0.365234375, 'learning_rate': 0.0003915032258064516, 'epoch': 0.04}\n",
      "{'loss': 3.3536, 'grad_norm': 0.443359375, 'learning_rate': 0.0003894709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.3982, 'grad_norm': 0.443359375, 'learning_rate': 0.0003874387096774193, 'epoch': 0.04}\n",
      "{'loss': 3.327, 'grad_norm': 0.3671875, 'learning_rate': 0.0003854064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.3699, 'grad_norm': 0.384765625, 'learning_rate': 0.000383374193548387, 'epoch': 0.04}\n",
      "{'loss': 3.3567, 'grad_norm': 0.35546875, 'learning_rate': 0.0003813419354838709, 'epoch': 0.04}\n",
      "{'loss': 3.4795, 'grad_norm': 0.3515625, 'learning_rate': 0.0003793096774193548, 'epoch': 0.04}\n",
      "{'loss': 3.4111, 'grad_norm': 0.34765625, 'learning_rate': 0.00037727741935483875, 'epoch': 0.04}\n",
      "{'loss': 3.3832, 'grad_norm': 0.37109375, 'learning_rate': 0.00037524516129032253, 'epoch': 0.04}\n",
      "{'loss': 3.356, 'grad_norm': 0.3515625, 'learning_rate': 0.0003732129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.3264, 'grad_norm': 0.37890625, 'learning_rate': 0.0003711806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.3712, 'grad_norm': 0.376953125, 'learning_rate': 0.00036914838709677415, 'epoch': 0.04}\n",
      "{'loss': 3.3841, 'grad_norm': 0.52734375, 'learning_rate': 0.000367116129032258, 'epoch': 0.04}\n",
      "{'loss': 3.3686, 'grad_norm': 0.38671875, 'learning_rate': 0.0003650838709677419, 'epoch': 0.04}\n",
      "{'loss': 3.4301, 'grad_norm': 0.373046875, 'learning_rate': 0.00036305161290322576, 'epoch': 0.04}\n",
      "{'loss': 3.3936, 'grad_norm': 0.375, 'learning_rate': 0.00036101935483870965, 'epoch': 0.04}\n",
      "{'loss': 3.3648, 'grad_norm': 0.376953125, 'learning_rate': 0.00035898709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.4222, 'grad_norm': 0.37109375, 'learning_rate': 0.0003569548387096773, 'epoch': 0.04}\n",
      "{'loss': 3.3901, 'grad_norm': 0.3671875, 'learning_rate': 0.00035492258064516127, 'epoch': 0.04}\n",
      "{'loss': 3.3875, 'grad_norm': 0.345703125, 'learning_rate': 0.0003528903225806451, 'epoch': 0.04}\n",
      "{'loss': 3.3992, 'grad_norm': 0.353515625, 'learning_rate': 0.000350858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.335, 'grad_norm': 0.357421875, 'learning_rate': 0.0003488258064516129, 'epoch': 0.04}\n",
      "{'loss': 3.357, 'grad_norm': 0.380859375, 'learning_rate': 0.0003467935483870967, 'epoch': 0.04}\n",
      "{'loss': 3.3721, 'grad_norm': 0.36328125, 'learning_rate': 0.00034476129032258066, 'epoch': 0.04}\n",
      "{'loss': 3.2303, 'grad_norm': 0.37109375, 'learning_rate': 0.00034272903225806444, 'epoch': 0.04}\n",
      "{'loss': 3.3624, 'grad_norm': 0.357421875, 'learning_rate': 0.0003406967741935484, 'epoch': 0.04}\n",
      "{'loss': 3.4316, 'grad_norm': 0.365234375, 'learning_rate': 0.0003386645161290322, 'epoch': 0.04}\n",
      "{'loss': 3.3784, 'grad_norm': 0.375, 'learning_rate': 0.00033663225806451606, 'epoch': 0.04}\n",
      "{'loss': 3.1377, 'grad_norm': 0.36328125, 'learning_rate': 0.0003346, 'epoch': 0.04}\n",
      "{'loss': 3.1974, 'grad_norm': 0.3671875, 'learning_rate': 0.00033256774193548384, 'epoch': 0.04}\n",
      "{'loss': 3.3239, 'grad_norm': 0.35546875, 'learning_rate': 0.0003305354838709678, 'epoch': 0.04}\n",
      "{'loss': 3.3289, 'grad_norm': 0.37890625, 'learning_rate': 0.00032850322580645156, 'epoch': 0.05}\n",
      "{'loss': 3.2574, 'grad_norm': 0.375, 'learning_rate': 0.00032647096774193545, 'epoch': 0.05}\n",
      "{'loss': 3.318, 'grad_norm': 0.337890625, 'learning_rate': 0.0003244387096774194, 'epoch': 0.05}\n",
      "{'loss': 3.3911, 'grad_norm': 0.3515625, 'learning_rate': 0.0003224064516129032, 'epoch': 0.05}\n",
      "{'loss': 3.3898, 'grad_norm': 0.375, 'learning_rate': 0.00032037419354838707, 'epoch': 0.05}\n",
      "{'loss': 3.3202, 'grad_norm': 0.369140625, 'learning_rate': 0.00031834193548387096, 'epoch': 0.05}\n",
      "{'loss': 3.2586, 'grad_norm': 0.3515625, 'learning_rate': 0.0003163096774193548, 'epoch': 0.05}\n",
      "{'loss': 3.2599, 'grad_norm': 0.400390625, 'learning_rate': 0.0003142774193548387, 'epoch': 0.05}\n",
      "{'loss': 3.3186, 'grad_norm': 0.3515625, 'learning_rate': 0.00031224516129032257, 'epoch': 0.05}\n",
      "{'loss': 3.2553, 'grad_norm': 0.33984375, 'learning_rate': 0.0003102129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.3423, 'grad_norm': 0.35546875, 'learning_rate': 0.0003081806451612903, 'epoch': 0.05}\n",
      "{'loss': 3.4161, 'grad_norm': 0.40234375, 'learning_rate': 0.0003061483870967742, 'epoch': 0.05}\n",
      "{'loss': 3.2174, 'grad_norm': 0.3984375, 'learning_rate': 0.00030411612903225797, 'epoch': 0.05}\n",
      "{'loss': 3.365, 'grad_norm': 0.353515625, 'learning_rate': 0.0003020838709677419, 'epoch': 0.05}\n",
      "{'loss': 3.2268, 'grad_norm': 0.359375, 'learning_rate': 0.00030005161290322575, 'epoch': 0.05}\n",
      "{'loss': 3.2756, 'grad_norm': 0.353515625, 'learning_rate': 0.0002980193548387097, 'epoch': 0.05}\n",
      "{'loss': 3.1288, 'grad_norm': 0.357421875, 'learning_rate': 0.00029598709677419353, 'epoch': 0.05}\n",
      "{'loss': 3.281, 'grad_norm': 0.345703125, 'learning_rate': 0.00029395483870967736, 'epoch': 0.05}\n",
      "{'loss': 3.1725, 'grad_norm': 0.341796875, 'learning_rate': 0.0002919225806451613, 'epoch': 0.05}\n",
      "{'loss': 3.2327, 'grad_norm': 0.34375, 'learning_rate': 0.0002898903225806451, 'epoch': 0.05}\n",
      "{'loss': 3.2344, 'grad_norm': 0.3671875, 'learning_rate': 0.00028785806451612903, 'epoch': 0.05}\n",
      "{'loss': 3.249, 'grad_norm': 0.328125, 'learning_rate': 0.00028582580645161287, 'epoch': 0.05}\n",
      "{'loss': 3.2599, 'grad_norm': 0.37890625, 'learning_rate': 0.00028379354838709676, 'epoch': 0.05}\n",
      "{'loss': 3.304, 'grad_norm': 0.337890625, 'learning_rate': 0.00028176129032258065, 'epoch': 0.05}\n",
      "{'loss': 3.1906, 'grad_norm': 0.33984375, 'learning_rate': 0.0002797290322580645, 'epoch': 0.05}\n",
      "{'loss': 3.3135, 'grad_norm': 0.3515625, 'learning_rate': 0.00027769677419354843, 'epoch': 0.05}\n",
      "{'loss': 3.0904, 'grad_norm': 0.359375, 'learning_rate': 0.0002756645161290322, 'epoch': 0.05}\n",
      "{'loss': 3.1944, 'grad_norm': 0.353515625, 'learning_rate': 0.0002736322580645161, 'epoch': 0.05}\n",
      "{'loss': 3.2556, 'grad_norm': 0.37109375, 'learning_rate': 0.0002716, 'epoch': 0.05}\n",
      "{'loss': 3.1987, 'grad_norm': 0.3359375, 'learning_rate': 0.0002695677419354838, 'epoch': 0.05}\n",
      "{'loss': 3.2256, 'grad_norm': 0.337890625, 'learning_rate': 0.0002675354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.2466, 'grad_norm': 0.353515625, 'learning_rate': 0.0002655032258064516, 'epoch': 0.05}\n",
      "{'loss': 3.2351, 'grad_norm': 0.376953125, 'learning_rate': 0.00026347096774193544, 'epoch': 0.05}\n",
      "{'loss': 3.1488, 'grad_norm': 0.3515625, 'learning_rate': 0.00026143870967741933, 'epoch': 0.05}\n",
      "{'loss': 3.2345, 'grad_norm': 0.41015625, 'learning_rate': 0.0002594064516129032, 'epoch': 0.05}\n",
      "{'loss': 3.2462, 'grad_norm': 0.34375, 'learning_rate': 0.000257374193548387, 'epoch': 0.05}\n",
      "{'loss': 3.1951, 'grad_norm': 0.337890625, 'learning_rate': 0.00025534193548387094, 'epoch': 0.05}\n",
      "{'loss': 3.163, 'grad_norm': 0.361328125, 'learning_rate': 0.00025330967741935483, 'epoch': 0.05}\n",
      "{'loss': 3.1352, 'grad_norm': 0.369140625, 'learning_rate': 0.0002512774193548387, 'epoch': 0.05}\n",
      "{'loss': 3.2105, 'grad_norm': 0.33203125, 'learning_rate': 0.00024924516129032256, 'epoch': 0.05}\n",
      "{'loss': 3.1889, 'grad_norm': 0.349609375, 'learning_rate': 0.0002472129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.2055, 'grad_norm': 0.353515625, 'learning_rate': 0.00024518064516129034, 'epoch': 0.05}\n",
      "{'loss': 3.1189, 'grad_norm': 0.3515625, 'learning_rate': 0.00024314838709677412, 'epoch': 0.05}\n",
      "{'loss': 3.1413, 'grad_norm': 0.341796875, 'learning_rate': 0.00024111612903225804, 'epoch': 0.05}\n",
      "{'loss': 3.1009, 'grad_norm': 0.34375, 'learning_rate': 0.00023908387096774195, 'epoch': 0.05}\n",
      "{'loss': 3.168, 'grad_norm': 0.322265625, 'learning_rate': 0.00023705161290322582, 'epoch': 0.05}\n",
      "{'loss': 3.2234, 'grad_norm': 0.35546875, 'learning_rate': 0.00023501935483870965, 'epoch': 0.05}\n",
      "{'loss': 3.1853, 'grad_norm': 0.330078125, 'learning_rate': 0.00023298709677419351, 'epoch': 0.05}\n",
      "{'loss': 3.1, 'grad_norm': 0.33203125, 'learning_rate': 0.00023095483870967743, 'epoch': 0.05}\n",
      "{'loss': 3.233, 'grad_norm': 0.3515625, 'learning_rate': 0.0002289225806451612, 'epoch': 0.05}\n",
      "{'loss': 3.1278, 'grad_norm': 0.326171875, 'learning_rate': 0.00022689032258064513, 'epoch': 0.05}\n",
      "{'loss': 3.1078, 'grad_norm': 0.353515625, 'learning_rate': 0.00022485806451612905, 'epoch': 0.05}\n",
      "{'loss': 3.1849, 'grad_norm': 0.33984375, 'learning_rate': 0.00022282580645161285, 'epoch': 0.05}\n",
      "{'loss': 3.1357, 'grad_norm': 0.353515625, 'learning_rate': 0.00022079354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.2365, 'grad_norm': 0.3515625, 'learning_rate': 0.0002187612903225806, 'epoch': 0.05}\n",
      "{'loss': 3.0625, 'grad_norm': 0.373046875, 'learning_rate': 0.00021672903225806447, 'epoch': 0.05}\n",
      "{'loss': 3.2634, 'grad_norm': 0.353515625, 'learning_rate': 0.0002146967741935484, 'epoch': 0.05}\n",
      "{'loss': 3.1346, 'grad_norm': 0.33203125, 'learning_rate': 0.00021266451612903225, 'epoch': 0.05}\n",
      "{'loss': 3.166, 'grad_norm': 0.337890625, 'learning_rate': 0.00021063225806451617, 'epoch': 0.05}\n",
      "{'loss': 3.1896, 'grad_norm': 0.322265625, 'learning_rate': 0.00020859999999999995, 'epoch': 0.05}\n",
      "{'loss': 3.0857, 'grad_norm': 0.359375, 'learning_rate': 0.00020656774193548386, 'epoch': 0.05}\n",
      "{'loss': 3.1155, 'grad_norm': 0.3828125, 'learning_rate': 0.00020453548387096773, 'epoch': 0.06}\n",
      "{'loss': 3.1326, 'grad_norm': 0.33203125, 'learning_rate': 0.00020250322580645156, 'epoch': 0.06}\n",
      "{'loss': 3.0766, 'grad_norm': 0.361328125, 'learning_rate': 0.00020047096774193548, 'epoch': 0.06}\n",
      "{'loss': 3.0949, 'grad_norm': 0.361328125, 'learning_rate': 0.00019843870967741934, 'epoch': 0.06}\n",
      "{'loss': 3.138, 'grad_norm': 0.36328125, 'learning_rate': 0.00019640645161290326, 'epoch': 0.06}\n",
      "{'loss': 3.1731, 'grad_norm': 0.3671875, 'learning_rate': 0.00019437419354838704, 'epoch': 0.06}\n",
      "{'loss': 3.1285, 'grad_norm': 0.341796875, 'learning_rate': 0.00019234193548387096, 'epoch': 0.06}\n",
      "{'loss': 3.1379, 'grad_norm': 0.3515625, 'learning_rate': 0.00019030967741935482, 'epoch': 0.06}\n",
      "{'loss': 3.1755, 'grad_norm': 0.333984375, 'learning_rate': 0.00018827741935483868, 'epoch': 0.06}\n",
      "{'loss': 3.0262, 'grad_norm': 0.3359375, 'learning_rate': 0.0001862451612903226, 'epoch': 0.06}\n",
      "{'loss': 3.1404, 'grad_norm': 0.349609375, 'learning_rate': 0.00018421290322580646, 'epoch': 0.06}\n",
      "{'loss': 3.06, 'grad_norm': 0.3203125, 'learning_rate': 0.0001821806451612903, 'epoch': 0.06}\n",
      "{'loss': 3.2104, 'grad_norm': 0.353515625, 'learning_rate': 0.00018014838709677416, 'epoch': 0.06}\n",
      "{'loss': 3.0479, 'grad_norm': 0.337890625, 'learning_rate': 0.00017811612903225808, 'epoch': 0.06}\n",
      "{'loss': 3.0623, 'grad_norm': 0.357421875, 'learning_rate': 0.00017608387096774186, 'epoch': 0.06}\n",
      "{'loss': 3.1323, 'grad_norm': 0.333984375, 'learning_rate': 0.00017405161290322578, 'epoch': 0.06}\n",
      "{'loss': 3.131, 'grad_norm': 0.337890625, 'learning_rate': 0.0001720193548387097, 'epoch': 0.06}\n",
      "{'loss': 3.1011, 'grad_norm': 0.328125, 'learning_rate': 0.0001699870967741935, 'epoch': 0.06}\n",
      "{'loss': 3.0483, 'grad_norm': 0.34375, 'learning_rate': 0.00016795483870967742, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 1763/30314 [29:59<7:58:56,  1.01s/it]2025-11-03 18:05:21,956 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.46 c\n",
      "{'train_runtime': 1800.4817, 'train_samples_per_second': 1077.524, 'train_steps_per_second': 16.837, 'train_loss': 4.1928955946379505, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 1764/30314 [30:00<8:05:40,  1.02s/it]\n",
      "2025-11-03 18:05:22,144 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:48<00:00, 25.89it/s]\n",
      "2025-11-03 18:06:10,884 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:06:12,030 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:06:12,030 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ. –í –Ω–∞–∑–≤–∞–Ω–∏–∏ ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–æ–π¬ª (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–∞—è¬ª), –∫–æ—Ç–æ—Ä–æ–µ –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\n",
      "\n",
      "–í –Ω–∞—á–∞–ª–µ XVI –≤–µ–∫–∞, –∫–æ–≥–¥–∞ –≤ XVII –≤–µ–∫–µ, –≤ –Ω–∞—á–∞–ª–µ XIX –≤–µ–∫–∞, –≤ —Å–æ—Å—Ç–∞–≤ –Ω–æ–≤–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏—è –ú–æ–≥–∏–ª–∞-–ö–æ–∫—É—Å—Å–∫–æ–≥–æ (–≤\n",
      "2025-11-03 18:06:12,030 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:06:12,030 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.4817, 'train_samples_per_second': 1077.524, 'train_steps_per_second': 16.837, 'total_flos': 2.9755307430877594e+17, 'train_loss': 4.1928955946379505, 'epoch': 0.05819189470038102}, 'eval': {'eval_loss': 3.7759594917297363, 'eval_runtime': 48.738, 'eval_samples_per_second': 102.589, 'eval_steps_per_second': 25.647, 'epoch': 0.05819189470038102}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ. –í –Ω–∞–∑–≤–∞–Ω–∏–∏ ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–æ–π¬ª (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–ü–æ–≥–æ—Ä–æ–¥—Å–∫–∞—è¬ª), –∫–æ—Ç–æ—Ä–æ–µ –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\\n\\n–í –Ω–∞—á–∞–ª–µ XVI –≤–µ–∫–∞, –∫–æ–≥–¥–∞ –≤ XVII –≤–µ–∫–µ, –≤ –Ω–∞—á–∞–ª–µ XIX –≤–µ–∫–∞, –≤ —Å–æ—Å—Ç–∞–≤ –Ω–æ–≤–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–º–µ–Ω–∏—è –ú–æ–≥–∏–ª–∞-–ö–æ–∫—É—Å—Å–∫–æ–≥–æ (–≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_Baseline_1_GPU\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/sd6qsqe4\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_173519-sd6qsqe4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "name = 'Baseline_1_GPU'  # 21005 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=1 accelerate launch \\\n",
    "  --num_processes 1 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --bf16 \\\n",
    "    --torch-compile \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46259d7a",
   "metadata": {},
   "source": [
    "### 1_GPU_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b39728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: baseline\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: World size: 1\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0\n",
      "2025-11-03 17:38:49,235 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:38:49,236 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ baseline setup (single GPU)\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:39:25,437 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_173928-fi6xxlxh\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs32_ga4_1_GPU_v2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/fi6xxlxh\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 32, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:39:29,927 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:39:29,928 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/15157 [00:00<?, ?it/s]W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:30.799000 2652045 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 17:39:39.498000 2652045 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.2178, 'grad_norm': 20.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.6646, 'grad_norm': 4.375, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6086, 'grad_norm': 3.125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4485, 'grad_norm': 2.125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2855, 'grad_norm': 1.5703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2741, 'grad_norm': 0.8046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1588, 'grad_norm': 0.70703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0711, 'grad_norm': 1.046875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0311, 'grad_norm': 0.53515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9181, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7596, 'grad_norm': 0.63671875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.65, 'grad_norm': 1.0078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4664, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3416, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.234, 'grad_norm': 0.9921875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0975, 'grad_norm': 0.76953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0061, 'grad_norm': 0.87890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9141, 'grad_norm': 1.0703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7888, 'grad_norm': 1.1953125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6668, 'grad_norm': 0.80859375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 1.203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5382, 'grad_norm': 0.73828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4091, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3719, 'grad_norm': 0.66796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1828, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0909, 'grad_norm': 0.61328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1185, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0292, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9103, 'grad_norm': 0.482421875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8807, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7463, 'grad_norm': 0.6328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.7806, 'grad_norm': 0.59765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6389, 'grad_norm': 0.62109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5508, 'grad_norm': 0.609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.591, 'grad_norm': 0.64453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.52, 'grad_norm': 0.486328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.4166, 'grad_norm': 0.7265625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3317, 'grad_norm': 0.67578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3242, 'grad_norm': 0.55078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3827, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.3428, 'grad_norm': 0.60546875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1412, 'grad_norm': 0.58203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.1289, 'grad_norm': 0.453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.0632, 'grad_norm': 0.466796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.115, 'grad_norm': 0.4296875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 4.9602, 'grad_norm': 0.494140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0676, 'grad_norm': 0.494140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0047, 'grad_norm': 0.490234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9609, 'grad_norm': 0.45703125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9024, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8252, 'grad_norm': 0.53125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9043, 'grad_norm': 0.462890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8285, 'grad_norm': 0.50390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.8326, 'grad_norm': 0.44140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.744, 'grad_norm': 0.455078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.7536, 'grad_norm': 0.43359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6146, 'grad_norm': 0.46484375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6642, 'grad_norm': 0.474609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.7544, 'grad_norm': 0.408203125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.649, 'grad_norm': 0.439453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6637, 'grad_norm': 0.53515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6458, 'grad_norm': 0.5234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.6243, 'grad_norm': 0.40625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5585, 'grad_norm': 0.390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5093, 'grad_norm': 0.423828125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.5621, 'grad_norm': 0.4609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4421, 'grad_norm': 0.400390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4455, 'grad_norm': 0.431640625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.388, 'grad_norm': 0.384765625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4292, 'grad_norm': 0.37890625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.4434, 'grad_norm': 0.43359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3991, 'grad_norm': 0.349609375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3895, 'grad_norm': 0.4140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.3978, 'grad_norm': 0.455078125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.395, 'grad_norm': 0.392578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.2923, 'grad_norm': 0.443359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.287, 'grad_norm': 0.40234375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2475, 'grad_norm': 0.390625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2237, 'grad_norm': 0.42578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2793, 'grad_norm': 0.44140625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2741, 'grad_norm': 0.39453125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.283, 'grad_norm': 0.478515625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2027, 'grad_norm': 0.361328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.2928, 'grad_norm': 0.41015625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.0646, 'grad_norm': 0.375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1405, 'grad_norm': 0.392578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1003, 'grad_norm': 0.443359375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1559, 'grad_norm': 0.37109375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1283, 'grad_norm': 0.466796875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1304, 'grad_norm': 0.34375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.1321, 'grad_norm': 0.373046875, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.0703, 'grad_norm': 0.3515625, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.0967, 'grad_norm': 0.48046875, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 3.992, 'grad_norm': 0.3828125, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.0744, 'grad_norm': 0.359375, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.0947, 'grad_norm': 0.3359375, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.0249, 'grad_norm': 0.333984375, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.0022, 'grad_norm': 0.345703125, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.0675, 'grad_norm': 0.349609375, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 3.9764, 'grad_norm': 0.3359375, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.005, 'grad_norm': 0.35546875, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 3.9821, 'grad_norm': 0.34375, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 3.9256, 'grad_norm': 0.35546875, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 3.9588, 'grad_norm': 0.3515625, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 3.9006, 'grad_norm': 0.396484375, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 3.8624, 'grad_norm': 0.33203125, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 3.8759, 'grad_norm': 0.380859375, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 3.8686, 'grad_norm': 0.3203125, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 3.8634, 'grad_norm': 0.345703125, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 3.8465, 'grad_norm': 0.34765625, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 3.8116, 'grad_norm': 0.373046875, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 3.8536, 'grad_norm': 0.330078125, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 3.8893, 'grad_norm': 0.345703125, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 3.8247, 'grad_norm': 0.375, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 3.8143, 'grad_norm': 0.3203125, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 3.8233, 'grad_norm': 0.314453125, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 3.7915, 'grad_norm': 0.384765625, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 3.7602, 'grad_norm': 0.3203125, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 3.7373, 'grad_norm': 0.328125, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 3.7163, 'grad_norm': 0.3359375, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 3.814, 'grad_norm': 0.34765625, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 3.7119, 'grad_norm': 0.341796875, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 3.6795, 'grad_norm': 0.390625, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 3.7561, 'grad_norm': 0.341796875, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 3.7325, 'grad_norm': 0.376953125, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 3.6696, 'grad_norm': 0.357421875, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 3.7054, 'grad_norm': 0.41796875, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 3.7237, 'grad_norm': 0.328125, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 3.7056, 'grad_norm': 0.32421875, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 3.7291, 'grad_norm': 0.34765625, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 3.7102, 'grad_norm': 0.30859375, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 3.6603, 'grad_norm': 0.306640625, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 3.6216, 'grad_norm': 0.337890625, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 3.7018, 'grad_norm': 0.3125, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 3.5705, 'grad_norm': 0.34375, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 3.5655, 'grad_norm': 0.306640625, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 3.6044, 'grad_norm': 0.322265625, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 3.6511, 'grad_norm': 0.30859375, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 3.6614, 'grad_norm': 0.3125, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 3.5474, 'grad_norm': 0.322265625, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 3.572, 'grad_norm': 0.32421875, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 3.6659, 'grad_norm': 0.376953125, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 3.5706, 'grad_norm': 0.31640625, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 3.5303, 'grad_norm': 0.353515625, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 3.4868, 'grad_norm': 0.318359375, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 3.4716, 'grad_norm': 0.32421875, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 3.5134, 'grad_norm': 0.3203125, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 3.5556, 'grad_norm': 0.33203125, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 3.5228, 'grad_norm': 0.345703125, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 3.4094, 'grad_norm': 0.30078125, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 3.4992, 'grad_norm': 0.296875, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 3.4978, 'grad_norm': 0.318359375, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 3.4548, 'grad_norm': 0.32421875, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 3.5087, 'grad_norm': 0.28515625, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 3.4366, 'grad_norm': 0.310546875, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 3.4329, 'grad_norm': 0.296875, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 3.4529, 'grad_norm': 0.302734375, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 3.3886, 'grad_norm': 0.30078125, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 3.3886, 'grad_norm': 0.271484375, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 3.4548, 'grad_norm': 0.279296875, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 3.4107, 'grad_norm': 0.287109375, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 3.3581, 'grad_norm': 0.2890625, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 3.4017, 'grad_norm': 0.3046875, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 3.3939, 'grad_norm': 0.3046875, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 3.439, 'grad_norm': 0.29296875, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 3.4206, 'grad_norm': 0.27734375, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 3.334, 'grad_norm': 0.29296875, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 3.3388, 'grad_norm': 0.294921875, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 3.355, 'grad_norm': 0.291015625, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 3.3845, 'grad_norm': 0.28515625, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 3.3881, 'grad_norm': 0.296875, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 3.3138, 'grad_norm': 0.28125, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 3.3624, 'grad_norm': 0.294921875, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 3.2829, 'grad_norm': 0.3125, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 3.3625, 'grad_norm': 0.29296875, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "{'loss': 3.2962, 'grad_norm': 0.28125, 'learning_rate': 0.0005256322580645161, 'epoch': 0.06}\n",
      "{'loss': 3.2926, 'grad_norm': 0.27734375, 'learning_rate': 0.0005235999999999999, 'epoch': 0.06}\n",
      "{'loss': 3.2885, 'grad_norm': 0.27734375, 'learning_rate': 0.0005215677419354839, 'epoch': 0.06}\n",
      "{'loss': 3.2715, 'grad_norm': 0.29296875, 'learning_rate': 0.0005195354838709677, 'epoch': 0.06}\n",
      "{'loss': 3.3443, 'grad_norm': 0.287109375, 'learning_rate': 0.0005175032258064515, 'epoch': 0.06}\n",
      "{'loss': 3.3378, 'grad_norm': 0.2890625, 'learning_rate': 0.0005154709677419354, 'epoch': 0.06}\n",
      "{'loss': 3.3411, 'grad_norm': 0.31640625, 'learning_rate': 0.0005134387096774193, 'epoch': 0.06}\n",
      "{'loss': 3.2782, 'grad_norm': 0.29296875, 'learning_rate': 0.0005114064516129032, 'epoch': 0.06}\n",
      "{'loss': 3.2895, 'grad_norm': 0.29296875, 'learning_rate': 0.0005093741935483871, 'epoch': 0.06}\n",
      "{'loss': 3.2627, 'grad_norm': 0.28515625, 'learning_rate': 0.0005073419354838709, 'epoch': 0.06}\n",
      "{'loss': 3.365, 'grad_norm': 0.287109375, 'learning_rate': 0.0005053096774193548, 'epoch': 0.06}\n",
      "{'loss': 3.258, 'grad_norm': 0.283203125, 'learning_rate': 0.0005032774193548386, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 939/15157 [29:58<7:32:45,  1.91s/it]2025-11-03 18:09:30,750 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.46 c\n",
      "{'loss': 3.2427, 'grad_norm': 0.279296875, 'learning_rate': 0.0005012451612903226, 'epoch': 0.06}\n",
      "{'train_runtime': 1800.4886, 'train_samples_per_second': 1077.52, 'train_steps_per_second': 8.418, 'train_loss': 4.574666688797322, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 940/15157 [30:00<7:33:51,  1.92s/it]\n",
      "2025-11-03 18:09:30,978 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [00:47<00:00, 26.46it/s]\n",
      "2025-11-03 18:10:18,677 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:10:19,744 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:10:19,744 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ –æ—Ç –Ω–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª (–≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Å ¬´–ê—Ä–∏-–ö–≤–∞–¥—Ä–∞—Ç¬ª).\n",
      "\n",
      "–í –Ω–∞—á–∞–ª–µ ¬´–¢—Ä–æ–ø–æ–ª–∏–π—Å–∫–æ–π¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª –≤ —Å–æ—Å—Ç–∞–≤–µ –≥—Ä—É–ø–ø—ã ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª —Å—Ç–∞–ª–∏ ¬´–ö—Ä—É–ø–Ω—ã–π –∫–∞–º–µ–Ω—å¬ª. –í\n",
      "2025-11-03 18:10:19,744 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:10:19,745 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.4886, 'train_samples_per_second': 1077.52, 'train_steps_per_second': 8.418, 'total_flos': 3.171200565195571e+17, 'train_loss': 4.574666688797322, 'epoch': 0.06201857258317251}, 'eval': {'eval_loss': 3.939300060272217, 'eval_runtime': 47.6959, 'eval_samples_per_second': 104.831, 'eval_steps_per_second': 26.208, 'epoch': 0.06201857258317251}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ –¥–∞–Ω–æ –æ—Ç –Ω–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª (–≤ –ø–µ—Ä–µ–≤–æ–¥–µ —Å ¬´–ê—Ä–∏-–ö–≤–∞–¥—Ä–∞—Ç¬ª).\\n\\n–í –Ω–∞—á–∞–ª–µ ¬´–¢—Ä–æ–ø–æ–ª–∏–π—Å–∫–æ–π¬ª –≤ ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–µ¬ª –≤ —Å–æ—Å—Ç–∞–≤–µ –≥—Ä—É–ø–ø—ã ¬´–ü–æ–≥–æ—Ä–µ–ª–∫–∞¬ª —Å—Ç–∞–ª–∏ ¬´–ö—Ä—É–ø–Ω—ã–π –∫–∞–º–µ–Ω—å¬ª. –í'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs32_ga4_1_GPU_v2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/fi6xxlxh\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_173928-fi6xxlxh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "name = '1_GPU_v2'  # 34353 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 accelerate launch \\\n",
    "  --num_processes 1 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29502 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --bf16 \\\n",
    "    --torch-compile \\\n",
    "    --batch-size 32 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a79970",
   "metadata": {},
   "source": [
    "### DeepSpeed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8434d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: World size: 2\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 17:41:17,720 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 17:41:17,721 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 17:41:17,721 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: ============================================================\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 17:41:17,731 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 17:41:53,995 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 17:41:54,121 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 17:41:56,263 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 17:41:56,275 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 17:41:56,278 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_174157-zci3incb\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_1\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zci3incb\n",
      "2025-11-03 17:41:58,242 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 17:41:58,243 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 17:41:58,243 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 17:41:58,243 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6922, 'grad_norm': 3.668818712234497, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5162, 'grad_norm': 5.018797397613525, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8406, 'grad_norm': 3.383873462677002, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4637, 'grad_norm': 1.1404584646224976, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1786, 'grad_norm': 1.0485060214996338, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0072, 'grad_norm': 0.9469280242919922, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8163, 'grad_norm': 0.9315477609634399, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7282, 'grad_norm': 0.7519756555557251, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6889, 'grad_norm': 0.6929640769958496, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5823, 'grad_norm': 0.6466392278671265, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4359, 'grad_norm': 0.9969204664230347, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3418, 'grad_norm': 0.7500238418579102, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2126, 'grad_norm': 0.7365084290504456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1456, 'grad_norm': 1.0828458070755005, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1163, 'grad_norm': 0.8652408719062805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0385, 'grad_norm': 1.0536725521087646, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9519, 'grad_norm': 0.9354153871536255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9417, 'grad_norm': 1.0707335472106934, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8195, 'grad_norm': 0.9179897904396057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7262, 'grad_norm': 0.8731324672698975, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6415, 'grad_norm': 0.9537513852119446, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6421, 'grad_norm': 0.798117995262146, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5473, 'grad_norm': 0.7531899213790894, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 0.8972017765045166, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3912, 'grad_norm': 0.7779728770256042, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3237, 'grad_norm': 0.9014979600906372, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4111, 'grad_norm': 1.1039460897445679, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3302, 'grad_norm': 0.957070529460907, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2359, 'grad_norm': 0.8378677368164062, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2309, 'grad_norm': 0.8541988730430603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1162, 'grad_norm': 0.8503371477127075, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1832, 'grad_norm': 1.0437371730804443, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0714, 'grad_norm': 0.9317355751991272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9724, 'grad_norm': 1.0102925300598145, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0475, 'grad_norm': 0.8593927621841431, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9847, 'grad_norm': 1.0534253120422363, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8935, 'grad_norm': 0.9747653007507324, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8134, 'grad_norm': 0.9832426309585571, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8463, 'grad_norm': 0.9903159737586975, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9006, 'grad_norm': 0.8788219690322876, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.873, 'grad_norm': 1.0442148447036743, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6724, 'grad_norm': 0.8916881680488586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6812, 'grad_norm': 0.8051306009292603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6383, 'grad_norm': 0.8629149794578552, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6937, 'grad_norm': 0.8521005511283875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.544, 'grad_norm': 1.1062430143356323, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6471, 'grad_norm': 1.1298426389694214, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6054, 'grad_norm': 0.8776903748512268, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5568, 'grad_norm': 0.9645063877105713, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.508, 'grad_norm': 0.8895986080169678, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4407, 'grad_norm': 0.9209122061729431, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5146, 'grad_norm': 0.8386083245277405, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4619, 'grad_norm': 0.8752357363700867, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4552, 'grad_norm': 0.8572624921798706, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3756, 'grad_norm': 1.0075575113296509, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3977, 'grad_norm': 0.8936896324157715, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2583, 'grad_norm': 0.9287660717964172, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3186, 'grad_norm': 0.9208709001541138, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4158, 'grad_norm': 0.7969366908073425, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.29, 'grad_norm': 0.8502371907234192, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3189, 'grad_norm': 0.8190291523933411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3151, 'grad_norm': 0.8564655184745789, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2886, 'grad_norm': 0.8515342473983765, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2173, 'grad_norm': 0.988594651222229, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1702, 'grad_norm': 1.038339376449585, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2426, 'grad_norm': 1.0462836027145386, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1066, 'grad_norm': 0.909101665019989, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1209, 'grad_norm': 0.8730531930923462, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0664, 'grad_norm': 0.9852443933486938, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0997, 'grad_norm': 0.7898673415184021, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1196, 'grad_norm': 0.9493786692619324, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0843, 'grad_norm': 0.8290815949440002, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0771, 'grad_norm': 1.2102653980255127, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0775, 'grad_norm': 0.9372451305389404, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0902, 'grad_norm': 0.8382816910743713, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9881, 'grad_norm': 0.8766090869903564, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9753, 'grad_norm': 0.8322007060050964, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.93, 'grad_norm': 0.8284767866134644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9013, 'grad_norm': 0.9744827151298523, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9883, 'grad_norm': 0.905106246471405, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9666, 'grad_norm': 0.8460886478424072, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9859, 'grad_norm': 0.9141154289245605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8999, 'grad_norm': 0.8842290639877319, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9915, 'grad_norm': 0.9232208132743835, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7489, 'grad_norm': 0.9172796010971069, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8429, 'grad_norm': 0.8144638538360596, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7998, 'grad_norm': 0.9605206251144409, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.862, 'grad_norm': 0.9390643239021301, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8131, 'grad_norm': 0.834696888923645, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8376, 'grad_norm': 0.8131949305534363, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8199, 'grad_norm': 0.9954076409339905, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.7698, 'grad_norm': 0.8776119947433472, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8024, 'grad_norm': 0.9735174775123596, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6924, 'grad_norm': 0.8690549731254578, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7678, 'grad_norm': 0.8721895217895508, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7896, 'grad_norm': 0.9969426989555359, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7024, 'grad_norm': 0.8049042224884033, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.704, 'grad_norm': 0.8616464734077454, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7652, 'grad_norm': 0.9246629476547241, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6866, 'grad_norm': 0.9274408221244812, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7041, 'grad_norm': 0.8567649126052856, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6942, 'grad_norm': 0.8865391612052917, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.6316, 'grad_norm': 0.9642078876495361, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.675, 'grad_norm': 0.9411467909812927, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6225, 'grad_norm': 0.9812774658203125, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5814, 'grad_norm': 0.8918157815933228, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5937, 'grad_norm': 1.0066958665847778, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5841, 'grad_norm': 0.8227797746658325, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5754, 'grad_norm': 0.9473503232002258, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5744, 'grad_norm': 0.9501177668571472, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5395, 'grad_norm': 0.871414303779602, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.5847, 'grad_norm': 0.9024458527565002, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6158, 'grad_norm': 0.8368567228317261, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.547, 'grad_norm': 0.9039610624313354, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5312, 'grad_norm': 0.791866660118103, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5414, 'grad_norm': 0.8737545609474182, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5235, 'grad_norm': 0.8648928999900818, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4892, 'grad_norm': 0.8738812804222107, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4718, 'grad_norm': 0.8620582818984985, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.467, 'grad_norm': 0.9816555380821228, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5523, 'grad_norm': 0.9490968585014343, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.4588, 'grad_norm': 0.9404143691062927, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4402, 'grad_norm': 0.9303212761878967, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5139, 'grad_norm': 0.9257434606552124, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4854, 'grad_norm': 0.96829754114151, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4346, 'grad_norm': 1.050953984260559, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4629, 'grad_norm': 1.118993878364563, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4783, 'grad_norm': 0.8649080991744995, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.8566702604293823, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.9625371098518372, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4649, 'grad_norm': 0.9149383306503296, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4291, 'grad_norm': 0.8810011744499207, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.378, 'grad_norm': 0.9099262952804565, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4707, 'grad_norm': 0.9336453676223755, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3377, 'grad_norm': 0.8025274276733398, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3405, 'grad_norm': 0.8041309714317322, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3813, 'grad_norm': 0.8100106716156006, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4301, 'grad_norm': 0.8509664535522461, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4611, 'grad_norm': 0.8618844747543335, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3274, 'grad_norm': 0.8955902457237244, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3731, 'grad_norm': 0.8158877491950989, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4656, 'grad_norm': 0.8626905083656311, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.358, 'grad_norm': 0.8353629112243652, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.3167, 'grad_norm': 0.8798050880432129, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2846, 'grad_norm': 0.8547061681747437, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2597, 'grad_norm': 0.8513766527175903, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.3015, 'grad_norm': 0.8333733677864075, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3691, 'grad_norm': 0.8628431558609009, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3291, 'grad_norm': 0.9778262972831726, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.1961, 'grad_norm': 0.8584219217300415, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.3205, 'grad_norm': 0.9154632091522217, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3164, 'grad_norm': 0.8557528257369995, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2579, 'grad_norm': 0.8773623704910278, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3276, 'grad_norm': 0.8342890739440918, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2425, 'grad_norm': 0.8907840847969055, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2454, 'grad_norm': 1.0231133699417114, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 1.007142186164856, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2036, 'grad_norm': 0.812870979309082, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.2003, 'grad_norm': 0.8798156976699829, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 0.9473292827606201, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2285, 'grad_norm': 0.8432570695877075, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1559, 'grad_norm': 0.830669641494751, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8659570217132568, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2275, 'grad_norm': 0.8856301307678223, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.2703, 'grad_norm': 0.8541915416717529, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.2755, 'grad_norm': 0.815159022808075, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1696, 'grad_norm': 0.8821364641189575, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1721, 'grad_norm': 0.8121861219406128, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1909, 'grad_norm': 0.8712433576583862, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2245, 'grad_norm': 0.8510169982910156, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2186, 'grad_norm': 0.9506018757820129, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1542, 'grad_norm': 0.932791531085968, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.1958, 'grad_norm': 0.8545206189155579, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 4.108, 'grad_norm': 0.9410622119903564, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 4.1713, 'grad_norm': 0.7748789191246033, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 876/15157 [29:59<8:10:38,  2.06s/it]2025-11-03 18:12:04,207 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.22 c\n",
      "{'train_runtime': 1801.2283, 'train_samples_per_second': 1077.078, 'train_steps_per_second': 8.415, 'train_loss': 5.239923280348392, 'epoch': 0.06}\n",
      "  6%|‚ñå         | 877/15157 [30:01<8:08:48,  2.05s/it]\n",
      "2025-11-03 18:12:04,389 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:12:04,393 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.25it/s]\n",
      "2025-11-03 18:12:28,374 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:12:29,454 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:12:29,454 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 2006 –≥–æ–¥—É. –í 2010 –≥–æ–¥—É, –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 36 –ª–µ—Ç, –≤ 2013 –≥–æ–¥—É –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º –∑–∞–≤–µ–¥—É—é—â–∏–º.\n",
      "\n",
      "–í 2015 –≥–æ–¥—É, –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –≤ ¬´–ü—è—Ç—å—É¬ª.\n",
      "\n",
      "–í 2014 –≥–æ–¥—É –≤ —Ç–µ–∞—Ç—Ä–µ, –ø–æ –¥–∞–Ω–Ω—ã–º –ø–µ—Ä–µ–ø–∏—Å–∏ 2011 –≥–æ–¥–∞, –Ω–∞ –º–µ—Å—Ç–µ –Ω–∞—á–∞–ª–∞ –≤\n",
      "2025-11-03 18:12:29,455 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:12:29,455 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.2283, 'train_samples_per_second': 1077.078, 'train_steps_per_second': 8.415, 'total_flos': 2.958662654975017e+17, 'train_loss': 5.239923280348392, 'epoch': 0.05786200867600244}, 'eval': {'eval_loss': 4.827373504638672, 'eval_runtime': 23.9817, 'eval_samples_per_second': 208.492, 'eval_steps_per_second': 26.062, 'epoch': 0.05786200867600244}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 2006 –≥–æ–¥—É. –í 2010 –≥–æ–¥—É, –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 36 –ª–µ—Ç, –≤ 2013 –≥–æ–¥—É –≤ —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º –∑–∞–≤–µ–¥—É—é—â–∏–º.\\n\\n–í 2015 –≥–æ–¥—É, –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –≤ ¬´–ü—è—Ç—å—É¬ª.\\n\\n–í 2014 –≥–æ–¥—É –≤ —Ç–µ–∞—Ç—Ä–µ, –ø–æ –¥–∞–Ω–Ω—ã–º –ø–µ—Ä–µ–ø–∏—Å–∏ 2011 –≥–æ–¥–∞, –Ω–∞ –º–µ—Å—Ç–µ –Ω–∞—á–∞–ª–∞ –≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_DeepSpeed_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zci3incb\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_174157-zci3incb/logs\u001b[0m\n",
      "[rank0]:[W1103 18:12:31.364719994 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_1'  # 35523 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29503 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922de6bd",
   "metadata": {},
   "source": [
    "### DeepSpeed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91229ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: World size: 2\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 18:12:33,701 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 2)\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 18:12:33,701 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:12:33,817 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 18:13:09,586 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:13:09,774 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:13:11,958 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:13:11,970 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 18:13:11,974 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_181312-nft9o7va\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/nft9o7va\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 18:13:13,961 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 18:13:13,961 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.7798, 'grad_norm': 4.454235553741455, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.812, 'grad_norm': 1.1511505842208862, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 9.1108, 'grad_norm': 0.6907618641853333, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6884, 'grad_norm': 0.4392479360103607, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4066, 'grad_norm': 0.3064589202404022, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2598, 'grad_norm': 0.46854639053344727, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0787, 'grad_norm': 0.2971721887588501, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.001, 'grad_norm': 0.30156153440475464, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9758, 'grad_norm': 0.28211864829063416, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9005, 'grad_norm': 0.27629169821739197, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8063, 'grad_norm': 0.2253260463476181, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7609, 'grad_norm': 0.3038266599178314, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6779, 'grad_norm': 0.25365614891052246, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6621, 'grad_norm': 0.34839579463005066, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6522, 'grad_norm': 0.27485111355781555, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6091, 'grad_norm': 0.3047346770763397, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5894, 'grad_norm': 0.35588470101356506, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5749, 'grad_norm': 0.3648209273815155, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4981, 'grad_norm': 0.34213685989379883, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4486, 'grad_norm': 0.3252623975276947, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3769, 'grad_norm': 0.36113014817237854, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3832, 'grad_norm': 0.3960557281970978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3152, 'grad_norm': 0.31657683849334717, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.337, 'grad_norm': 0.3433096706867218, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1899, 'grad_norm': 0.3341600298881531, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1403, 'grad_norm': 0.3360576331615448, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.2185, 'grad_norm': 0.3185850977897644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1562, 'grad_norm': 0.3112231492996216, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1007, 'grad_norm': 0.31848135590553284, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.083, 'grad_norm': 0.32603323459625244, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9926, 'grad_norm': 0.32045257091522217, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0254, 'grad_norm': 0.3346325159072876, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9342, 'grad_norm': 0.29385778307914734, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8594, 'grad_norm': 0.2963966727256775, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.932, 'grad_norm': 0.3823137879371643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8761, 'grad_norm': 0.32335802912712097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7925, 'grad_norm': 0.3637615740299225, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.724, 'grad_norm': 0.3655754327774048, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7519, 'grad_norm': 0.40341511368751526, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.779, 'grad_norm': 0.32469722628593445, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7674, 'grad_norm': 0.3360317647457123, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 0.3705793619155884, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5738, 'grad_norm': 0.30030012130737305, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5623, 'grad_norm': 0.3573712408542633, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5826, 'grad_norm': 0.36503905057907104, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4648, 'grad_norm': 0.4010322391986847, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.5251, 'grad_norm': 0.29925772547721863, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4958, 'grad_norm': 0.31168434023857117, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4494, 'grad_norm': 0.3172328472137451, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3963, 'grad_norm': 0.3231424391269684, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.348, 'grad_norm': 0.36385414004325867, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4111, 'grad_norm': 0.34250786900520325, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3578, 'grad_norm': 0.3524267375469208, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3286, 'grad_norm': 0.29518604278564453, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2773, 'grad_norm': 0.33538439869880676, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2908, 'grad_norm': 0.3221695125102997, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1683, 'grad_norm': 0.35099560022354126, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2085, 'grad_norm': 0.2988779842853546, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2991, 'grad_norm': 0.33221280574798584, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1761, 'grad_norm': 0.40171706676483154, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1873, 'grad_norm': 0.3142092227935791, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2123, 'grad_norm': 0.31641751527786255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1663, 'grad_norm': 0.32732561230659485, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1133, 'grad_norm': 0.36741089820861816, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0557, 'grad_norm': 0.4393484592437744, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.138, 'grad_norm': 0.3109776973724365, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9981, 'grad_norm': 0.35914888978004456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0191, 'grad_norm': 0.3538433015346527, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9597, 'grad_norm': 0.37219491600990295, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9799, 'grad_norm': 0.3403673470020294, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0109, 'grad_norm': 0.3522985279560089, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9644, 'grad_norm': 0.31357496976852417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9513, 'grad_norm': 0.37986108660697937, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9389, 'grad_norm': 0.34722381830215454, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9565, 'grad_norm': 0.3252619504928589, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.8815, 'grad_norm': 0.30623528361320496, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8752, 'grad_norm': 0.29321974515914917, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8339, 'grad_norm': 0.3290019929409027, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.771, 'grad_norm': 0.320248007774353, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.883, 'grad_norm': 0.3240400552749634, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8658, 'grad_norm': 0.3640105128288269, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8648, 'grad_norm': 0.3070066571235657, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7845, 'grad_norm': 0.3596435785293579, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8324, 'grad_norm': 0.30248793959617615, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6447, 'grad_norm': 0.3452083468437195, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7226, 'grad_norm': 0.3048035502433777, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6741, 'grad_norm': 0.3311581313610077, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7535, 'grad_norm': 0.33844393491744995, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6934, 'grad_norm': 0.2992381751537323, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7102, 'grad_norm': 0.3232366144657135, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6673, 'grad_norm': 0.34503433108329773, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 5.6363, 'grad_norm': 0.3414338529109955, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 5.6695, 'grad_norm': 0.3340323865413666, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 5.5686, 'grad_norm': 0.2953379154205322, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 5.6312, 'grad_norm': 0.33869630098342896, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 5.6431, 'grad_norm': 0.31590256094932556, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 5.5586, 'grad_norm': 0.34281596541404724, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 5.5648, 'grad_norm': 0.42304888367652893, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 5.6195, 'grad_norm': 0.3659343719482422, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 5.5448, 'grad_norm': 0.2783743441104889, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 5.5707, 'grad_norm': 0.3218107521533966, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 5.5673, 'grad_norm': 0.31465089321136475, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 5.5025, 'grad_norm': 0.32628804445266724, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 5.5503, 'grad_norm': 0.34743961691856384, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 5.4926, 'grad_norm': 0.3055354356765747, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 5.4479, 'grad_norm': 0.5234678387641907, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 5.4685, 'grad_norm': 0.32617372274398804, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 5.4509, 'grad_norm': 0.3556486964225769, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 5.4252, 'grad_norm': 0.3654335141181946, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 5.4582, 'grad_norm': 0.31860828399658203, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 5.404, 'grad_norm': 0.3205486536026001, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 5.4522, 'grad_norm': 0.39203187823295593, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 5.4721, 'grad_norm': 0.30983930826187134, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "  4%|‚ñé         | 568/15157 [29:58<12:48:54,  3.16s/it]2025-11-03 18:43:21,225 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.84 c\n",
      "{'train_runtime': 1801.851, 'train_samples_per_second': 1076.706, 'train_steps_per_second': 8.412, 'train_loss': 6.516830628701799, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 569/15157 [30:01<12:49:55,  3.17s/it]\n",
      "2025-11-03 18:43:21,374 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:43:21,391 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.40it/s]\n",
      "2025-11-03 18:43:45,263 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 18:43:46,312 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 18:43:46,313 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Ç–∞–∫.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –õ–æ–Ω–¥–æ–Ω–µ –≤ —Å–µ–ª–µ –≤ –≥–æ—Ä–æ–¥–µ –®–∞–Ω—å–Ω–æ-–í.¬†–í.¬†–ê.¬†–í.¬†–§.¬†–í.¬†–®.¬†–í.¬†–õ.¬†–°.¬†–õ.: –ù–∞—É–∫–∞.¬†‚Äî –ú.: –ù–∞—É–∫–∞, 2007.\n",
      "\n",
      "–í 1961¬†–≥\n",
      "2025-11-03 18:43:46,313 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 18:43:46,313 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.851, 'train_samples_per_second': 1076.706, 'train_steps_per_second': 8.412, 'total_flos': 1.919588427230085e+17, 'train_loss': 6.516830628701799, 'epoch': 0.037541029574282084}, 'eval': {'eval_loss': 6.077337265014648, 'eval_runtime': 23.8686, 'eval_samples_per_second': 209.48, 'eval_steps_per_second': 26.185, 'epoch': 0.037541029574282084}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Ç–∞–∫.\\n\\n–£—á–∏–ª—Å—è –≤ –õ–æ–Ω–¥–æ–Ω–µ –≤ —Å–µ–ª–µ –≤ –≥–æ—Ä–æ–¥–µ –®–∞–Ω—å–Ω–æ-–í.\\xa0–í.\\xa0–ê.\\xa0–í.\\xa0–§.\\xa0–í.\\xa0–®.\\xa0–í.\\xa0–õ.\\xa0–°.\\xa0–õ.: –ù–∞—É–∫–∞.\\xa0‚Äî –ú.: –ù–∞—É–∫–∞, 2007.\\n\\n–í 1961\\xa0–≥'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_DeepSpeed_2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/nft9o7va\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_181312-nft9o7va/logs\u001b[0m\n",
      "[rank0]:[W1103 18:43:49.059038118 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_2'  # 33135 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29504 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 2 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b4dda",
   "metadata": {},
   "source": [
    "### DeepSpeed_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: World size: 2\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 18:15:35,846 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 3)\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   overlap_comm: True\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 18:15:35,846 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: ============================================================\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 18:15:35,853 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 18:16:11,755 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:16:12,151 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 18:16:14,255 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:16:14,266 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True, 'stage3_prefetch_bucket_size': 500000000, 'stage3_param_persistence_threshold': 100000, 'stage3_max_live_parameters': 1000000000, 'stage3_max_reuse_distance': 1000000000, 'stage3_gather_16bit_weights_on_model_save': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 18:16:14,270 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_181615-0hzzdpbr\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_DeepSpeed_3\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/0hzzdpbr\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 18:16:16,193 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 18:16:16,194 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "Parameter Offload - Persistent parameters statistics: param_count = 49, numel = 54272\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6923, 'grad_norm': 3.6686655097746415, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5164, 'grad_norm': 5.038104472096909, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8408, 'grad_norm': 3.378780287138547, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4639, 'grad_norm': 1.138786406505714, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1786, 'grad_norm': 1.0461821231771407, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.007, 'grad_norm': 0.9544622024922441, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8161, 'grad_norm': 0.9355534419367514, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.728, 'grad_norm': 0.7604584328022364, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6889, 'grad_norm': 0.7006783320900247, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5826, 'grad_norm': 0.6495457724862668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4363, 'grad_norm': 0.9687593987575245, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3419, 'grad_norm': 0.7123596091597003, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2132, 'grad_norm': 0.7397988119023997, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1458, 'grad_norm': 1.088446608287305, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.116, 'grad_norm': 0.8647698004273836, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0382, 'grad_norm': 1.0740004714847242, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.952, 'grad_norm': 0.9312154623967168, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9419, 'grad_norm': 1.1001762148230307, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8198, 'grad_norm': 0.9295160526464471, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7263, 'grad_norm': 0.8783628941699118, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6418, 'grad_norm': 0.9664533908613936, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6425, 'grad_norm': 0.8065055679690432, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5477, 'grad_norm': 0.7515727800765956, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5766, 'grad_norm': 0.9016929483563586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3917, 'grad_norm': 0.7796490862862352, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3243, 'grad_norm': 0.9225948472436843, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4116, 'grad_norm': 1.1022671598518248, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3312, 'grad_norm': 0.9677615658309514, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2364, 'grad_norm': 0.8328573449091918, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2315, 'grad_norm': 0.8477178330803297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1168, 'grad_norm': 0.8728834707958798, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1838, 'grad_norm': 1.0399749854554272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0719, 'grad_norm': 0.9345049994762274, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9732, 'grad_norm': 1.024096602301595, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0478, 'grad_norm': 0.8582576046140278, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9849, 'grad_norm': 1.0546145345417732, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8939, 'grad_norm': 0.9674367821552126, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8142, 'grad_norm': 0.9956267563839062, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8465, 'grad_norm': 0.9813303691153088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9012, 'grad_norm': 0.8758007376970978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8739, 'grad_norm': 1.047776960308627, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6729, 'grad_norm': 0.888957986901388, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6816, 'grad_norm': 0.8077809607986471, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6385, 'grad_norm': 0.8591134463352412, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6942, 'grad_norm': 0.8552180473896958, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5446, 'grad_norm': 1.103853596144282, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6477, 'grad_norm': 1.1310697328127202, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6061, 'grad_norm': 0.8815733764256164, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5574, 'grad_norm': 0.9620597983115597, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5087, 'grad_norm': 0.8803177870977738, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4417, 'grad_norm': 0.9158506692991487, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5153, 'grad_norm': 0.8421436456404593, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.463, 'grad_norm': 0.8840385190928453, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4559, 'grad_norm': 0.8569972150135463, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3761, 'grad_norm': 0.9986714373766857, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3982, 'grad_norm': 0.8958509139875254, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2589, 'grad_norm': 0.9334281461796646, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3191, 'grad_norm': 0.9009156778348731, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4164, 'grad_norm': 0.8025089916963281, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2904, 'grad_norm': 0.8604925855242838, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3195, 'grad_norm': 0.8239474578921805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3157, 'grad_norm': 0.8516390445330702, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2892, 'grad_norm': 0.8554077219739088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.218, 'grad_norm': 0.988212880196313, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1706, 'grad_norm': 1.0378519718728358, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2431, 'grad_norm': 1.0498656526032168, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.107, 'grad_norm': 0.9113654133371218, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1213, 'grad_norm': 0.8693178016038134, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0669, 'grad_norm': 0.9882475266367782, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.791020866135482, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1198, 'grad_norm': 0.9475528529429813, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0848, 'grad_norm': 0.8394621446566618, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0778, 'grad_norm': 1.2157962400382296, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0782, 'grad_norm': 0.9460464905704643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0908, 'grad_norm': 0.8458119416301931, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9888, 'grad_norm': 0.8814883198404059, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.976, 'grad_norm': 0.8389241872167115, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9305, 'grad_norm': 0.8234012540963013, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.902, 'grad_norm': 0.9771300512741184, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9888, 'grad_norm': 0.9083272175955754, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9672, 'grad_norm': 0.8471878221796395, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9864, 'grad_norm': 0.9118531826054335, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9003, 'grad_norm': 0.8833711115644334, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9919, 'grad_norm': 0.9209230367573716, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7493, 'grad_norm': 0.9165489825921537, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8432, 'grad_norm': 0.8108080042634097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8001, 'grad_norm': 0.953483909747508, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8624, 'grad_norm': 0.9328252594698135, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8135, 'grad_norm': 0.8324237243810227, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8379, 'grad_norm': 0.8161158230424689, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8202, 'grad_norm': 0.985186735442075, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.77, 'grad_norm': 0.8712857288047461, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8026, 'grad_norm': 0.9713614836531296, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6929, 'grad_norm': 0.8701215727170363, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7682, 'grad_norm': 0.8754376020560871, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7899, 'grad_norm': 0.9908918244459293, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7026, 'grad_norm': 0.8064597339009851, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7046, 'grad_norm': 0.8617681029261028, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7655, 'grad_norm': 0.9222676315711975, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.687, 'grad_norm': 0.92291603691172, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8547991685504982, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6945, 'grad_norm': 0.8927314469155044, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.6321, 'grad_norm': 0.9685155719602763, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6756, 'grad_norm': 0.9371410125214955, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6229, 'grad_norm': 0.974017673529815, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5818, 'grad_norm': 0.8895976507428887, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5938, 'grad_norm': 1.0040532489741272, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5842, 'grad_norm': 0.819692972334174, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5757, 'grad_norm': 0.94733784263099, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5747, 'grad_norm': 0.9469483594603054, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5398, 'grad_norm': 0.8729129401284417, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.585, 'grad_norm': 0.9024696003732986, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6161, 'grad_norm': 0.8335436558588407, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5473, 'grad_norm': 0.9068393629982003, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5314, 'grad_norm': 0.7916696649508305, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5415, 'grad_norm': 0.872044788459042, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5237, 'grad_norm': 0.8648677430085027, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4893, 'grad_norm': 0.871713866103787, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4721, 'grad_norm': 0.8625541217363673, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.4671, 'grad_norm': 0.9813430683324117, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5525, 'grad_norm': 0.9507654019307644, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.9444311432434342, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4404, 'grad_norm': 0.9265397991502312, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5143, 'grad_norm': 0.9223589790102357, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4857, 'grad_norm': 0.9743350066698281, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.435, 'grad_norm': 1.0534058105454671, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4633, 'grad_norm': 1.1147617491416895, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4788, 'grad_norm': 0.8708640169199661, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4592, 'grad_norm': 0.8551973653014545, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4928, 'grad_norm': 0.9669475442821185, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4651, 'grad_norm': 0.9095261171735732, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4295, 'grad_norm': 0.8809266148557976, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.3781, 'grad_norm': 0.9085483165964268, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.471, 'grad_norm': 0.9326344480202532, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.338, 'grad_norm': 0.8033044348506737, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3407, 'grad_norm': 0.8048520569741922, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3813, 'grad_norm': 0.8083669630896226, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4304, 'grad_norm': 0.8497074244953333, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 690/15157 [29:58<10:25:52,  2.60s/it]2025-11-03 18:46:22,402 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.23 c\n",
      "{'train_runtime': 1801.2486, 'train_samples_per_second': 1077.066, 'train_steps_per_second': 8.415, 'train_loss': 5.504332236029132, 'epoch': 0.05}\n",
      "  5%|‚ñç         | 691/15157 [30:01<10:28:28,  2.61s/it]\n",
      "2025-11-03 18:46:22,535 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 18:46:22,535 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:37<00:00, 16.63it/s]\n",
      "2025-11-03 18:47:00,508 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'DeepSpeed_3'  # 36829 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29505 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 3 \\\n",
    "    --reduce-bucket-size 500000000 \\\n",
    "    --allgather-bucket-size 500000000 \\\n",
    "    --overlap-comm \\\n",
    "    --stage3-prefetch-bucket-size 500000000 \\\n",
    "    --stage3-param-persistence-threshold 100000 \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e7ac5",
   "metadata": {},
   "source": [
    "### FSDP_full_shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dd36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,464 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:17:16,465 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:17:16,465 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:17:16,569 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:17:51,879 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:17:52,134 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:17:52,134 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:17:52,136 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f2236deaf00>]}\n",
      "/app/lib/training.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 19:17:53,846 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:17:54,380 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:17:54,468 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:17:54,468 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:17:54,470 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f17094fe1b0>]}\n",
      "/app/lib/training.py:104: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_191757-gnt85jw8\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_FSDP_full_shard\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/gnt85jw8\n",
      "2025-11-03 19:17:58,167 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:17:58,168 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:17:58,168 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:17:58,168 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6921, 'grad_norm': 3.6685993671417236, 'learning_rate': 4.998680477667085e-05, 'epoch': 0.0}\n",
      "{'loss': 9.516, 'grad_norm': 5.000145435333252, 'learning_rate': 4.99703107475094e-05, 'epoch': 0.0}\n",
      "{'loss': 8.8406, 'grad_norm': 3.3742730617523193, 'learning_rate': 4.995381671834796e-05, 'epoch': 0.0}\n",
      "{'loss': 8.4638, 'grad_norm': 1.1423261165618896, 'learning_rate': 4.9937322689186515e-05, 'epoch': 0.0}\n",
      "{'loss': 8.1791, 'grad_norm': 1.057668685913086, 'learning_rate': 4.992082866002507e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0078, 'grad_norm': 0.9447437524795532, 'learning_rate': 4.990433463086363e-05, 'epoch': 0.0}\n",
      "{'loss': 7.8169, 'grad_norm': 0.9370400309562683, 'learning_rate': 4.988784060170218e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7288, 'grad_norm': 0.7628574371337891, 'learning_rate': 4.9871346572540744e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6897, 'grad_norm': 0.7042715549468994, 'learning_rate': 4.98548525433793e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5835, 'grad_norm': 0.6564656496047974, 'learning_rate': 4.983835851421785e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4374, 'grad_norm': 0.9480372071266174, 'learning_rate': 4.982186448505641e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3434, 'grad_norm': 0.7316735982894897, 'learning_rate': 4.9805370455894966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.215, 'grad_norm': 0.7324602007865906, 'learning_rate': 4.9788876426733526e-05, 'epoch': 0.0}\n",
      "{'loss': 7.148, 'grad_norm': 1.0855151414871216, 'learning_rate': 4.977238239757208e-05, 'epoch': 0.0}\n",
      "{'loss': 7.1185, 'grad_norm': 0.8853920102119446, 'learning_rate': 4.9755888368410634e-05, 'epoch': 0.0}\n",
      "{'loss': 7.0408, 'grad_norm': 1.0450620651245117, 'learning_rate': 4.9739394339249195e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9544, 'grad_norm': 0.9343412518501282, 'learning_rate': 4.972290031008775e-05, 'epoch': 0.01}\n",
      "{'loss': 6.9447, 'grad_norm': 1.0575039386749268, 'learning_rate': 4.970640628092631e-05, 'epoch': 0.01}\n",
      "{'loss': 6.8225, 'grad_norm': 0.9116678237915039, 'learning_rate': 4.968991225176486e-05, 'epoch': 0.01}\n",
      "{'loss': 6.7294, 'grad_norm': 0.8690633773803711, 'learning_rate': 4.967341822260342e-05, 'epoch': 0.01}\n",
      "{'loss': 6.6447, 'grad_norm': 0.9283594489097595, 'learning_rate': 4.965692419344198e-05, 'epoch': 0.01}\n",
      "{'loss': 6.6454, 'grad_norm': 0.7795161008834839, 'learning_rate': 4.964043016428053e-05, 'epoch': 0.01}\n",
      "{'loss': 6.5507, 'grad_norm': 0.7706556916236877, 'learning_rate': 4.9623936135119085e-05, 'epoch': 0.01}\n",
      "{'loss': 6.5793, 'grad_norm': 0.8847196698188782, 'learning_rate': 4.9607442105957645e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3949, 'grad_norm': 0.7742242217063904, 'learning_rate': 4.95909480767962e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3275, 'grad_norm': 0.8789457678794861, 'learning_rate': 4.957445404763476e-05, 'epoch': 0.01}\n",
      "{'loss': 6.415, 'grad_norm': 1.0843669176101685, 'learning_rate': 4.9557960018473314e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3351, 'grad_norm': 0.9792466163635254, 'learning_rate': 4.954146598931187e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2407, 'grad_norm': 0.868530809879303, 'learning_rate': 4.952497196015043e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2357, 'grad_norm': 0.8396028280258179, 'learning_rate': 4.950847793098898e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1203, 'grad_norm': 0.8297266364097595, 'learning_rate': 4.949198390182754e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1877, 'grad_norm': 1.020311713218689, 'learning_rate': 4.9475489872666096e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0762, 'grad_norm': 0.9547456502914429, 'learning_rate': 4.945899584350465e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9777, 'grad_norm': 1.015638828277588, 'learning_rate': 4.944250181434321e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0532, 'grad_norm': 0.8578469753265381, 'learning_rate': 4.9426007785181764e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9901, 'grad_norm': 1.0535486936569214, 'learning_rate': 4.940951375602032e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8993, 'grad_norm': 0.9839456677436829, 'learning_rate': 4.939301972685888e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8188, 'grad_norm': 0.9774653315544128, 'learning_rate': 4.937652569769743e-05, 'epoch': 0.01}\n",
      "{'loss': 5.852, 'grad_norm': 0.9903869032859802, 'learning_rate': 4.936003166853599e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9061, 'grad_norm': 0.8830069899559021, 'learning_rate': 4.934353763937455e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8785, 'grad_norm': 1.028009295463562, 'learning_rate': 4.93270436102131e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6784, 'grad_norm': 0.927141547203064, 'learning_rate': 4.931054958105166e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6874, 'grad_norm': 0.8230565190315247, 'learning_rate': 4.9294055551890215e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6446, 'grad_norm': 0.8691125512123108, 'learning_rate': 4.9277561522728776e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6999, 'grad_norm': 0.8522950410842896, 'learning_rate': 4.926106749356733e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5506, 'grad_norm': 1.1313395500183105, 'learning_rate': 4.9244573464405884e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6533, 'grad_norm': 1.1135324239730835, 'learning_rate': 4.9228079435244444e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6115, 'grad_norm': 0.8765648007392883, 'learning_rate': 4.9211585406083e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5629, 'grad_norm': 0.9576177597045898, 'learning_rate': 4.919509137692156e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5143, 'grad_norm': 0.8940756916999817, 'learning_rate': 4.917859734776011e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4477, 'grad_norm': 0.9207929372787476, 'learning_rate': 4.9162103318598666e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5214, 'grad_norm': 0.8377256393432617, 'learning_rate': 4.914560928943723e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4691, 'grad_norm': 0.879462480545044, 'learning_rate': 4.912911526027578e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4624, 'grad_norm': 0.8720620274543762, 'learning_rate': 4.9112621231114334e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3831, 'grad_norm': 1.0094987154006958, 'learning_rate': 4.9096127201952895e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4051, 'grad_norm': 0.9029775261878967, 'learning_rate': 4.907963317279145e-05, 'epoch': 0.02}\n",
      "{'loss': 5.266, 'grad_norm': 0.9360216856002808, 'learning_rate': 4.906313914363001e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3264, 'grad_norm': 0.9177511930465698, 'learning_rate': 4.904664511446856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.4237, 'grad_norm': 0.8080633282661438, 'learning_rate': 4.903015108530712e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2978, 'grad_norm': 0.8596243858337402, 'learning_rate': 4.901365705614568e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3269, 'grad_norm': 0.8231866359710693, 'learning_rate': 4.899716302698423e-05, 'epoch': 0.02}\n",
      "{'loss': 5.3235, 'grad_norm': 0.8587458729743958, 'learning_rate': 4.898066899782279e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2969, 'grad_norm': 0.8600620031356812, 'learning_rate': 4.8964174968661346e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2262, 'grad_norm': 0.9906424283981323, 'learning_rate': 4.89476809394999e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1788, 'grad_norm': 1.0400859117507935, 'learning_rate': 4.893118691033846e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2516, 'grad_norm': 1.06809401512146, 'learning_rate': 4.8914692881177014e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1154, 'grad_norm': 0.9251386523246765, 'learning_rate': 4.8898198852015575e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1295, 'grad_norm': 0.864880383014679, 'learning_rate': 4.888170482285413e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0754, 'grad_norm': 0.9806000590324402, 'learning_rate': 4.886521079369268e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1088, 'grad_norm': 0.8003785014152527, 'learning_rate': 4.884871676453124e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1286, 'grad_norm': 0.941691517829895, 'learning_rate': 4.8832222735369797e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0937, 'grad_norm': 0.8454050421714783, 'learning_rate': 4.881572870620835e-05, 'epoch': 0.02}\n",
      "{'loss': 5.087, 'grad_norm': 1.2191836833953857, 'learning_rate': 4.879923467704691e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0873, 'grad_norm': 0.9526812434196472, 'learning_rate': 4.8782740647885465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.8473213911056519, 'learning_rate': 4.8766246618724025e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9983, 'grad_norm': 0.8750153183937073, 'learning_rate': 4.874975258956258e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9857, 'grad_norm': 0.8498088717460632, 'learning_rate': 4.873325856040113e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9406, 'grad_norm': 0.8415212631225586, 'learning_rate': 4.8716764531239694e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9118, 'grad_norm': 0.9755232334136963, 'learning_rate': 4.870027050207825e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9991, 'grad_norm': 0.9088501334190369, 'learning_rate': 4.868377647291681e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9774, 'grad_norm': 0.8449509143829346, 'learning_rate': 4.866728244375536e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9967, 'grad_norm': 0.8987208008766174, 'learning_rate': 4.8650788414593916e-05, 'epoch': 0.03}\n",
      "{'loss': 4.9108, 'grad_norm': 0.8797411322593689, 'learning_rate': 4.8634294385432476e-05, 'epoch': 0.03}\n",
      "{'loss': 5.0022, 'grad_norm': 0.91493159532547, 'learning_rate': 4.861780035627103e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7599, 'grad_norm': 0.9096333980560303, 'learning_rate': 4.860130632710959e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8538, 'grad_norm': 0.8110061883926392, 'learning_rate': 4.8584812297948144e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8107, 'grad_norm': 0.9565772414207458, 'learning_rate': 4.85683182687867e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8736, 'grad_norm': 0.93027263879776, 'learning_rate': 4.855182423962526e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8245, 'grad_norm': 0.8339365124702454, 'learning_rate': 4.853533021046381e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8491, 'grad_norm': 0.8148435950279236, 'learning_rate': 4.8518836181302366e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8312, 'grad_norm': 0.9840726852416992, 'learning_rate': 4.850234215214093e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7811, 'grad_norm': 0.8608699440956116, 'learning_rate': 4.848584812297948e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8139, 'grad_norm': 0.9694945812225342, 'learning_rate': 4.846935409381804e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8743197917938232, 'learning_rate': 4.8452860064656595e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7797, 'grad_norm': 0.8785078525543213, 'learning_rate': 4.843636603549515e-05, 'epoch': 0.03}\n",
      "{'loss': 4.8017, 'grad_norm': 1.0118496417999268, 'learning_rate': 4.841987200633371e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7144, 'grad_norm': 0.811053454875946, 'learning_rate': 4.8403377977172263e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7164, 'grad_norm': 0.8681556582450867, 'learning_rate': 4.8386883948010824e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7772, 'grad_norm': 0.9140840172767639, 'learning_rate': 4.837038991884938e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6992, 'grad_norm': 0.922348141670227, 'learning_rate': 4.835389588968793e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7163, 'grad_norm': 0.847205400466919, 'learning_rate': 4.833740186052649e-05, 'epoch': 0.03}\n",
      "{'loss': 4.7066, 'grad_norm': 0.8906052708625793, 'learning_rate': 4.8320907831365046e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6443, 'grad_norm': 0.9738845825195312, 'learning_rate': 4.83044138022036e-05, 'epoch': 0.03}\n",
      "{'loss': 4.688, 'grad_norm': 0.9475910067558289, 'learning_rate': 4.828791977304216e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6353, 'grad_norm': 0.98466557264328, 'learning_rate': 4.8271425743880714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5942, 'grad_norm': 0.8803571462631226, 'learning_rate': 4.8254931714719275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6065, 'grad_norm': 0.9985359907150269, 'learning_rate': 4.823843768555783e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5969, 'grad_norm': 0.8307783603668213, 'learning_rate': 4.822194365639638e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5884, 'grad_norm': 0.959983766078949, 'learning_rate': 4.820544962723494e-05, 'epoch': 0.04}\n",
      "{'loss': 4.588, 'grad_norm': 0.9481823444366455, 'learning_rate': 4.81889555980735e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5532, 'grad_norm': 0.877038300037384, 'learning_rate': 4.817246156891206e-05, 'epoch': 0.04}\n",
      "{'loss': 4.598, 'grad_norm': 0.9016987085342407, 'learning_rate': 4.815596753975061e-05, 'epoch': 0.04}\n",
      "{'loss': 4.629, 'grad_norm': 0.8373340368270874, 'learning_rate': 4.8139473510589165e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5602, 'grad_norm': 0.9142613410949707, 'learning_rate': 4.8122979481427726e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5448, 'grad_norm': 0.7920125126838684, 'learning_rate': 4.810648545226628e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5553, 'grad_norm': 0.8880935907363892, 'learning_rate': 4.808999142310484e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5371, 'grad_norm': 0.8799552917480469, 'learning_rate': 4.8073497393943394e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5029, 'grad_norm': 0.8831263780593872, 'learning_rate': 4.805700336478195e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4856, 'grad_norm': 0.8571293950080872, 'learning_rate': 4.804050933562051e-05, 'epoch': 0.04}\n",
      "{'loss': 4.481, 'grad_norm': 0.9903404712677002, 'learning_rate': 4.802401530645906e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5663, 'grad_norm': 0.9656529426574707, 'learning_rate': 4.8007521277297616e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4727, 'grad_norm': 0.9243872761726379, 'learning_rate': 4.7991027248136176e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4545, 'grad_norm': 0.930270791053772, 'learning_rate': 4.797453321897473e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5288, 'grad_norm': 0.9271360635757446, 'learning_rate': 4.795803918981329e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5002, 'grad_norm': 0.9885192513465881, 'learning_rate': 4.7941545160651845e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4493, 'grad_norm': 1.0475068092346191, 'learning_rate': 4.79250511314904e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4769, 'grad_norm': 1.1011706590652466, 'learning_rate': 4.790855710232896e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.8717222809791565, 'learning_rate': 4.789206307316751e-05, 'epoch': 0.04}\n",
      "{'loss': 4.4734, 'grad_norm': 0.8510183095932007, 'learning_rate': 4.7875569044006074e-05, 'epoch': 0.04}\n",
      "{'loss': 4.5071, 'grad_norm': 0.9741666913032532, 'learning_rate': 4.785907501484463e-05, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 653/15157 [29:59<11:05:30,  2.75s/it]2025-11-03 19:48:02,003 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1802.23 c\n",
      "{'train_runtime': 1802.2373, 'train_samples_per_second': 1076.475, 'train_steps_per_second': 8.41, 'train_loss': 5.574778247681597, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 654/15157 [30:02<11:06:06,  2.76s/it]\n",
      "2025-11-03 19:48:02,157 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 19:48:02,186 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:45<00:00,  5.94it/s]\n",
      "2025-11-03 19:49:47,745 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard'  # 30117 MiB\n",
    "# FULL_SHARD | SHARD_GRAD_OP | NO_SHARD | HYBRID_SHARD | HYBRID_SHARD_ZERO2\n",
    "# \"full_shard\", \"shard_grad_op\", \"no_shard\", \"hybrid_shard\"\n",
    "    # --fsdp-cpu-offload \\ –≤—ã–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ CPU (–£–º–µ–Ω—å—à–∞–µ—Ç –ø–∞–º—è—Ç—å GPU, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ)\n",
    "    # --fsdp-activation-checkpointing \\  –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç (~30% –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö) (–ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34dea",
   "metadata": {},
   "source": [
    "### FSDP_full_shard_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bd3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:37:23,097 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:37:23,190 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:37:23,190 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:37:23,191 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:37:59,475 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:37:59,734 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:37:59,734 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:37:59,735 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fc728012390>, <lib.callbacks.InspectCallback object at 0x7fc728012480>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 19:38:01,332 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:38:02,160 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:38:02,252 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:38:02,252 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:38:02,253 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f7f7c4b5a60>, <lib.callbacks.InspectCallback object at 0x7f7f7bcd2d20>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_193804-pgrtmpg4\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga2_FSDP_full_shard_v2\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/pgrtmpg4\n",
      "2025-11-03 19:38:05,776 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:38:05,776 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 2, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:38:05,777 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:38:05,777 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:38:07,332 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:38:07,341 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "{'loss': 10.6444, 'grad_norm': 6.5313920974731445, 'learning_rate': 4.999340238833543e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6673, 'grad_norm': 3.2273755073547363, 'learning_rate': 4.9985155373754705e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9237, 'grad_norm': 3.645207405090332, 'learning_rate': 4.997690835917398e-05, 'epoch': 0.0}\n",
      "{'loss': 8.5601, 'grad_norm': 1.3648924827575684, 'learning_rate': 4.996866134459326e-05, 'epoch': 0.0}\n",
      "{'loss': 8.2867, 'grad_norm': 1.3722723722457886, 'learning_rate': 4.996041433001254e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0122, 'grad_norm': 1.1596969366073608, 'learning_rate': 4.995216731543181e-05, 'epoch': 0.0}\n",
      "{'loss': 7.9121, 'grad_norm': 0.9208484292030334, 'learning_rate': 4.9943920300851096e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7859, 'grad_norm': 0.8623735904693604, 'learning_rate': 4.993567328627037e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7536, 'grad_norm': 1.0223972797393799, 'learning_rate': 4.992742627168966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5907, 'grad_norm': 0.9671335220336914, 'learning_rate': 4.991917925710893e-05, 'epoch': 0.0}\n",
      "{'loss': 7.6455, 'grad_norm': 1.3890206813812256, 'learning_rate': 4.991093224252821e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5677, 'grad_norm': 0.8959805369377136, 'learning_rate': 4.990268522794749e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4675, 'grad_norm': 1.2158596515655518, 'learning_rate': 4.9894438213366764e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3385, 'grad_norm': 1.021163821220398, 'learning_rate': 4.988619119878604e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3313, 'grad_norm': 1.2728992700576782, 'learning_rate': 4.9877944184205325e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2466, 'grad_norm': 1.184386968612671, 'learning_rate': 4.9869697169624595e-05, 'epoch': 0.0}\n",
      "{'loss': 7.226, 'grad_norm': 1.343267560005188, 'learning_rate': 4.986145015504388e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2143, 'grad_norm': 1.0885170698165894, 'learning_rate': 4.9853203140463156e-05, 'epoch': 0.0}\n",
      "{'loss': 7.186, 'grad_norm': 1.4330761432647705, 'learning_rate': 4.984495612588243e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9693, 'grad_norm': 1.0004644393920898, 'learning_rate': 4.983670911130171e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8406, 'grad_norm': 0.9873746633529663, 'learning_rate': 4.982846209672099e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9043, 'grad_norm': 0.9993258714675903, 'learning_rate': 4.982021508214027e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7424, 'grad_norm': 1.2514643669128418, 'learning_rate': 4.981196806755955e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8038, 'grad_norm': 1.064741611480713, 'learning_rate': 4.9803721052978824e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5962, 'grad_norm': 1.0138590335845947, 'learning_rate': 4.979547403839811e-05, 'epoch': 0.0}\n",
      "{'loss': 6.6334, 'grad_norm': 1.0394448041915894, 'learning_rate': 4.978722702381738e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5559, 'grad_norm': 1.0663433074951172, 'learning_rate': 4.977898000923666e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5233, 'grad_norm': 0.9639054536819458, 'learning_rate': 4.977073299465594e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5537, 'grad_norm': 0.9138987064361572, 'learning_rate': 4.9762485980075215e-05, 'epoch': 0.0}\n",
      "{'loss': 6.4483, 'grad_norm': 1.0875293016433716, 'learning_rate': 4.975423896549449e-05, 'epoch': 0.0}\n",
      "{'loss': 6.3772, 'grad_norm': 0.9539157748222351, 'learning_rate': 4.9745991950913776e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4714, 'grad_norm': 1.08989417552948, 'learning_rate': 4.9737744936333046e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3867, 'grad_norm': 0.9997318387031555, 'learning_rate': 4.972949792175233e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2735, 'grad_norm': 1.0559349060058594, 'learning_rate': 4.9721250907171607e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4121, 'grad_norm': 1.0875474214553833, 'learning_rate': 4.971300389259089e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2265, 'grad_norm': 1.2501388788223267, 'learning_rate': 4.970475687801016e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2732, 'grad_norm': 1.1039116382598877, 'learning_rate': 4.9696509863429444e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1098, 'grad_norm': 1.0326392650604248, 'learning_rate': 4.968826284884872e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0915, 'grad_norm': 1.060094952583313, 'learning_rate': 4.9680015834268e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1395, 'grad_norm': 1.1035743951797485, 'learning_rate': 4.9671768819687275e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0362, 'grad_norm': 1.1139506101608276, 'learning_rate': 4.966352180510656e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0501, 'grad_norm': 1.361629843711853, 'learning_rate': 4.965527479052583e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0638, 'grad_norm': 1.1630226373672485, 'learning_rate': 4.964702777594511e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0308, 'grad_norm': 1.0884668827056885, 'learning_rate': 4.963878076136439e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9067, 'grad_norm': 1.0826722383499146, 'learning_rate': 4.9630533746783666e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0173, 'grad_norm': 1.010643720626831, 'learning_rate': 4.962228673220294e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0348, 'grad_norm': 1.0483556985855103, 'learning_rate': 4.961403971762223e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9791, 'grad_norm': 1.2650882005691528, 'learning_rate': 4.9605792703041504e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9043, 'grad_norm': 1.1566051244735718, 'learning_rate': 4.959754568846078e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7435, 'grad_norm': 0.9544485211372375, 'learning_rate': 4.958929867388006e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8208, 'grad_norm': 1.2155009508132935, 'learning_rate': 4.958105165929934e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7167, 'grad_norm': 1.0473588705062866, 'learning_rate': 4.957280464471861e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9486, 'grad_norm': 1.1100026369094849, 'learning_rate': 4.9564557630137895e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7813, 'grad_norm': 1.1373603343963623, 'learning_rate': 4.955631061555717e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7994, 'grad_norm': 1.47096586227417, 'learning_rate': 4.954806360097645e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7695, 'grad_norm': 1.0950521230697632, 'learning_rate': 4.9539816586395726e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6905, 'grad_norm': 1.1174144744873047, 'learning_rate': 4.953156957181501e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6856, 'grad_norm': 1.051835298538208, 'learning_rate': 4.9523322557234286e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6396, 'grad_norm': 1.021802544593811, 'learning_rate': 4.951507554265356e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7389, 'grad_norm': 0.9070085287094116, 'learning_rate': 4.950682852807284e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6989, 'grad_norm': 1.0314044952392578, 'learning_rate': 4.9498581513492124e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4755, 'grad_norm': 0.9903817176818848, 'learning_rate': 4.9490334498911394e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6814, 'grad_norm': 1.0982643365859985, 'learning_rate': 4.948208748433068e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6682, 'grad_norm': 1.0954315662384033, 'learning_rate': 4.9473840469749954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5227, 'grad_norm': 1.4312175512313843, 'learning_rate': 4.946559345516923e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5659, 'grad_norm': 1.120503306388855, 'learning_rate': 4.945734644058851e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4352, 'grad_norm': 1.1130056381225586, 'learning_rate': 4.944909942600779e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4847, 'grad_norm': 1.056164026260376, 'learning_rate': 4.944085241142706e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5386, 'grad_norm': 1.1242332458496094, 'learning_rate': 4.9432605396846346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5591, 'grad_norm': 1.064380168914795, 'learning_rate': 4.942435838226562e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5723, 'grad_norm': 0.9790740013122559, 'learning_rate': 4.9416111367684906e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3953, 'grad_norm': 1.0622392892837524, 'learning_rate': 4.9407864353104176e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4267, 'grad_norm': 1.0991655588150024, 'learning_rate': 4.939961733852346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.382, 'grad_norm': 1.1173521280288696, 'learning_rate': 4.939137032394274e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3039, 'grad_norm': 1.047497034072876, 'learning_rate': 4.9383123309362014e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3389, 'grad_norm': 1.1278008222579956, 'learning_rate': 4.937487629478129e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3674, 'grad_norm': 0.9965013265609741, 'learning_rate': 4.9366629280200574e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3137, 'grad_norm': 1.1301568746566772, 'learning_rate': 4.9358382265619845e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5109, 'grad_norm': 0.9397605061531067, 'learning_rate': 4.935013525103913e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3577, 'grad_norm': 1.076525092124939, 'learning_rate': 4.9341888236458405e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4144, 'grad_norm': 1.0290720462799072, 'learning_rate': 4.933364122187768e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3834, 'grad_norm': 1.0764391422271729, 'learning_rate': 4.932539420729696e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2228, 'grad_norm': 1.1614902019500732, 'learning_rate': 4.931714719271624e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1843, 'grad_norm': 1.090602159500122, 'learning_rate': 4.930890017813552e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2014, 'grad_norm': 1.1497918367385864, 'learning_rate': 4.9300653163554796e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2625, 'grad_norm': 1.3094556331634521, 'learning_rate': 4.9292406148974073e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1546, 'grad_norm': 1.208540916442871, 'learning_rate': 4.928415913439336e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1954, 'grad_norm': 1.0286390781402588, 'learning_rate': 4.927591211981263e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2857, 'grad_norm': 1.0963047742843628, 'learning_rate': 4.926766510523191e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1918, 'grad_norm': 0.9852927923202515, 'learning_rate': 4.925941809065119e-05, 'epoch': 0.01}\n",
      "{'loss': 5.0279, 'grad_norm': 0.9711804389953613, 'learning_rate': 4.9251171076070465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1245, 'grad_norm': 1.0416266918182373, 'learning_rate': 4.924292406148974e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2345, 'grad_norm': 1.1587598323822021, 'learning_rate': 4.9234677046909025e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1673, 'grad_norm': 1.0791149139404297, 'learning_rate': 4.9226430032328295e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0438, 'grad_norm': 1.1286945343017578, 'learning_rate': 4.921818301774758e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2545, 'grad_norm': 1.1003646850585938, 'learning_rate': 4.9209936003166856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2145, 'grad_norm': 0.9384528994560242, 'learning_rate': 4.920168898858614e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9995, 'grad_norm': 0.9224302768707275, 'learning_rate': 4.919344197400541e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1453, 'grad_norm': 1.0209921598434448, 'learning_rate': 4.9185194959424694e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9835, 'grad_norm': 1.0382697582244873, 'learning_rate': 4.917694794484397e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0404, 'grad_norm': 1.004152536392212, 'learning_rate': 4.916870093026325e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9612, 'grad_norm': 1.1325302124023438, 'learning_rate': 4.9160453915682524e-05, 'epoch': 0.02}\n",
      "{'loss': 5.077, 'grad_norm': 0.9699444770812988, 'learning_rate': 4.915220690110181e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0896, 'grad_norm': 1.0638997554779053, 'learning_rate': 4.914395988652108e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9973, 'grad_norm': 1.1551052331924438, 'learning_rate': 4.913571287194036e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0661, 'grad_norm': 0.996765673160553, 'learning_rate': 4.912746585735964e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0973, 'grad_norm': 1.092122197151184, 'learning_rate': 4.911921884277892e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9443, 'grad_norm': 1.0544276237487793, 'learning_rate': 4.911097182819819e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8622, 'grad_norm': 1.2231124639511108, 'learning_rate': 4.9102724813617476e-05, 'epoch': 0.02}\n",
      "{'loss': 5.024, 'grad_norm': 1.041122317314148, 'learning_rate': 4.909447779903675e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9775, 'grad_norm': 1.089453101158142, 'learning_rate': 4.908623078445603e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9634, 'grad_norm': 1.0052860975265503, 'learning_rate': 4.907798376987531e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8447, 'grad_norm': 1.0101381540298462, 'learning_rate': 4.906973675529459e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8017, 'grad_norm': 1.0388035774230957, 'learning_rate': 4.906148974071386e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9357, 'grad_norm': 0.9926142692565918, 'learning_rate': 4.9053242726133144e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8476, 'grad_norm': 1.0412185192108154, 'learning_rate': 4.904499571155242e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0609, 'grad_norm': 1.0392355918884277, 'learning_rate': 4.90367486969717e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9396, 'grad_norm': 0.9341914653778076, 'learning_rate': 4.9028501682390975e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8953, 'grad_norm': 1.0489909648895264, 'learning_rate': 4.902025466781026e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8604, 'grad_norm': 1.180835485458374, 'learning_rate': 4.9012007653229536e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9404, 'grad_norm': 0.9948098063468933, 'learning_rate': 4.900376063864881e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8669, 'grad_norm': 1.0536701679229736, 'learning_rate': 4.899551362406809e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8946, 'grad_norm': 0.9966706037521362, 'learning_rate': 4.898726660948737e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8983, 'grad_norm': 1.0302015542984009, 'learning_rate': 4.897901959490664e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8993, 'grad_norm': 1.0842093229293823, 'learning_rate': 4.897077258032593e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8642, 'grad_norm': 1.1616989374160767, 'learning_rate': 4.8962525565745204e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8386, 'grad_norm': 0.9798850417137146, 'learning_rate': 4.895427855116448e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7661, 'grad_norm': 1.0196082592010498, 'learning_rate': 4.894603153658376e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7272, 'grad_norm': 1.090603232383728, 'learning_rate': 4.893778452200304e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7994, 'grad_norm': 1.0505355596542358, 'learning_rate': 4.892953750742231e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8151, 'grad_norm': 0.9707903861999512, 'learning_rate': 4.8921290492841595e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8455, 'grad_norm': 1.0490247011184692, 'learning_rate': 4.891304347826087e-05, 'epoch': 0.02}\n",
      "{'loss': 4.858, 'grad_norm': 0.9563772082328796, 'learning_rate': 4.8904796463680156e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5422, 'grad_norm': 1.062784194946289, 'learning_rate': 4.8896549449099426e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7993, 'grad_norm': 1.0139859914779663, 'learning_rate': 4.888830243451871e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6396, 'grad_norm': 1.046374797821045, 'learning_rate': 4.8880055419937986e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6918, 'grad_norm': 1.0056557655334473, 'learning_rate': 4.887180840535726e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6401, 'grad_norm': 1.157609224319458, 'learning_rate': 4.886356139077654e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6794, 'grad_norm': 1.0043175220489502, 'learning_rate': 4.8855314376195824e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7256, 'grad_norm': 1.1832711696624756, 'learning_rate': 4.8847067361615094e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7387, 'grad_norm': 0.9323859810829163, 'learning_rate': 4.883882034703438e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6963, 'grad_norm': 1.1491827964782715, 'learning_rate': 4.8830573332453655e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6466, 'grad_norm': 0.9812183976173401, 'learning_rate': 4.882232631787294e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7297, 'grad_norm': 0.992109477519989, 'learning_rate': 4.881407930329221e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6663, 'grad_norm': 1.2287836074829102, 'learning_rate': 4.880583228871149e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7133, 'grad_norm': 1.0841212272644043, 'learning_rate': 4.879758527413077e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6149, 'grad_norm': 0.9832476377487183, 'learning_rate': 4.8789338259550046e-05, 'epoch': 0.02}\n",
      "{'loss': 4.76, 'grad_norm': 0.9344678521156311, 'learning_rate': 4.878109124496932e-05, 'epoch': 0.02}\n",
      "{'loss': 4.704, 'grad_norm': 1.0734599828720093, 'learning_rate': 4.8772844230388607e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6975, 'grad_norm': 1.0407136678695679, 'learning_rate': 4.876459721580788e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6185, 'grad_norm': 1.1576107740402222, 'learning_rate': 4.875635020122716e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5762, 'grad_norm': 0.9878236651420593, 'learning_rate': 4.874810318664644e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5029, 'grad_norm': 1.0747416019439697, 'learning_rate': 4.8739856172065714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6683, 'grad_norm': 1.0084227323532104, 'learning_rate': 4.873160915748499e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6394, 'grad_norm': 1.0914009809494019, 'learning_rate': 4.8723362142904275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4652, 'grad_norm': 0.9519321322441101, 'learning_rate': 4.871511512832355e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5509, 'grad_norm': 1.2421261072158813, 'learning_rate': 4.870686811374283e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5087, 'grad_norm': 1.012853741645813, 'learning_rate': 4.8698621099162105e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5223, 'grad_norm': 0.998762845993042, 'learning_rate': 4.869037408458139e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6848, 'grad_norm': 1.1022077798843384, 'learning_rate': 4.868212707000066e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6026, 'grad_norm': 1.0309686660766602, 'learning_rate': 4.867388005541994e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5715, 'grad_norm': 1.0592811107635498, 'learning_rate': 4.866563304083922e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6757, 'grad_norm': 0.9432093501091003, 'learning_rate': 4.86573860262585e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5393, 'grad_norm': 1.0088406801223755, 'learning_rate': 4.8649139011677774e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5431, 'grad_norm': 0.9762718081474304, 'learning_rate': 4.864089199709706e-05, 'epoch': 0.03}\n",
      "{'loss': 4.505, 'grad_norm': 0.9821547269821167, 'learning_rate': 4.863264498251633e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5873, 'grad_norm': 1.0166200399398804, 'learning_rate': 4.862439796793561e-05, 'epoch': 0.03}\n",
      "{'loss': 4.674, 'grad_norm': 1.0620497465133667, 'learning_rate': 4.861615095335489e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3706, 'grad_norm': 1.0072818994522095, 'learning_rate': 4.860790393877417e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4114, 'grad_norm': 1.0058624744415283, 'learning_rate': 4.859965692419344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4734, 'grad_norm': 1.0765188932418823, 'learning_rate': 4.8591409909612726e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4653, 'grad_norm': 1.0471206903457642, 'learning_rate': 4.8583162895032e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4349, 'grad_norm': 1.1005496978759766, 'learning_rate': 4.857491588045128e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4313, 'grad_norm': 0.9662685990333557, 'learning_rate': 4.8566668865870556e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4832, 'grad_norm': 0.9477719068527222, 'learning_rate': 4.855842185128984e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5019, 'grad_norm': 1.0253028869628906, 'learning_rate': 4.855017483670911e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4575, 'grad_norm': 0.9929068684577942, 'learning_rate': 4.8541927822128394e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4436, 'grad_norm': 0.9480090141296387, 'learning_rate': 4.853368080754767e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4527, 'grad_norm': 0.9162320494651794, 'learning_rate': 4.852543379296695e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4905, 'grad_norm': 0.9413788914680481, 'learning_rate': 4.8517186778386225e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5195, 'grad_norm': 0.9664589762687683, 'learning_rate': 4.850893976380551e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4227, 'grad_norm': 0.9785865545272827, 'learning_rate': 4.8500692749224785e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4685, 'grad_norm': 1.0251414775848389, 'learning_rate': 4.849244573464406e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3794, 'grad_norm': 1.114363670349121, 'learning_rate': 4.848419872006334e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4262, 'grad_norm': 1.0935510396957397, 'learning_rate': 4.847595170548262e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4759, 'grad_norm': 0.9929882884025574, 'learning_rate': 4.846770469090189e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3377, 'grad_norm': 1.0329346656799316, 'learning_rate': 4.8459457676321176e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3363, 'grad_norm': 1.0098180770874023, 'learning_rate': 4.845121066174045e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4679, 'grad_norm': 1.0334932804107666, 'learning_rate': 4.844296364715973e-05, 'epoch': 0.03}\n",
      "{'loss': 4.374, 'grad_norm': 1.0937193632125854, 'learning_rate': 4.843471663257901e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4582, 'grad_norm': 1.0132368803024292, 'learning_rate': 4.842646961799829e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4351, 'grad_norm': 0.9898399114608765, 'learning_rate': 4.841822260341757e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2785, 'grad_norm': 0.9562243819236755, 'learning_rate': 4.8409975588836845e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4475, 'grad_norm': 0.9146280288696289, 'learning_rate': 4.840172857425612e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3932, 'grad_norm': 0.9829525351524353, 'learning_rate': 4.8393481559675405e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3242, 'grad_norm': 1.034329891204834, 'learning_rate': 4.8385234545094675e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3949, 'grad_norm': 0.9865344166755676, 'learning_rate': 4.837698753051396e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4569, 'grad_norm': 1.062667727470398, 'learning_rate': 4.8368740515933236e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2508, 'grad_norm': 1.0316967964172363, 'learning_rate': 4.836049350135251e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4239, 'grad_norm': 0.9955689907073975, 'learning_rate': 4.835224648677179e-05, 'epoch': 0.03}\n",
      "{'loss': 4.273, 'grad_norm': 1.014431357383728, 'learning_rate': 4.8343999472191073e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4695, 'grad_norm': 1.0195432901382446, 'learning_rate': 4.8335752457610344e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3675, 'grad_norm': 1.0507800579071045, 'learning_rate': 4.832750544302963e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3484, 'grad_norm': 1.0973351001739502, 'learning_rate': 4.8319258428448904e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2405, 'grad_norm': 1.0506153106689453, 'learning_rate': 4.831101141386819e-05, 'epoch': 0.03}\n",
      "{'loss': 4.3367, 'grad_norm': 1.0019389390945435, 'learning_rate': 4.830276439928746e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2619, 'grad_norm': 0.9787523150444031, 'learning_rate': 4.829451738470674e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4012, 'grad_norm': 1.0298384428024292, 'learning_rate': 4.828627037012602e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2744, 'grad_norm': 1.1903619766235352, 'learning_rate': 4.8278023355545295e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2926, 'grad_norm': 0.9904480576515198, 'learning_rate': 4.826977634096457e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2618, 'grad_norm': 0.9850509762763977, 'learning_rate': 4.8261529326383856e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2295, 'grad_norm': 0.9893596172332764, 'learning_rate': 4.8253282311803126e-05, 'epoch': 0.03}\n",
      "{'loss': 4.2349, 'grad_norm': 1.012485146522522, 'learning_rate': 4.824503529722241e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2829, 'grad_norm': 1.0510729551315308, 'learning_rate': 4.823678828264169e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2644, 'grad_norm': 1.0484381914138794, 'learning_rate': 4.8228541268060964e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2433, 'grad_norm': 0.9664254188537598, 'learning_rate': 4.822029425348024e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2256, 'grad_norm': 0.985930860042572, 'learning_rate': 4.8212047238899524e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2789, 'grad_norm': 1.0419929027557373, 'learning_rate': 4.82038002243188e-05, 'epoch': 0.04}\n",
      "{'loss': 4.17, 'grad_norm': 0.9965826272964478, 'learning_rate': 4.819555320973808e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3219, 'grad_norm': 1.008651852607727, 'learning_rate': 4.8187306195157355e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3211, 'grad_norm': 1.0295435190200806, 'learning_rate': 4.817905918057664e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0926, 'grad_norm': 0.9688014388084412, 'learning_rate': 4.817081216599591e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2809, 'grad_norm': 0.9813153743743896, 'learning_rate': 4.816256515141519e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2389, 'grad_norm': 1.0399940013885498, 'learning_rate': 4.815431813683447e-05, 'epoch': 0.04}\n",
      "{'loss': 4.3175, 'grad_norm': 0.9841344356536865, 'learning_rate': 4.8146071122253746e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2693, 'grad_norm': 1.0478426218032837, 'learning_rate': 4.813782410767302e-05, 'epoch': 0.04}\n",
      "{'loss': 4.245, 'grad_norm': 1.0579465627670288, 'learning_rate': 4.812957709309231e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2138, 'grad_norm': 0.9716281294822693, 'learning_rate': 4.812133007851158e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2554, 'grad_norm': 0.9244222640991211, 'learning_rate': 4.811308306393086e-05, 'epoch': 0.04}\n",
      "{'loss': 4.166, 'grad_norm': 0.9379757642745972, 'learning_rate': 4.810483604935014e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2282, 'grad_norm': 1.0778900384902954, 'learning_rate': 4.809658903476942e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2285, 'grad_norm': 0.9654243588447571, 'learning_rate': 4.808834202018869e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2957, 'grad_norm': 0.9799751043319702, 'learning_rate': 4.8080095005607975e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1134, 'grad_norm': 0.9998027682304382, 'learning_rate': 4.807184799102725e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1624, 'grad_norm': 0.9859493374824524, 'learning_rate': 4.806360097644653e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1954, 'grad_norm': 1.05072820186615, 'learning_rate': 4.8055353961865806e-05, 'epoch': 0.04}\n",
      "{'loss': 4.2451, 'grad_norm': 1.0829925537109375, 'learning_rate': 4.804710694728509e-05, 'epoch': 0.04}\n",
      "{'loss': 4.0853, 'grad_norm': 1.0532948970794678, 'learning_rate': 4.803885993270436e-05, 'epoch': 0.04}\n",
      "{'loss': 4.125, 'grad_norm': 0.9371650815010071, 'learning_rate': 4.803061291812364e-05, 'epoch': 0.04}\n",
      "{'loss': 4.1703, 'grad_norm': 1.0054192543029785, 'learning_rate': 4.802236590354292e-05, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 1204/30314 [29:58<12:04:16,  1.49s/it]2025-11-03 20:08:07,778 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.44 c\n",
      "{'loss': 4.1741, 'grad_norm': 0.9618086218833923, 'learning_rate': 4.8014118888962204e-05, 'epoch': 0.04}\n",
      "{'train_runtime': 1800.4499, 'train_samples_per_second': 1077.543, 'train_steps_per_second': 16.837, 'train_loss': 5.22179103867147, 'epoch': 0.04}\n",
      "  4%|‚ñç         | 1205/30314 [30:00<12:04:53,  1.49s/it]\n",
      "2025-11-03 20:08:07,901 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 20:08:07,930 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:44<00:00,  5.96it/s]\n",
      "2025-11-03 20:09:53,219 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v2'  # 30117 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29502 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 2 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72e493",
   "metadata": {},
   "source": [
    "### FSDP_full_shard_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: World size: 2\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 19:51:37,530 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ FSDP setup\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   sharding_strategy: full_shard\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   cpu_offload: False\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   activation_checkpointing: False\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   backward_prefetch: BACKWARD_POST\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   forward_prefetch: True\n",
      "2025-11-03 19:51:37,530 [INFO] __main__:   state_dict_type: FULL_STATE_DICT\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: ============================================================\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: fsdp\n",
      "2025-11-03 19:51:37,743 [INFO] __main__: ============================================================\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 19:52:13,847 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:52:13,938 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:52:13,938 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:52:13,939 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7f94718b2270>, <lib.callbacks.InspectCallback object at 0x7f94718b2000>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "2025-11-03 19:52:16,567 [INFO] lib.modeling: –ú–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16, ~1.79 GB, –Ω–∞ CPU (–º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ GPU)\n",
      "2025-11-03 19:52:16,835 [INFO] solution: FSDP:\n",
      "full_shard auto_wrap\n",
      "2025-11-03 19:52:16,835 [INFO] solution: FSDP config:\n",
      "{'fsdp_transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'fsdp_backward_prefetch': 'BACKWARD_POST', 'fsdp_forward_prefetch': True, 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_use_orig_params': True, 'fsdp_sync_module_states': True, 'limit_all_gathers': True}\n",
      "2025-11-03 19:52:16,837 [INFO] lib.training: Trainer kwargs: {'model': Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(50257, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      "), 'args': TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=2500,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],\n",
      "fsdp_config={'limit_all_gathers': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'backward_prefetch': 'BACKWARD_POST', 'forward_prefetch': True, 'state_dict_type': 'FULL_STATE_DICT', 'cpu_ram_efficient_loading': True, 'use_orig_params': True, 'sync_module_states': True, 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/app/output_dir/logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_loss,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/app/output_dir/gpt2-1b-russian,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/app/output_dir/gpt2-1b-russian,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=2500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "), 'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1940063\n",
      "}), 'eval_dataset': Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5000\n",
      "}), 'tokenizer': GPT2TokenizerFast(name_or_path='ai-forever/rugpt3small_based_on_gpt2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), 'callbacks': [<lib.callbacks.TimeoutCallback object at 0x7fd4f167e720>, <lib.callbacks.InspectCallback object at 0x7fd4f16c7c20>]}\n",
      "/app/lib/training.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_195216-o0r41zhk\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs8_ga4_FSDP_full_shard_v3\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/o0r41zhk\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 19:52:17,412 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 19:52:17,413 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 19:52:18,459 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1731: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:1737: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:52:19,867 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: Distributed type: DistributedType.FSDP\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: Model wrapper type: FullyShardedDataParallel (module: torch.distributed.fsdp.fully_sharded_data_parallel)\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: –ú–æ–¥–µ–ª—å –æ–±—ë—Ä–Ω—É—Ç–∞ –≤ FSDP\n",
      "2025-11-03 19:52:19,876 [INFO] lib.callbacks: FSDP sharding strategy: ShardingStrategy.FULL_SHARD\n",
      "{'loss': 10.6419, 'grad_norm': 6.161344528198242, 'learning_rate': 4.999340238833543e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6551, 'grad_norm': 3.477644443511963, 'learning_rate': 4.9985155373754705e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9179, 'grad_norm': 3.2947916984558105, 'learning_rate': 4.997690835917398e-05, 'epoch': 0.0}\n",
      "{'loss': 8.5555, 'grad_norm': 1.495105266571045, 'learning_rate': 4.996866134459326e-05, 'epoch': 0.0}\n",
      "{'loss': 8.279, 'grad_norm': 1.198621392250061, 'learning_rate': 4.996041433001254e-05, 'epoch': 0.0}\n",
      "{'loss': 8.0085, 'grad_norm': 1.3304463624954224, 'learning_rate': 4.995216731543181e-05, 'epoch': 0.0}\n",
      "{'loss': 7.908, 'grad_norm': 0.8468947410583496, 'learning_rate': 4.9943920300851096e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7852, 'grad_norm': 0.9712560176849365, 'learning_rate': 4.993567328627037e-05, 'epoch': 0.0}\n",
      "{'loss': 7.7532, 'grad_norm': 0.8447855114936829, 'learning_rate': 4.992742627168966e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5855, 'grad_norm': 1.0049594640731812, 'learning_rate': 4.991917925710893e-05, 'epoch': 0.0}\n",
      "{'loss': 7.633, 'grad_norm': 1.3147387504577637, 'learning_rate': 4.991093224252821e-05, 'epoch': 0.0}\n",
      "{'loss': 7.5566, 'grad_norm': 1.072090983390808, 'learning_rate': 4.990268522794749e-05, 'epoch': 0.0}\n",
      "{'loss': 7.4553, 'grad_norm': 1.2696805000305176, 'learning_rate': 4.9894438213366764e-05, 'epoch': 0.0}\n",
      "{'loss': 7.326, 'grad_norm': 0.961532473564148, 'learning_rate': 4.988619119878604e-05, 'epoch': 0.0}\n",
      "{'loss': 7.3225, 'grad_norm': 1.1015723943710327, 'learning_rate': 4.9877944184205325e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2337, 'grad_norm': 1.3451743125915527, 'learning_rate': 4.9869697169624595e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2041, 'grad_norm': 1.4968640804290771, 'learning_rate': 4.986145015504388e-05, 'epoch': 0.0}\n",
      "{'loss': 7.2032, 'grad_norm': 1.1569275856018066, 'learning_rate': 4.9853203140463156e-05, 'epoch': 0.0}\n",
      "{'loss': 7.1694, 'grad_norm': 1.3323944807052612, 'learning_rate': 4.984495612588243e-05, 'epoch': 0.0}\n",
      "{'loss': 6.9561, 'grad_norm': 0.9584651589393616, 'learning_rate': 4.983670911130171e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8203, 'grad_norm': 0.9726276993751526, 'learning_rate': 4.982846209672099e-05, 'epoch': 0.0}\n",
      "{'loss': 6.8941, 'grad_norm': 0.995424211025238, 'learning_rate': 4.982021508214027e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7298, 'grad_norm': 1.227007269859314, 'learning_rate': 4.981196806755955e-05, 'epoch': 0.0}\n",
      "{'loss': 6.7842, 'grad_norm': 1.026127576828003, 'learning_rate': 4.9803721052978824e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5935, 'grad_norm': 0.9543346762657166, 'learning_rate': 4.979547403839811e-05, 'epoch': 0.0}\n",
      "{'loss': 6.6222, 'grad_norm': 1.0552695989608765, 'learning_rate': 4.978722702381738e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5549, 'grad_norm': 1.1391561031341553, 'learning_rate': 4.977898000923666e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5098, 'grad_norm': 0.9919469952583313, 'learning_rate': 4.977073299465594e-05, 'epoch': 0.0}\n",
      "{'loss': 6.5447, 'grad_norm': 0.9068463444709778, 'learning_rate': 4.9762485980075215e-05, 'epoch': 0.0}\n",
      "{'loss': 6.4389, 'grad_norm': 1.1114656925201416, 'learning_rate': 4.975423896549449e-05, 'epoch': 0.0}\n",
      "{'loss': 6.3631, 'grad_norm': 0.9163604974746704, 'learning_rate': 4.9745991950913776e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4611, 'grad_norm': 1.1020753383636475, 'learning_rate': 4.9737744936333046e-05, 'epoch': 0.01}\n",
      "{'loss': 6.3807, 'grad_norm': 1.000560998916626, 'learning_rate': 4.972949792175233e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2759, 'grad_norm': 1.0252231359481812, 'learning_rate': 4.9721250907171607e-05, 'epoch': 0.01}\n",
      "{'loss': 6.4022, 'grad_norm': 1.074475646018982, 'learning_rate': 4.971300389259089e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2225, 'grad_norm': 1.1453893184661865, 'learning_rate': 4.970475687801016e-05, 'epoch': 0.01}\n",
      "{'loss': 6.2648, 'grad_norm': 1.1735650300979614, 'learning_rate': 4.9696509863429444e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1069, 'grad_norm': 1.0566627979278564, 'learning_rate': 4.968826284884872e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0896, 'grad_norm': 1.074960470199585, 'learning_rate': 4.9680015834268e-05, 'epoch': 0.01}\n",
      "{'loss': 6.1333, 'grad_norm': 1.1017637252807617, 'learning_rate': 4.9671768819687275e-05, 'epoch': 0.01}\n",
      "{'loss': 6.03, 'grad_norm': 1.1273071765899658, 'learning_rate': 4.966352180510656e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0433, 'grad_norm': 1.3091951608657837, 'learning_rate': 4.965527479052583e-05, 'epoch': 0.01}\n",
      "{'loss': 6.057, 'grad_norm': 1.1595767736434937, 'learning_rate': 4.964702777594511e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0316, 'grad_norm': 1.063482642173767, 'learning_rate': 4.963878076136439e-05, 'epoch': 0.01}\n",
      "{'loss': 5.901, 'grad_norm': 1.0704915523529053, 'learning_rate': 4.9630533746783666e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0156, 'grad_norm': 1.0088330507278442, 'learning_rate': 4.962228673220294e-05, 'epoch': 0.01}\n",
      "{'loss': 6.0152, 'grad_norm': 1.058014154434204, 'learning_rate': 4.961403971762223e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9663, 'grad_norm': 1.2822837829589844, 'learning_rate': 4.9605792703041504e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9029, 'grad_norm': 1.131990909576416, 'learning_rate': 4.959754568846078e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7286, 'grad_norm': 0.9675118327140808, 'learning_rate': 4.958929867388006e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8197, 'grad_norm': 1.2069330215454102, 'learning_rate': 4.958105165929934e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7134, 'grad_norm': 1.0483969449996948, 'learning_rate': 4.957280464471861e-05, 'epoch': 0.01}\n",
      "{'loss': 5.9452, 'grad_norm': 1.1181797981262207, 'learning_rate': 4.9564557630137895e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7756, 'grad_norm': 1.1422513723373413, 'learning_rate': 4.955631061555717e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8032, 'grad_norm': 1.4966661930084229, 'learning_rate': 4.954806360097645e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7696, 'grad_norm': 1.0849645137786865, 'learning_rate': 4.9539816586395726e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6892, 'grad_norm': 1.1216474771499634, 'learning_rate': 4.953156957181501e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6887, 'grad_norm': 1.0435843467712402, 'learning_rate': 4.9523322557234286e-05, 'epoch': 0.01}\n",
      "{'loss': 5.648, 'grad_norm': 1.041558027267456, 'learning_rate': 4.951507554265356e-05, 'epoch': 0.01}\n",
      "{'loss': 5.745, 'grad_norm': 0.9225491881370544, 'learning_rate': 4.950682852807284e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6975, 'grad_norm': 1.0287679433822632, 'learning_rate': 4.9498581513492124e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4825, 'grad_norm': 0.9838809967041016, 'learning_rate': 4.9490334498911394e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6692, 'grad_norm': 1.0883960723876953, 'learning_rate': 4.948208748433068e-05, 'epoch': 0.01}\n",
      "{'loss': 5.6684, 'grad_norm': 1.1099557876586914, 'learning_rate': 4.9473840469749954e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5461, 'grad_norm': 1.398963212966919, 'learning_rate': 4.946559345516923e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5691, 'grad_norm': 1.1353247165679932, 'learning_rate': 4.945734644058851e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4419, 'grad_norm': 1.103093147277832, 'learning_rate': 4.944909942600779e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4895, 'grad_norm': 1.0738216638565063, 'learning_rate': 4.944085241142706e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5304, 'grad_norm': 1.124693751335144, 'learning_rate': 4.9432605396846346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5634, 'grad_norm': 1.0684622526168823, 'learning_rate': 4.942435838226562e-05, 'epoch': 0.01}\n",
      "{'loss': 5.5733, 'grad_norm': 0.9761757254600525, 'learning_rate': 4.9416111367684906e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3908, 'grad_norm': 1.0903525352478027, 'learning_rate': 4.9407864353104176e-05, 'epoch': 0.01}\n",
      "{'loss': 5.431, 'grad_norm': 1.1220921277999878, 'learning_rate': 4.939961733852346e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3783, 'grad_norm': 1.1272591352462769, 'learning_rate': 4.939137032394274e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3062, 'grad_norm': 1.0519825220108032, 'learning_rate': 4.9383123309362014e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3408, 'grad_norm': 1.1458935737609863, 'learning_rate': 4.937487629478129e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3559, 'grad_norm': 1.0003204345703125, 'learning_rate': 4.9366629280200574e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3134, 'grad_norm': 1.1891167163848877, 'learning_rate': 4.9358382265619845e-05, 'epoch': 0.01}\n",
      "{'loss': 5.513, 'grad_norm': 0.9363975524902344, 'learning_rate': 4.935013525103913e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3568, 'grad_norm': 1.0879229307174683, 'learning_rate': 4.9341888236458405e-05, 'epoch': 0.01}\n",
      "{'loss': 5.4212, 'grad_norm': 1.0583535432815552, 'learning_rate': 4.933364122187768e-05, 'epoch': 0.01}\n",
      "{'loss': 5.3783, 'grad_norm': 1.1007628440856934, 'learning_rate': 4.932539420729696e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2266, 'grad_norm': 1.1876333951950073, 'learning_rate': 4.931714719271624e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1794, 'grad_norm': 1.1003211736679077, 'learning_rate': 4.930890017813552e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2109, 'grad_norm': 1.1900625228881836, 'learning_rate': 4.9300653163554796e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2586, 'grad_norm': 1.3015501499176025, 'learning_rate': 4.9292406148974073e-05, 'epoch': 0.01}\n",
      "{'loss': 5.151, 'grad_norm': 1.2091796398162842, 'learning_rate': 4.928415913439336e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1944, 'grad_norm': 1.0244771242141724, 'learning_rate': 4.927591211981263e-05, 'epoch': 0.01}\n",
      "{'loss': 5.2759, 'grad_norm': 1.1053807735443115, 'learning_rate': 4.926766510523191e-05, 'epoch': 0.01}\n",
      "{'loss': 5.1877, 'grad_norm': 0.9803803563117981, 'learning_rate': 4.925941809065119e-05, 'epoch': 0.01}\n",
      "{'loss': 5.03, 'grad_norm': 1.0061402320861816, 'learning_rate': 4.9251171076070465e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1323, 'grad_norm': 1.0581709146499634, 'learning_rate': 4.924292406148974e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2329, 'grad_norm': 1.1692291498184204, 'learning_rate': 4.9234677046909025e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1678, 'grad_norm': 1.0997494459152222, 'learning_rate': 4.9226430032328295e-05, 'epoch': 0.02}\n",
      "{'loss': 5.053, 'grad_norm': 1.1355148553848267, 'learning_rate': 4.921818301774758e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2467, 'grad_norm': 1.0697600841522217, 'learning_rate': 4.9209936003166856e-05, 'epoch': 0.02}\n",
      "{'loss': 5.2164, 'grad_norm': 0.947257936000824, 'learning_rate': 4.920168898858614e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0045, 'grad_norm': 0.9384860992431641, 'learning_rate': 4.919344197400541e-05, 'epoch': 0.02}\n",
      "{'loss': 5.1391, 'grad_norm': 1.0149863958358765, 'learning_rate': 4.9185194959424694e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9973, 'grad_norm': 1.0488492250442505, 'learning_rate': 4.917694794484397e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0235, 'grad_norm': 1.0280767679214478, 'learning_rate': 4.916870093026325e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9668, 'grad_norm': 1.1263558864593506, 'learning_rate': 4.9160453915682524e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0763, 'grad_norm': 0.9636012315750122, 'learning_rate': 4.915220690110181e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0829, 'grad_norm': 1.0556645393371582, 'learning_rate': 4.914395988652108e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0027, 'grad_norm': 1.1828125715255737, 'learning_rate': 4.913571287194036e-05, 'epoch': 0.02}\n",
      "{'loss': 5.07, 'grad_norm': 1.0107133388519287, 'learning_rate': 4.912746585735964e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0952, 'grad_norm': 1.0630176067352295, 'learning_rate': 4.911921884277892e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9512, 'grad_norm': 1.0486544370651245, 'learning_rate': 4.911097182819819e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8643, 'grad_norm': 1.1907598972320557, 'learning_rate': 4.9102724813617476e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0203, 'grad_norm': 1.0592553615570068, 'learning_rate': 4.909447779903675e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9779, 'grad_norm': 1.0902537107467651, 'learning_rate': 4.908623078445603e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9587, 'grad_norm': 1.0077029466629028, 'learning_rate': 4.907798376987531e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8344, 'grad_norm': 1.0105639696121216, 'learning_rate': 4.906973675529459e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8012, 'grad_norm': 1.0242735147476196, 'learning_rate': 4.906148974071386e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9231, 'grad_norm': 0.9816271662712097, 'learning_rate': 4.9053242726133144e-05, 'epoch': 0.02}\n",
      "{'loss': 4.835, 'grad_norm': 1.0263639688491821, 'learning_rate': 4.904499571155242e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0517, 'grad_norm': 1.0459864139556885, 'learning_rate': 4.90367486969717e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9445, 'grad_norm': 0.9311509132385254, 'learning_rate': 4.9028501682390975e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8924, 'grad_norm': 1.0497759580612183, 'learning_rate': 4.902025466781026e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8646, 'grad_norm': 1.2211323976516724, 'learning_rate': 4.9012007653229536e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9415, 'grad_norm': 1.0044968128204346, 'learning_rate': 4.900376063864881e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8729, 'grad_norm': 1.0624114274978638, 'learning_rate': 4.899551362406809e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8974, 'grad_norm': 1.0089800357818604, 'learning_rate': 4.898726660948737e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8938, 'grad_norm': 1.0398837327957153, 'learning_rate': 4.897901959490664e-05, 'epoch': 0.02}\n",
      "{'loss': 4.9006, 'grad_norm': 1.0911967754364014, 'learning_rate': 4.897077258032593e-05, 'epoch': 0.02}\n",
      "{'loss': 4.849, 'grad_norm': 1.138630747795105, 'learning_rate': 4.8962525565745204e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8407, 'grad_norm': 0.9779117703437805, 'learning_rate': 4.895427855116448e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7744, 'grad_norm': 1.0078010559082031, 'learning_rate': 4.894603153658376e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7173, 'grad_norm': 1.0831915140151978, 'learning_rate': 4.893778452200304e-05, 'epoch': 0.02}\n",
      "{'loss': 4.797, 'grad_norm': 1.0523992776870728, 'learning_rate': 4.892953750742231e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8212, 'grad_norm': 0.9763866662979126, 'learning_rate': 4.8921290492841595e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8439, 'grad_norm': 1.0647414922714233, 'learning_rate': 4.891304347826087e-05, 'epoch': 0.02}\n",
      "{'loss': 4.8632, 'grad_norm': 0.9521463513374329, 'learning_rate': 4.8904796463680156e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5346, 'grad_norm': 1.0839732885360718, 'learning_rate': 4.8896549449099426e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7985, 'grad_norm': 1.0150030851364136, 'learning_rate': 4.888830243451871e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6397, 'grad_norm': 1.0470433235168457, 'learning_rate': 4.8880055419937986e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6956, 'grad_norm': 0.9974631667137146, 'learning_rate': 4.887180840535726e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6435, 'grad_norm': 1.139997959136963, 'learning_rate': 4.886356139077654e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6754, 'grad_norm': 1.0230571031570435, 'learning_rate': 4.8855314376195824e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7319, 'grad_norm': 1.1717771291732788, 'learning_rate': 4.8847067361615094e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7383, 'grad_norm': 0.9462061524391174, 'learning_rate': 4.883882034703438e-05, 'epoch': 0.02}\n",
      "{'loss': 4.703, 'grad_norm': 1.1449016332626343, 'learning_rate': 4.8830573332453655e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6395, 'grad_norm': 0.9956967234611511, 'learning_rate': 4.882232631787294e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7279, 'grad_norm': 0.99331134557724, 'learning_rate': 4.881407930329221e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6618, 'grad_norm': 1.2324771881103516, 'learning_rate': 4.880583228871149e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7081, 'grad_norm': 1.0728100538253784, 'learning_rate': 4.879758527413077e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6113, 'grad_norm': 0.9720578193664551, 'learning_rate': 4.8789338259550046e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7627, 'grad_norm': 0.9374616146087646, 'learning_rate': 4.878109124496932e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7048, 'grad_norm': 1.047967791557312, 'learning_rate': 4.8772844230388607e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6989, 'grad_norm': 1.0282737016677856, 'learning_rate': 4.876459721580788e-05, 'epoch': 0.02}\n",
      "{'loss': 4.6158, 'grad_norm': 1.1517817974090576, 'learning_rate': 4.875635020122716e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5737, 'grad_norm': 0.9916568994522095, 'learning_rate': 4.874810318664644e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5124, 'grad_norm': 1.0642269849777222, 'learning_rate': 4.8739856172065714e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6655, 'grad_norm': 0.9978151917457581, 'learning_rate': 4.873160915748499e-05, 'epoch': 0.03}\n",
      "{'loss': 4.6361, 'grad_norm': 1.1204804182052612, 'learning_rate': 4.8723362142904275e-05, 'epoch': 0.03}\n",
      "{'loss': 4.4643, 'grad_norm': 0.9572274684906006, 'learning_rate': 4.871511512832355e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5537, 'grad_norm': 1.232858657836914, 'learning_rate': 4.870686811374283e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5102, 'grad_norm': 1.0248379707336426, 'learning_rate': 4.8698621099162105e-05, 'epoch': 0.03}\n",
      "{'loss': 4.5215, 'grad_norm': 1.0052692890167236, 'learning_rate': 4.869037408458139e-05, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 795/30314 [29:59<18:39:46,  2.28s/it]2025-11-03 20:22:21,437 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.56 c\n",
      "{'train_runtime': 1801.5717, 'train_samples_per_second': 1076.872, 'train_steps_per_second': 16.826, 'train_loss': 5.665090173932176, 'epoch': 0.03}\n",
      "  3%|‚ñé         | 796/30314 [30:01<18:33:27,  2.26s/it]\n",
      "2025-11-03 20:22:21,563 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 20:22:21,578 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [01:45<00:00,  5.95it/s]\n",
      "2025-11-03 20:24:07,061 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 20:24:07,061 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.5717, 'train_samples_per_second': 1076.872, 'train_steps_per_second': 16.826, 'total_flos': 6.713499068871475e+16, 'train_loss': 5.665090173932176, 'epoch': 0.026258927540534747}, 'eval': {'eval_loss': 5.252961158752441, 'eval_runtime': 105.4808, 'eval_samples_per_second': 47.402, 'eval_steps_per_second': 5.925, 'epoch': 0.026258927540534747}}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs8_ga4_FSDP_full_shard_v3\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/o0r41zhk\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_195216-o0r41zhk/logs\u001b[0m\n",
      "[rank0]:[W1103 20:24:10.707600155 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "name = 'FSDP_full_shard_v3'  # 20817 MiB\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "  --num_processes 2 \\\n",
    "  --num_machines 1 \\\n",
    "  --main_process_port 29501 \\\n",
    "  /app/train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --fsdp-sharding-strategy full_shard \\\n",
    "    --fsdp-backward-prefetch BACKWARD_POST \\\n",
    "    --fsdp-forward-prefetch \\\n",
    "    --fsdp-state-dict-type FULL_STATE_DICT \\\n",
    "    --fsdp-transformer-layers Qwen3DecoderLayer \\\n",
    "    --bf16 \\\n",
    "    --batch-size 8 \\\n",
    "    --grad-accum 4 \\\n",
    "    --no-generation \\\n",
    "    --run-name {name} \\\n",
    "  2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4100d43",
   "metadata": {},
   "source": [
    "### FSDP_full_shard_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66309d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b766eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1198ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6336aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b59740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37ab25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2f2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602aa91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990782de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: World size: 2\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,064 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   overlap_comm: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 230219.09it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3867.30it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 274473.88it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3991.76it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 13:23:57,067 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:23:57,268 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:23:59,435 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 13:23:59,449 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 13:23:59,452 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_132400-zbzb8ce6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_ds_stage1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zbzb8ce6\u001b[0m\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 13:24:01,614 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6923, 'grad_norm': 3.668853282928467, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.517, 'grad_norm': 5.114009380340576, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8416, 'grad_norm': 3.3759467601776123, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.465, 'grad_norm': 1.1403710842132568, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1789, 'grad_norm': 1.0019216537475586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0067, 'grad_norm': 0.9517723917961121, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8157, 'grad_norm': 0.9263848662376404, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7275, 'grad_norm': 0.7535514235496521, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6883, 'grad_norm': 0.68058842420578, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5827, 'grad_norm': 0.6326014995574951, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4356, 'grad_norm': 1.0102779865264893, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3417, 'grad_norm': 0.7251059412956238, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2124, 'grad_norm': 0.7712108492851257, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1452, 'grad_norm': 1.054246425628662, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1149, 'grad_norm': 0.8243618607521057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0371, 'grad_norm': 1.0601383447647095, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9507, 'grad_norm': 0.9164965748786926, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9402, 'grad_norm': 1.108020305633545, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8186, 'grad_norm': 0.9205239415168762, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7246, 'grad_norm': 0.8873196840286255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6404, 'grad_norm': 0.935792088508606, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6416, 'grad_norm': 0.8166505098342896, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5472, 'grad_norm': 0.7756077647209167, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5763, 'grad_norm': 0.9025195837020874, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3916, 'grad_norm': 0.7719050049781799, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3242, 'grad_norm': 0.960091233253479, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4118, 'grad_norm': 1.125348448753357, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.331, 'grad_norm': 0.9406046271324158, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2364, 'grad_norm': 0.8053609132766724, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2317, 'grad_norm': 0.8614730834960938, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1169, 'grad_norm': 0.8707703351974487, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1839, 'grad_norm': 1.0312784910202026, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0716, 'grad_norm': 0.9302330017089844, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9732, 'grad_norm': 1.0339421033859253, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0483, 'grad_norm': 0.8666685223579407, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9848, 'grad_norm': 1.0706892013549805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8938, 'grad_norm': 0.966848611831665, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8142, 'grad_norm': 0.9936823844909668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8462, 'grad_norm': 0.9697319269180298, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9013, 'grad_norm': 0.8925601840019226, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8742, 'grad_norm': 1.0408942699432373, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6732, 'grad_norm': 0.8731564283370972, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6816, 'grad_norm': 0.8089210987091064, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6384, 'grad_norm': 0.8519142866134644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6944, 'grad_norm': 0.8569779992103577, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5444, 'grad_norm': 1.0784058570861816, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6478, 'grad_norm': 1.1607645750045776, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6064, 'grad_norm': 0.8774507641792297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5571, 'grad_norm': 0.9840154647827148, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5084, 'grad_norm': 0.872087836265564, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4415, 'grad_norm': 0.9287772178649902, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5155, 'grad_norm': 0.849916398525238, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4628, 'grad_norm': 0.8863853216171265, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.456, 'grad_norm': 0.8531199097633362, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3763, 'grad_norm': 0.9935510754585266, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3983, 'grad_norm': 0.88322913646698, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2589, 'grad_norm': 0.9309230446815491, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3191, 'grad_norm': 0.9106134176254272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4164, 'grad_norm': 0.7992880940437317, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2906, 'grad_norm': 0.8568545579910278, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3196, 'grad_norm': 0.8275390267372131, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3159, 'grad_norm': 0.8496686816215515, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2891, 'grad_norm': 0.8499513864517212, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2181, 'grad_norm': 0.9966992735862732, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1707, 'grad_norm': 1.0388319492340088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2432, 'grad_norm': 1.057230830192566, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1071, 'grad_norm': 0.9126641154289246, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1212, 'grad_norm': 0.872837483882904, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0669, 'grad_norm': 0.9906071424484253, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.7880737781524658, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1199, 'grad_norm': 0.9567427039146423, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0848, 'grad_norm': 0.830866813659668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0777, 'grad_norm': 1.2204185724258423, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0781, 'grad_norm': 0.9339037537574768, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0908, 'grad_norm': 0.8431245684623718, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9885, 'grad_norm': 0.8697077631950378, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.976, 'grad_norm': 0.8482258319854736, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9304, 'grad_norm': 0.8283457159996033, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9021, 'grad_norm': 0.987497091293335, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9889, 'grad_norm': 0.906536877155304, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9673, 'grad_norm': 0.8449062705039978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9865, 'grad_norm': 0.9135952591896057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9002, 'grad_norm': 0.8819920420646667, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.992, 'grad_norm': 0.9165951013565063, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7494, 'grad_norm': 0.914996862411499, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8431, 'grad_norm': 0.8120441436767578, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8, 'grad_norm': 0.9576306939125061, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8625, 'grad_norm': 0.9311659932136536, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8136, 'grad_norm': 0.830401599407196, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8378, 'grad_norm': 0.8160765767097473, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8201, 'grad_norm': 0.9861581921577454, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.77, 'grad_norm': 0.8668427467346191, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8024, 'grad_norm': 0.9742636680603027, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6928, 'grad_norm': 0.8702391386032104, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7683, 'grad_norm': 0.8803965449333191, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7899, 'grad_norm': 0.998832643032074, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7026, 'grad_norm': 0.8062134385108948, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7046, 'grad_norm': 0.8597113490104675, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7653, 'grad_norm': 0.9179651737213135, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6869, 'grad_norm': 0.9309827089309692, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8554878830909729, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6945, 'grad_norm': 0.8895065188407898, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.632, 'grad_norm': 0.9682652354240417, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6754, 'grad_norm': 0.9450482130050659, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6229, 'grad_norm': 0.9777501821517944, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5817, 'grad_norm': 0.8886628746986389, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5939, 'grad_norm': 1.0024586915969849, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5841, 'grad_norm': 0.8208844661712646, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5756, 'grad_norm': 0.9499568343162537, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5745, 'grad_norm': 0.9411032795906067, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5397, 'grad_norm': 0.8725298047065735, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.5847, 'grad_norm': 0.9008686542510986, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.616, 'grad_norm': 0.8291811347007751, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5472, 'grad_norm': 0.9068216681480408, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5313, 'grad_norm': 0.7944591045379639, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5414, 'grad_norm': 0.8708136677742004, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5236, 'grad_norm': 0.8637734055519104, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4893, 'grad_norm': 0.873452365398407, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4721, 'grad_norm': 0.8644837737083435, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.467, 'grad_norm': 0.9855910539627075, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5522, 'grad_norm': 0.9470599889755249, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.9512214064598083, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4402, 'grad_norm': 0.9265450239181519, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5141, 'grad_norm': 0.9191153645515442, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4855, 'grad_norm': 0.9722633957862854, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4347, 'grad_norm': 1.054284930229187, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4631, 'grad_norm': 1.1165261268615723, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4786, 'grad_norm': 0.8716094493865967, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4591, 'grad_norm': 0.8535889387130737, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.9702112674713135, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.465, 'grad_norm': 0.9044908881187439, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4295, 'grad_norm': 0.8749884366989136, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.378, 'grad_norm': 0.9084829092025757, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4707, 'grad_norm': 0.9387164115905762, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3379, 'grad_norm': 0.8029746413230896, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3408, 'grad_norm': 0.8033086061477661, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3812, 'grad_norm': 0.8083414435386658, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4302, 'grad_norm': 0.8474555611610413, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4612, 'grad_norm': 0.865831732749939, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3274, 'grad_norm': 0.8965657353401184, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3731, 'grad_norm': 0.8146787285804749, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4656, 'grad_norm': 0.8597487807273865, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.3581, 'grad_norm': 0.8324158787727356, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.3166, 'grad_norm': 0.8755508661270142, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2847, 'grad_norm': 0.8546525239944458, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2597, 'grad_norm': 0.8498454689979553, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.3016, 'grad_norm': 0.8345364928245544, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3691, 'grad_norm': 0.8695455193519592, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3292, 'grad_norm': 0.9809775352478027, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.1963, 'grad_norm': 0.8571671843528748, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.3208, 'grad_norm': 0.9230198264122009, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3167, 'grad_norm': 0.8580373525619507, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2582, 'grad_norm': 0.8793766498565674, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3276, 'grad_norm': 0.8359715342521667, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2428, 'grad_norm': 0.8955110907554626, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2457, 'grad_norm': 1.0296374559402466, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2792, 'grad_norm': 1.004420280456543, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2035, 'grad_norm': 0.8063120245933533, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.2006, 'grad_norm': 0.882687509059906, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 0.9540773630142212, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8488317131996155, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1561, 'grad_norm': 0.82745361328125, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8628276586532593, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2275, 'grad_norm': 0.8829584717750549, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.2703, 'grad_norm': 0.8585745096206665, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.2756, 'grad_norm': 0.816656231880188, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1696, 'grad_norm': 0.8778014183044434, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1722, 'grad_norm': 0.8101583123207092, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1912, 'grad_norm': 0.8710595965385437, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2246, 'grad_norm': 0.8498103618621826, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2187, 'grad_norm': 0.9481128454208374, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1543, 'grad_norm': 0.9330139756202698, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.1959, 'grad_norm': 0.852118968963623, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 869/15157 [29:58<8:12:08,  2.07s/it]2025-11-03 13:54:07,116 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.26 c\n",
      "{'loss': 4.1081, 'grad_norm': 0.9389750957489014, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'train_runtime': 1800.2727, 'train_samples_per_second': 1077.65, 'train_steps_per_second': 8.419, 'train_loss': 5.248912660006819, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 870/15157 [30:00<8:12:43,  2.07s/it]\n",
      "2025-11-03 13:54:07,305 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 13:54:07,311 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.32it/s]\n",
      "2025-11-03 13:54:31,247 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 13:54:32,296 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 13:54:32,297 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\n",
      "\n",
      "–í 1919 –≥–æ–¥—É –≤\n",
      "2025-11-03 13:54:32,297 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 13:54:32,297 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.2727, 'train_samples_per_second': 1077.65, 'train_steps_per_second': 8.419, 'total_flos': 2.9350473316171776e+17, 'train_loss': 5.248912660006819, 'epoch': 0.05740016824187243}, 'eval': {'eval_loss': 4.828156471252441, 'eval_runtime': 23.9401, 'eval_samples_per_second': 208.854, 'eval_steps_per_second': 26.107, 'epoch': 0.05740016824187243}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\\n\\n–í 1919 –≥–æ–¥—É –≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_ds_stage1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zbzb8ce6\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_132400-zbzb8ce6/logs\u001b[0m\n",
      "[rank0]:[W1103 13:54:35.051270647 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "    --num_processes 2 \\\n",
    "    --num_machines 1 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --main_process_port 29501 \\\n",
    "    ../train_distributed.py \\\n",
    "        --mode deepspeed \\\n",
    "        --deepspeed-stage 1 \\\n",
    "        --batch-size 16 \\\n",
    "        --grad-accum 4 \\\n",
    "        --run-name \"ds_stage1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102658c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 15:59:41,682 [INFO] __main__: ============================================================\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: baseline\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: ============================================================\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: World size: 1\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: CUDA_VISIBLE_DEVICES: 1\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "False\n",
      "2025-11-03 15:59:41,683 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ baseline setup (single GPU)\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 16:00:16,922 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, dtype=torch.bfloat16\n",
      "/app/lib/training.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.10\n",
      "wandb: Run data is saved locally in /app/hw2_parallel_pretrain/wandb/run-20251103_160020-ocsbeelq\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run bs16_ga4_Baseline_1_gpu\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ksorzz/llm_hw2-aylesnov\n",
      "wandb: üöÄ View run at https://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ocsbeelq\n",
      "2025-11-03 16:00:21,119 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 16:00:21,119 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 16:00:21,119 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 16:00:21,120 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 0/30314 [00:00<?, ?it/s]W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] or:\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] to include these operations in the captured graph.\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] Graph break: from user code at:\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     return model_forward(*args, **kwargs)\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     return func(*args, **kwargs)\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     output = func(self, *args, **kwargs)\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     causal_mask = self._update_causal_mask(\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 16:00:21.884000 2313240 torch/_dynamo/variables/tensor.py:780] [0/0] \n",
      "W1103 16:00:30.247000 2313240 torch/_dynamo/convert_frame.py:861] [11/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1103 16:00:30.247000 2313240 torch/_dynamo/convert_frame.py:861] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "W1103 16:00:30.247000 2313240 torch/_dynamo/convert_frame.py:861] [11/8]    last reason: 11/0: L['self'].layer_idx == 0                                    \n",
      "W1103 16:00:30.247000 2313240 torch/_dynamo/convert_frame.py:861] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 16:00:30.247000 2313240 torch/_dynamo/convert_frame.py:861] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 12.8747, 'grad_norm': 40.0, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 10.3269, 'grad_norm': 3.265625, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.72, 'grad_norm': 4.09375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.5398, 'grad_norm': 1.7578125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4124, 'grad_norm': 1.328125, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "  0%|          | 26/30314 [00:36<8:24:49,  1.00s/it]^C\n"
     ]
    }
   ],
   "source": [
    "name = \"Baseline_1_gpu\"\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=1 accelerate launch \\\n",
    "    --main_process_port 29501 \\\n",
    "    --num_processes 1 \\\n",
    "    --num_machines 1 \\\n",
    "    /app/train_distributed.py \\\n",
    "        --mode baseline \\\n",
    "        --bf16 true \\\n",
    "        --batch-size 16 \\\n",
    "        --grad-accum 4 \\\n",
    "        --run-name {name} \\\n",
    "    2>&1 | tee /app/data/logs/{name}.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d6d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51b78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc70b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b4de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/app/hw2_parallel_pretrain/train_distributed.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫ –Ω–∞ –æ–¥–Ω–æ–º GPU –±–µ–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "!CUDA_VISIBLE_DEVICES=3 python ../train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"baseline_1gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffc1552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: World size: 2\n",
      "2025-11-03 13:23:21,061 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:23:21,062 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:23:21,064 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   overlap_comm: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 13:23:21,064 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 230219.09it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3867.30it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 274473.88it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3991.76it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 13:23:57,067 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:23:57,268 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:23:59,435 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "2025-11-03 13:23:59,449 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 13:23:59,452 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(**trainer_kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_132400-zbzb8ce6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_ds_stage1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zbzb8ce6\u001b[0m\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 13:24:01,613 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 13:24:01,614 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6923, 'grad_norm': 3.668853282928467, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.517, 'grad_norm': 5.114009380340576, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8416, 'grad_norm': 3.3759467601776123, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.465, 'grad_norm': 1.1403710842132568, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1789, 'grad_norm': 1.0019216537475586, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0067, 'grad_norm': 0.9517723917961121, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8157, 'grad_norm': 0.9263848662376404, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7275, 'grad_norm': 0.7535514235496521, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6883, 'grad_norm': 0.68058842420578, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5827, 'grad_norm': 0.6326014995574951, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4356, 'grad_norm': 1.0102779865264893, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3417, 'grad_norm': 0.7251059412956238, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2124, 'grad_norm': 0.7712108492851257, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1452, 'grad_norm': 1.054246425628662, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1149, 'grad_norm': 0.8243618607521057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0371, 'grad_norm': 1.0601383447647095, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9507, 'grad_norm': 0.9164965748786926, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9402, 'grad_norm': 1.108020305633545, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8186, 'grad_norm': 0.9205239415168762, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7246, 'grad_norm': 0.8873196840286255, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6404, 'grad_norm': 0.935792088508606, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6416, 'grad_norm': 0.8166505098342896, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5472, 'grad_norm': 0.7756077647209167, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5763, 'grad_norm': 0.9025195837020874, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3916, 'grad_norm': 0.7719050049781799, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3242, 'grad_norm': 0.960091233253479, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4118, 'grad_norm': 1.125348448753357, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.331, 'grad_norm': 0.9406046271324158, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2364, 'grad_norm': 0.8053609132766724, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2317, 'grad_norm': 0.8614730834960938, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1169, 'grad_norm': 0.8707703351974487, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1839, 'grad_norm': 1.0312784910202026, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0716, 'grad_norm': 0.9302330017089844, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9732, 'grad_norm': 1.0339421033859253, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0483, 'grad_norm': 0.8666685223579407, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9848, 'grad_norm': 1.0706892013549805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8938, 'grad_norm': 0.966848611831665, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8142, 'grad_norm': 0.9936823844909668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8462, 'grad_norm': 0.9697319269180298, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9013, 'grad_norm': 0.8925601840019226, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8742, 'grad_norm': 1.0408942699432373, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6732, 'grad_norm': 0.8731564283370972, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6816, 'grad_norm': 0.8089210987091064, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6384, 'grad_norm': 0.8519142866134644, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6944, 'grad_norm': 0.8569779992103577, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5444, 'grad_norm': 1.0784058570861816, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6478, 'grad_norm': 1.1607645750045776, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6064, 'grad_norm': 0.8774507641792297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5571, 'grad_norm': 0.9840154647827148, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5084, 'grad_norm': 0.872087836265564, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4415, 'grad_norm': 0.9287772178649902, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5155, 'grad_norm': 0.849916398525238, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4628, 'grad_norm': 0.8863853216171265, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.456, 'grad_norm': 0.8531199097633362, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3763, 'grad_norm': 0.9935510754585266, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3983, 'grad_norm': 0.88322913646698, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2589, 'grad_norm': 0.9309230446815491, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3191, 'grad_norm': 0.9106134176254272, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4164, 'grad_norm': 0.7992880940437317, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2906, 'grad_norm': 0.8568545579910278, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3196, 'grad_norm': 0.8275390267372131, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3159, 'grad_norm': 0.8496686816215515, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2891, 'grad_norm': 0.8499513864517212, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2181, 'grad_norm': 0.9966992735862732, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1707, 'grad_norm': 1.0388319492340088, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2432, 'grad_norm': 1.057230830192566, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1071, 'grad_norm': 0.9126641154289246, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1212, 'grad_norm': 0.872837483882904, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0669, 'grad_norm': 0.9906071424484253, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1002, 'grad_norm': 0.7880737781524658, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1199, 'grad_norm': 0.9567427039146423, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0848, 'grad_norm': 0.830866813659668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0777, 'grad_norm': 1.2204185724258423, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0781, 'grad_norm': 0.9339037537574768, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0908, 'grad_norm': 0.8431245684623718, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9885, 'grad_norm': 0.8697077631950378, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.976, 'grad_norm': 0.8482258319854736, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9304, 'grad_norm': 0.8283457159996033, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9021, 'grad_norm': 0.987497091293335, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9889, 'grad_norm': 0.906536877155304, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9673, 'grad_norm': 0.8449062705039978, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9865, 'grad_norm': 0.9135952591896057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9002, 'grad_norm': 0.8819920420646667, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.992, 'grad_norm': 0.9165951013565063, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7494, 'grad_norm': 0.914996862411499, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8431, 'grad_norm': 0.8120441436767578, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8, 'grad_norm': 0.9576306939125061, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8625, 'grad_norm': 0.9311659932136536, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8136, 'grad_norm': 0.830401599407196, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8378, 'grad_norm': 0.8160765767097473, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8201, 'grad_norm': 0.9861581921577454, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.77, 'grad_norm': 0.8668427467346191, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8024, 'grad_norm': 0.9742636680603027, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6928, 'grad_norm': 0.8702391386032104, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7683, 'grad_norm': 0.8803965449333191, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7899, 'grad_norm': 0.998832643032074, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7026, 'grad_norm': 0.8062134385108948, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7046, 'grad_norm': 0.8597113490104675, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7653, 'grad_norm': 0.9179651737213135, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6869, 'grad_norm': 0.9309827089309692, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7045, 'grad_norm': 0.8554878830909729, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6945, 'grad_norm': 0.8895065188407898, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.632, 'grad_norm': 0.9682652354240417, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6754, 'grad_norm': 0.9450482130050659, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6229, 'grad_norm': 0.9777501821517944, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5817, 'grad_norm': 0.8886628746986389, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5939, 'grad_norm': 1.0024586915969849, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5841, 'grad_norm': 0.8208844661712646, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5756, 'grad_norm': 0.9499568343162537, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5745, 'grad_norm': 0.9411032795906067, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5397, 'grad_norm': 0.8725298047065735, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.5847, 'grad_norm': 0.9008686542510986, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.616, 'grad_norm': 0.8291811347007751, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5472, 'grad_norm': 0.9068216681480408, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5313, 'grad_norm': 0.7944591045379639, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5414, 'grad_norm': 0.8708136677742004, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5236, 'grad_norm': 0.8637734055519104, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4893, 'grad_norm': 0.873452365398407, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4721, 'grad_norm': 0.8644837737083435, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.467, 'grad_norm': 0.9855910539627075, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5522, 'grad_norm': 0.9470599889755249, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.9512214064598083, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4402, 'grad_norm': 0.9265450239181519, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5141, 'grad_norm': 0.9191153645515442, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4855, 'grad_norm': 0.9722633957862854, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4347, 'grad_norm': 1.054284930229187, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4631, 'grad_norm': 1.1165261268615723, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4786, 'grad_norm': 0.8716094493865967, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4591, 'grad_norm': 0.8535889387130737, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.9702112674713135, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.465, 'grad_norm': 0.9044908881187439, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4295, 'grad_norm': 0.8749884366989136, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.378, 'grad_norm': 0.9084829092025757, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4707, 'grad_norm': 0.9387164115905762, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3379, 'grad_norm': 0.8029746413230896, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3408, 'grad_norm': 0.8033086061477661, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3812, 'grad_norm': 0.8083414435386658, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4302, 'grad_norm': 0.8474555611610413, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4612, 'grad_norm': 0.865831732749939, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3274, 'grad_norm': 0.8965657353401184, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3731, 'grad_norm': 0.8146787285804749, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4656, 'grad_norm': 0.8597487807273865, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.3581, 'grad_norm': 0.8324158787727356, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.3166, 'grad_norm': 0.8755508661270142, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2847, 'grad_norm': 0.8546525239944458, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2597, 'grad_norm': 0.8498454689979553, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.3016, 'grad_norm': 0.8345364928245544, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3691, 'grad_norm': 0.8695455193519592, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3292, 'grad_norm': 0.9809775352478027, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.1963, 'grad_norm': 0.8571671843528748, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.3208, 'grad_norm': 0.9230198264122009, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3167, 'grad_norm': 0.8580373525619507, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2582, 'grad_norm': 0.8793766498565674, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3276, 'grad_norm': 0.8359715342521667, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2428, 'grad_norm': 0.8955110907554626, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2457, 'grad_norm': 1.0296374559402466, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2792, 'grad_norm': 1.004420280456543, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2035, 'grad_norm': 0.8063120245933533, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.2006, 'grad_norm': 0.882687509059906, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2789, 'grad_norm': 0.9540773630142212, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8488317131996155, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1561, 'grad_norm': 0.82745361328125, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2286, 'grad_norm': 0.8628276586532593, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2275, 'grad_norm': 0.8829584717750549, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.2703, 'grad_norm': 0.8585745096206665, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.2756, 'grad_norm': 0.816656231880188, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1696, 'grad_norm': 0.8778014183044434, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1722, 'grad_norm': 0.8101583123207092, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1912, 'grad_norm': 0.8710595965385437, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2246, 'grad_norm': 0.8498103618621826, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2187, 'grad_norm': 0.9481128454208374, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1543, 'grad_norm': 0.9330139756202698, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.1959, 'grad_norm': 0.852118968963623, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 869/15157 [29:58<8:12:08,  2.07s/it]2025-11-03 13:54:07,116 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.26 c\n",
      "{'loss': 4.1081, 'grad_norm': 0.9389750957489014, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'train_runtime': 1800.2727, 'train_samples_per_second': 1077.65, 'train_steps_per_second': 8.419, 'train_loss': 5.248912660006819, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 870/15157 [30:00<8:12:43,  2.07s/it]\n",
      "2025-11-03 13:54:07,305 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 13:54:07,311 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.32it/s]\n",
      "2025-11-03 13:54:31,247 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 13:54:32,296 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 13:54:32,297 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\n",
      "\n",
      "–í 1919 –≥–æ–¥—É –≤\n",
      "2025-11-03 13:54:32,297 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 13:54:32,297 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.2727, 'train_samples_per_second': 1077.65, 'train_steps_per_second': 8.419, 'total_flos': 2.9350473316171776e+17, 'train_loss': 5.248912660006819, 'epoch': 0.05740016824187243}, 'eval': {'eval_loss': 4.828156471252441, 'eval_runtime': 23.9401, 'eval_samples_per_second': 208.854, 'eval_steps_per_second': 26.107, 'epoch': 0.05740016824187243}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\\n\\n–í 1919 –≥–æ–¥—É –≤'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_ds_stage1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/zbzb8ce6\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_132400-zbzb8ce6/logs\u001b[0m\n",
      "[rank0]:[W1103 13:54:35.051270647 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "    --num_processes 2 \\\n",
    "    --num_machines 1 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --main_process_port 29501 \\\n",
    "    ../train_distributed.py \\\n",
    "        --mode deepspeed \\\n",
    "        --deepspeed-stage 1 \\\n",
    "        --batch-size 16 \\\n",
    "        --grad-accum 4 \\\n",
    "        --run-name \"ds_stage1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 13:16:38,590 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:16:38,590 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:16:38,591 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: ============================================================\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: World size: 2\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 13:16:38,669 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "2025-11-03 13:16:38,669 [INFO] __main__:   overlap_comm: False\n",
      "2025-11-03 13:16:38,669 [INFO] __main__:   offload_optimizer: False\n",
      "2025-11-03 13:16:38,669 [INFO] __main__:   offload_params: False\n",
      "2025-11-03 13:16:38,669 [INFO] __main__:   reduce_bucket_size: 500000000\n",
      "2025-11-03 13:16:38,669 [INFO] __main__:   allgather_bucket_size: 500000000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 236715.57it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3277.94it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 277883.49it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 4217.89it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 13:17:14,723 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:17:18,181 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "--------------------------------\n",
      "Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "--------------------------------\n",
      "2025-11-03 13:17:20,322 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 354, in <module>\n",
      "[rank1]:     sys.exit(main())\n",
      "[rank1]:              ^^^^^^\n",
      "[rank1]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 290, in main\n",
      "[rank1]:     setup = build_deepspeed_setup(\n",
      "[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/app/solution.py\", line 219, in build_deepspeed_setup\n",
      "[rank1]:     return build_trainer_setup(\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/app/solution.py\", line 171, in build_trainer_setup\n",
      "[rank1]:     trainer = create_trainer(\n",
      "[rank1]:               ^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/app/lib/training.py\", line 83, in create_trainer\n",
      "[rank1]:     use_fsdp = training_args.fsdp is not None and len(training_args.fsdp) > 0\n",
      "[rank1]:                ^^^^^^^^^^^^^^^^^^\n",
      "[rank1]: AttributeError: 'tuple' object has no attribute 'fsdp'\n",
      "2025-11-03 13:17:20,331 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'allgather_bucket_size': 500000000, 'allgather_partitions': True, 'reduce_scatter': True, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 354, in <module>\n",
      "[rank0]:     sys.exit(main())\n",
      "[rank0]:              ^^^^^^\n",
      "[rank0]:   File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 290, in main\n",
      "[rank0]:     setup = build_deepspeed_setup(\n",
      "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/app/solution.py\", line 219, in build_deepspeed_setup\n",
      "[rank0]:     return build_trainer_setup(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/app/solution.py\", line 171, in build_trainer_setup\n",
      "[rank0]:     trainer = create_trainer(\n",
      "[rank0]:               ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/app/lib/training.py\", line 83, in create_trainer\n",
      "[rank0]:     use_fsdp = training_args.fsdp is not None and len(training_args.fsdp) > 0\n",
      "[rank0]:                ^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: AttributeError: 'tuple' object has no attribute 'fsdp'\n",
      "[rank0]:[W1103 13:17:21.884692531 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "W1103 13:17:22.715000 1871114 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1871338 closing signal SIGTERM\n",
      "E1103 13:17:22.748000 1871114 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 1871339) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1204, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 825, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "../train_distributed.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-03_13:17:22\n",
      "  host      : da18cebda4f2\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1871339)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2,3 accelerate launch \\\n",
    "    --num_processes 2 \\\n",
    "    --num_machines 1 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --main_process_port 29501 \\\n",
    "    ../train_distributed.py \\\n",
    "        --mode deepspeed \\\n",
    "        --deepspeed-stage 1 \\\n",
    "        --batch-size 16 \\\n",
    "        --grad-accum 4 \\\n",
    "        --run-name \"ds_stage1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7f71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78940421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf94b2f",
   "metadata": {},
   "source": [
    "### DeepSpeed (stage 1) baseline\n",
    "- GPU utilization: 58057 MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497adcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 accelerate launch \\\n",
    "    --num_processes 2 \\\n",
    "    --num_machines 1 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    ../train_distributed.py \\\n",
    "        --batch-size 2 \\\n",
    "        --grad-accum 8 \\\n",
    "        --mode deepspeed \\\n",
    "        --deepspeed-stage 1 \\\n",
    "        --run-name \"DeepSpeed_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a83b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d106f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e60bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-03 08:00:43,127] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
      "[2025-11-03 08:00:43,127] [INFO] [runner.py:630:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info ../train_distributed.py --mode deepspeed --deepspeed-stage 1 --run-name DeepSpeed_1\n",
      "[2025-11-03 08:00:48,993] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.23.4\n",
      "[2025-11-03 08:00:48,993] [INFO] [launch.py:155:main] 0 AWS_OFI_NCCL_VERSION=1.12.1\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:180:main] dist_world_size=2\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "[2025-11-03 08:00:48,994] [INFO] [launch.py:272:main] process 987776 spawned with command: ['/usr/bin/python3', '-u', '../train_distributed.py', '--local_rank=0', '--mode', 'deepspeed', '--deepspeed-stage', '1', '--run-name', 'DeepSpeed_1']\n",
      "[2025-11-03 08:00:48,995] [INFO] [launch.py:272:main] process 987777 spawned with command: ['/usr/bin/python3', '-u', '../train_distributed.py', '--local_rank=1', '--mode', 'deepspeed', '--deepspeed-stage', '1', '--run-name', 'DeepSpeed_1']\n",
      "2025-11-03 08:00:54,866 [INFO] __main__: ============================================================\n",
      "2025-11-03 08:00:54,866 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 08:00:54,866 [INFO] __main__: ============================================================\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: ============================================================\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: ============================================================\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: World size: 2\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 08:00:54,870 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 251344.06it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 4094.37it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 244922.86it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3658.18it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 08:01:33,514 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-03 08:01:35,076 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-03 08:01:39,077 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-11-03 08:01:39,089 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-03 08:01:39,093 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_080139-dtmvmy2c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_DeepSpeed_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/dtmvmy2c\u001b[0m\n",
      "2025-11-03 08:01:40,974 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 08:01:40,974 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': False, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 08:01:40,975 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 08:01:40,975 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "{'loss': 10.6923, 'grad_norm': 3.668886661529541, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5169, 'grad_norm': 5.0848259925842285, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8414, 'grad_norm': 3.376645088195801, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4647, 'grad_norm': 1.139607548713684, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1788, 'grad_norm': 1.0166484117507935, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0068, 'grad_norm': 0.9529062509536743, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8158, 'grad_norm': 0.9280924201011658, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7277, 'grad_norm': 0.7572976350784302, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6886, 'grad_norm': 0.6920034289360046, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.5826, 'grad_norm': 0.6365709900856018, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4361, 'grad_norm': 1.0112247467041016, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3418, 'grad_norm': 0.7088518738746643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2128, 'grad_norm': 0.7551684975624084, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1454, 'grad_norm': 1.0765776634216309, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1151, 'grad_norm': 0.8405313491821289, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0372, 'grad_norm': 1.0586222410202026, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9507, 'grad_norm': 0.9185164570808411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9408, 'grad_norm': 1.1382321119308472, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8191, 'grad_norm': 0.9295570850372314, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7251, 'grad_norm': 0.8769192695617676, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6407, 'grad_norm': 0.9296510219573975, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6418, 'grad_norm': 0.8026238679885864, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5474, 'grad_norm': 0.7830801606178284, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5761, 'grad_norm': 0.8849901556968689, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3916, 'grad_norm': 0.7777788639068604, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.324, 'grad_norm': 0.9552931785583496, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4119, 'grad_norm': 1.125848412513733, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3314, 'grad_norm': 0.9530217051506042, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2364, 'grad_norm': 0.8163461089134216, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2317, 'grad_norm': 0.8481809496879578, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1166, 'grad_norm': 0.8597080707550049, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1836, 'grad_norm': 1.0309721231460571, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0716, 'grad_norm': 0.9134701490402222, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9729, 'grad_norm': 1.0352431535720825, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0481, 'grad_norm': 0.8619648814201355, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9846, 'grad_norm': 1.0468069314956665, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8937, 'grad_norm': 0.9569676518440247, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8136, 'grad_norm': 0.9954938888549805, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8458, 'grad_norm': 0.9851377606391907, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.901, 'grad_norm': 0.8868262767791748, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.874, 'grad_norm': 1.0580955743789673, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6731, 'grad_norm': 0.8876131772994995, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6814, 'grad_norm': 0.819622814655304, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6381, 'grad_norm': 0.8524339199066162, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.694, 'grad_norm': 0.8564214706420898, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5443, 'grad_norm': 1.0821787118911743, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6478, 'grad_norm': 1.1606947183609009, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6062, 'grad_norm': 0.8767840266227722, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5569, 'grad_norm': 0.9851747751235962, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5082, 'grad_norm': 0.875636100769043, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4414, 'grad_norm': 0.9244468212127686, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5154, 'grad_norm': 0.8509955406188965, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4628, 'grad_norm': 0.8822569847106934, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4558, 'grad_norm': 0.8552053570747375, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3762, 'grad_norm': 1.0057010650634766, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3984, 'grad_norm': 0.890267550945282, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2588, 'grad_norm': 0.934053897857666, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.319, 'grad_norm': 0.9125190377235413, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4163, 'grad_norm': 0.8011823296546936, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2904, 'grad_norm': 0.8548840880393982, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3194, 'grad_norm': 0.8216761946678162, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3157, 'grad_norm': 0.8513379693031311, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2891, 'grad_norm': 0.8514724373817444, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.218, 'grad_norm': 0.9933102130889893, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1706, 'grad_norm': 1.0468170642852783, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2431, 'grad_norm': 1.059382677078247, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.107, 'grad_norm': 0.9173117876052856, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1211, 'grad_norm': 0.8715150356292725, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0669, 'grad_norm': 0.986964225769043, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1001, 'grad_norm': 0.79168701171875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1198, 'grad_norm': 0.951166033744812, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0847, 'grad_norm': 0.832638144493103, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0775, 'grad_norm': 1.2090805768966675, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0778, 'grad_norm': 0.9313634037971497, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0905, 'grad_norm': 0.8407249450683594, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.9884, 'grad_norm': 0.8772987127304077, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9758, 'grad_norm': 0.8408275842666626, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9303, 'grad_norm': 0.8244236707687378, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9018, 'grad_norm': 0.9789159297943115, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9888, 'grad_norm': 0.9088580012321472, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9671, 'grad_norm': 0.8473283648490906, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9864, 'grad_norm': 0.9116654992103577, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9003, 'grad_norm': 0.8923336267471313, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9918, 'grad_norm': 0.9166820645332336, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7492, 'grad_norm': 0.9134496450424194, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8431, 'grad_norm': 0.814933717250824, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7998, 'grad_norm': 0.9493736028671265, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8622, 'grad_norm': 0.9363677501678467, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8135, 'grad_norm': 0.8326226472854614, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8378, 'grad_norm': 0.8172690272331238, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.82, 'grad_norm': 0.986993670463562, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.7697, 'grad_norm': 0.8710469007492065, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.8024, 'grad_norm': 0.9741200804710388, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.6926, 'grad_norm': 0.8712822198867798, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7681, 'grad_norm': 0.8828373551368713, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7896, 'grad_norm': 0.9930153489112854, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.7025, 'grad_norm': 0.8077489733695984, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7044, 'grad_norm': 0.8623912334442139, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7653, 'grad_norm': 0.9247799515724182, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6868, 'grad_norm': 0.9330095052719116, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7044, 'grad_norm': 0.8570369482040405, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6943, 'grad_norm': 0.8849122524261475, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.6319, 'grad_norm': 0.9657155871391296, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6753, 'grad_norm': 0.9446120262145996, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6227, 'grad_norm': 0.9754360318183899, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5815, 'grad_norm': 0.8851128220558167, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.5937, 'grad_norm': 1.0053298473358154, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5841, 'grad_norm': 0.8235338926315308, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5754, 'grad_norm': 0.947355329990387, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5745, 'grad_norm': 0.947279691696167, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5398, 'grad_norm': 0.8718757629394531, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.5848, 'grad_norm': 0.901652455329895, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6161, 'grad_norm': 0.8330569267272949, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5472, 'grad_norm': 0.905674159526825, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5311, 'grad_norm': 0.7911359071731567, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.5414, 'grad_norm': 0.8739374876022339, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5237, 'grad_norm': 0.866355299949646, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4892, 'grad_norm': 0.8743909597396851, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.472, 'grad_norm': 0.8595970869064331, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.4669, 'grad_norm': 0.9820914268493652, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5521, 'grad_norm': 0.9510051012039185, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.4588, 'grad_norm': 0.9424822330474854, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4401, 'grad_norm': 0.9261564612388611, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.514, 'grad_norm': 0.922075092792511, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4855, 'grad_norm': 0.9686397314071655, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4348, 'grad_norm': 1.0572723150253296, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.4632, 'grad_norm': 1.1179131269454956, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4785, 'grad_norm': 0.8695668578147888, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.459, 'grad_norm': 0.8510275483131409, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4927, 'grad_norm': 0.9668416380882263, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4649, 'grad_norm': 0.9045689702033997, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4292, 'grad_norm': 0.8790872693061829, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.3778, 'grad_norm': 0.9074903130531311, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4707, 'grad_norm': 0.9352619051933289, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3378, 'grad_norm': 0.803787887096405, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3407, 'grad_norm': 0.8047147989273071, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3812, 'grad_norm': 0.8080335855484009, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.43, 'grad_norm': 0.8463324308395386, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4611, 'grad_norm': 0.8624976873397827, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3272, 'grad_norm': 0.8995963931083679, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3732, 'grad_norm': 0.8183782696723938, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4655, 'grad_norm': 0.8648661375045776, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.3582, 'grad_norm': 0.835231363773346, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.3166, 'grad_norm': 0.8783912062644958, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2845, 'grad_norm': 0.8482621312141418, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2597, 'grad_norm': 0.8497687578201294, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.3016, 'grad_norm': 0.8368536233901978, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.369, 'grad_norm': 0.8665884733200073, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3291, 'grad_norm': 0.9815393686294556, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.1963, 'grad_norm': 0.8582843542098999, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.3207, 'grad_norm': 0.9199989438056946, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3167, 'grad_norm': 0.859683096408844, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2582, 'grad_norm': 0.8843957781791687, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3277, 'grad_norm': 0.8384367823600769, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2427, 'grad_norm': 0.8952849507331848, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2456, 'grad_norm': 1.0193899869918823, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.279, 'grad_norm': 0.9977849721908569, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2037, 'grad_norm': 0.8086522221565247, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.2006, 'grad_norm': 0.8832888007164001, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2788, 'grad_norm': 0.9539489150047302, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2285, 'grad_norm': 0.8475479483604431, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1559, 'grad_norm': 0.8272361159324646, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2284, 'grad_norm': 0.8647887110710144, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2277, 'grad_norm': 0.8833691477775574, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.2703, 'grad_norm': 0.8557849526405334, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.2755, 'grad_norm': 0.8127745985984802, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1696, 'grad_norm': 0.8785773515701294, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1721, 'grad_norm': 0.8103262186050415, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.191, 'grad_norm': 0.870949923992157, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2244, 'grad_norm': 0.8477890491485596, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2187, 'grad_norm': 0.9485028982162476, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1541, 'grad_norm': 0.9302243590354919, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.1957, 'grad_norm': 0.8543559908866882, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 869/15157 [29:59<8:12:19,  2.07s/it]2025-11-03 08:31:47,386 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.29 c\n",
      "{'loss': 4.1078, 'grad_norm': 0.9402413368225098, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'train_runtime': 1801.3002, 'train_samples_per_second': 1077.035, 'train_steps_per_second': 8.414, 'train_loss': 5.248835392655997, 'epoch': 0.06}\n",
      "  6%|‚ñà‚ñà                                   | 870/15157 [30:01<8:13:00,  2.07s/it]\n",
      "2025-11-03 08:31:47,573 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-03 08:31:47,591 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:23<00:00, 26.37it/s]\n",
      "2025-11-03 08:32:11,494 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-03 08:32:12,556 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-03 08:32:12,556 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\n",
      "\n",
      "–í 1919 –≥–æ–¥—É –≤\n",
      "2025-11-03 08:32:12,556 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-03 08:32:12,557 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.3002, 'train_samples_per_second': 1077.035, 'train_steps_per_second': 8.414, 'total_flos': 2.9350473316171776e+17, 'train_loss': 5.248835392655997, 'epoch': 0.05740016824187243}, 'eval': {'eval_loss': 4.827876567840576, 'eval_runtime': 23.9007, 'eval_samples_per_second': 209.198, 'eval_steps_per_second': 26.15, 'epoch': 0.05740016824187243}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ –ú–æ—Å–∫–æ–≤—Å–∫–æ–º —Å—Ç–∏–ª–µ –≤ 1935 –≥–æ–¥—É. –í 1945 –≥–æ–¥—É, –≤–æ –≤—Ä–µ–º—è –í—Ç–æ—Ä–æ–π –º–∏—Ä–æ–≤–æ–π –≤–æ–π–Ω—ã, –≤ 1944 –≥–æ–¥—É –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 13 –ª–µ—Ç, –∫–æ–≥–¥–∞ –±—ã–ª–∞ –æ—Ç–∫—Ä—ã—Ç–∞ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª—å–Ω–∏–∫–∞ —à—Ç–∞–±–∞. –í 1926 –≥–æ–¥—É, –ø–æ—Å–ª–µ –µ—ë —Å–º–µ—Ä—Ç–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –æ–Ω —Å—Ç–∞–ª –∑–∞–≤–µ–¥—É—é—â–∏–º –≤–æ–π—Å–∫–∞–º–∏.\\n\\n–í 1919 –≥–æ–¥—É –≤'}\n",
      "[2025-11-03 08:32:14,234] [INFO] [launch.py:367:main] Process 987777 exits successfully.\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_DeepSpeed_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/dtmvmy2c\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_080139-dtmvmy2c/logs\u001b[0m\n",
      "[rank0]:[W1103 08:32:15.683422472 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "[2025-11-03 08:32:18,236] [INFO] [launch.py:367:main] Process 987776 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 deepspeed --num_gpus=2 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --run-name \"DeepSpeed_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34af0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 deepspeed --num_gpus=2 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 2 \\\n",
    "    --run-name \"DeepSpeed_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 deepspeed --num_gpus=2 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 3 \\\n",
    "    --run-name \"DeepSpeed_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e0775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958fbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75994b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: ============================================================\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: ============================================================\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: World size: 1\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-03 07:26:22,810 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-03 07:26:22,812 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 279620.27it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 4215.85it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-03 07:26:58,404 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-03 07:26:59,242 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 8, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251103_072701-59o4c2fg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs8_ga4_lr5e-05_adamw_torch_DeepSpeed_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/59o4c2fg\u001b[0m\n",
      "2025-11-03 07:27:02,870 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-03 07:27:02,871 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-03 07:27:02,871 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-03 07:27:02,871 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/30314 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 220, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 210, in main\n",
      "    metrics = run_training_session(setup, final_evaluation=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/app/solution.py\", line 248, in run_training_session\n",
      "    return run_training(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/app/lib/training.py\", line 138, in run_training\n",
      "    train_output = trainer.train()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2240, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 3791, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 2446, in backward\n",
      "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'backward'\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs8_ga4_lr5e-05_adamw_torch_DeepSpeed_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/59o4c2fg\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251103_072701-59o4c2fg/logs\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1213, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 795, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python3', '../train_distributed.py', '--mode', 'deepspeed', '--deepspeed-stage', '1', '--run-name', 'DeepSpeed_1']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch \\\n",
    "  --num_processes 2 --num_machines 1 --gpu_ids 2,3 \\\n",
    "  --mixed_precision bf16 \\\n",
    "  ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --run-name \"DeepSpeed_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6b19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f4719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fb652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ba68059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/app/hw2_parallel_pretrain/../train_distributed.py\", line 9, in <module>\n",
      "    from lib.constants import OUTPUT_DIR\n",
      "  File \"/app/lib/__init__.py\", line 24, in <module>\n",
      "    from .tokenizer import build_tokenizer\n",
      "  File \"/app/lib/tokenizer.py\", line 7, in <module>\n",
      "    from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\", line 1018, in <module>\n",
      "    import_structure = define_import_structure(Path(__file__).parent / \"models\", prefix=\"models\")\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2597, in define_import_structure\n",
      "W1103 07:19:13.956000 817813 torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "    import_structure = create_import_structure_from_path(module_path)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2310, in create_import_structure_from_path\n",
      "W1103 07:19:13.957000 817813 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 817901 closing signal SIGINT\n",
      "    import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n",
      "object address  : 0x7f5c8b5999c0\n",
      "object refcount : 3\n",
      "object type     : 0xa35620\n",
      "object type name: KeyboardInterrupt\n",
      "object repr     : KeyboardInterrupt()\n",
      "lost sys.stderr\n",
      "^C\n",
      "W1103 07:19:14.006000 817813 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 817901 closing signal SIGTERM\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 711, in run\n",
      "    result = self._invoke_run(role)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 870, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 817813 got signal: 2\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.6.0a0+df5bbc09d1.nv24.12', 'console_scripts', 'torchrun')())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 720, in run\n",
      "    self._shutdown(e.sigval)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py\", line 372, in _shutdown\n",
      "    self._pcontext.close(death_sig)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 572, in close\n",
      "    self._close(death_sig=death_sig, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 909, in _close\n",
      "    handler.proc.wait(time_to_wait)\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 1264, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/subprocess.py\", line 2047, in _wait\n",
      "    time.sleep(delay)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 817813 got signal: 2\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 torchrun --nproc_per_node=1 --master_port=29501 ../train_distributed.py \\\n",
    "    --mode baseline \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"baseline_test\"\n",
    "\n",
    "# ~ 29.5–≥–± –Ω–∞ 1 GPU, –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–∞—è (95-100%)\n",
    "# –£—Å–ø–µ–ª–æ –ø—Ä–æ–π—Ç–∏ 1175 —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf21aab",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 18:36:29,438 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,438 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-02 18:36:29,438 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: World size: 2\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: CUDA_VISIBLE_DEVICES: 0,1\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-02 18:36:29,879 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 1)\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 270600.26it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 2788.49it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 254682.60it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3978.58it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-02 18:37:07,957 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-02 18:37:08,732 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-02 18:37:10,905 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-11-02 18:37:10,919 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-02 18:37:10,922 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 1, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251102_183711-q6scqvdn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/q6scqvdn\u001b[0m\n",
      "2025-11-02 18:37:12,857 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-02 18:37:12,857 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-02 18:37:12,858 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-02 18:37:12,858 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:17.531000 77671 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/15157 [00:00<?, ?it/s][rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:37:18.366000 77670 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1102 18:37:27.316000 77671 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1102 18:37:27.536000 77670 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 10.6919, 'grad_norm': 3.667879104614258, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.5142, 'grad_norm': 4.834280014038086, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.8381, 'grad_norm': 3.3495538234710693, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4606, 'grad_norm': 1.1649107933044434, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.1784, 'grad_norm': 1.1234673261642456, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0081, 'grad_norm': 0.9271871447563171, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8174, 'grad_norm': 0.9527190327644348, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7291, 'grad_norm': 0.7766323685646057, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6891, 'grad_norm': 0.7055681347846985, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.579, 'grad_norm': 0.6939873099327087, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.4326, 'grad_norm': 0.9865968823432922, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.3369, 'grad_norm': 0.8434524536132812, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.2052, 'grad_norm': 0.7207794785499573, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1416, 'grad_norm': 1.0892233848571777, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.1121, 'grad_norm': 0.924543023109436, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.0345, 'grad_norm': 1.024427890777588, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9505, 'grad_norm': 0.9839465022087097, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.938, 'grad_norm': 0.9596324563026428, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8168, 'grad_norm': 0.922659158706665, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7224, 'grad_norm': 0.8615850210189819, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6355, 'grad_norm': 0.8510178923606873, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.6369, 'grad_norm': 0.7569057941436768, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5431, 'grad_norm': 0.7975549101829529, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5706, 'grad_norm': 0.8864257335662842, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3867, 'grad_norm': 0.8010669350624084, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3187, 'grad_norm': 0.8247262835502625, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4063, 'grad_norm': 1.0405585765838623, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.3246, 'grad_norm': 0.8971840739250183, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2296, 'grad_norm': 0.8301199078559875, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.2263, 'grad_norm': 0.8263239860534668, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1102, 'grad_norm': 0.8279992938041687, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.1778, 'grad_norm': 1.0444562435150146, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0663, 'grad_norm': 0.9371616244316101, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.9674, 'grad_norm': 0.9814410209655762, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.0441, 'grad_norm': 0.8351292610168457, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.981, 'grad_norm': 0.9901759624481201, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8896, 'grad_norm': 0.9831438660621643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8088, 'grad_norm': 0.9669746160507202, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8419, 'grad_norm': 0.9979885220527649, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.896, 'grad_norm': 0.8153659105300903, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.8679, 'grad_norm': 0.9968246817588806, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6678, 'grad_norm': 1.005164623260498, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6786, 'grad_norm': 0.8769382834434509, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6353, 'grad_norm': 0.8790581226348877, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.6899, 'grad_norm': 0.8514677286148071, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 5.5394, 'grad_norm': 1.1342464685440063, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.6428, 'grad_norm': 1.093551754951477, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5992, 'grad_norm': 0.8555868268013, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5517, 'grad_norm': 0.9614889025688171, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5031, 'grad_norm': 0.9059622883796692, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4323, 'grad_norm': 0.917082667350769, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.5109, 'grad_norm': 0.832839846611023, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.456, 'grad_norm': 0.8498347401618958, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4512, 'grad_norm': 0.9275631308555603, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3715, 'grad_norm': 1.0327497720718384, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3933, 'grad_norm': 0.9068701267242432, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2537, 'grad_norm': 0.9387128353118896, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3144, 'grad_norm': 0.9276350140571594, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.4114, 'grad_norm': 0.787821888923645, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2858, 'grad_norm': 0.860021710395813, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3146, 'grad_norm': 0.8115845322608948, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.3107, 'grad_norm': 0.8559317588806152, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2852, 'grad_norm': 0.8631620407104492, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.2138, 'grad_norm': 1.000807523727417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1666, 'grad_norm': 1.0203839540481567, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.239, 'grad_norm': 1.0554753541946411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1035, 'grad_norm': 0.9241543412208557, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1167, 'grad_norm': 0.8686804175376892, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0631, 'grad_norm': 0.9896292686462402, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0961, 'grad_norm': 0.8021072745323181, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.1158, 'grad_norm': 0.9412711262702942, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0807, 'grad_norm': 0.8521290421485901, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0746, 'grad_norm': 1.255803108215332, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0749, 'grad_norm': 0.9653118252754211, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.0875, 'grad_norm': 0.8498719334602356, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 4.985, 'grad_norm': 0.8668097853660583, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9725, 'grad_norm': 0.8692824244499207, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9274, 'grad_norm': 0.8487938046455383, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8984, 'grad_norm': 0.9646814465522766, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9856, 'grad_norm': 0.9075605273246765, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.9639, 'grad_norm': 0.8378884196281433, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.983, 'grad_norm': 0.901380717754364, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8973, 'grad_norm': 0.8806772828102112, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.989, 'grad_norm': 0.926916778087616, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7466, 'grad_norm': 0.9136208891868591, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8408, 'grad_norm': 0.8051066398620605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.7971, 'grad_norm': 0.9543333649635315, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8591, 'grad_norm': 0.9228493571281433, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8108, 'grad_norm': 0.8425719738006592, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8353, 'grad_norm': 0.8186773657798767, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.8174, 'grad_norm': 0.966996431350708, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 4.767, 'grad_norm': 0.8752743601799011, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 4.7999, 'grad_norm': 0.9559480547904968, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 4.69, 'grad_norm': 0.8767609000205994, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 4.7658, 'grad_norm': 0.8830179572105408, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 4.7873, 'grad_norm': 0.979434609413147, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 4.6995, 'grad_norm': 0.7963792085647583, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 4.7015, 'grad_norm': 0.8574976921081543, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 4.7628, 'grad_norm': 0.9147696495056152, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 4.6845, 'grad_norm': 0.931691586971283, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 4.7021, 'grad_norm': 0.8537668585777283, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 4.6919, 'grad_norm': 0.8725757598876953, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 4.629, 'grad_norm': 0.9671273827552795, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 4.6728, 'grad_norm': 0.9306085109710693, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 4.6205, 'grad_norm': 0.9877721667289734, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 4.5798, 'grad_norm': 0.8931983709335327, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 4.592, 'grad_norm': 1.0162192583084106, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 4.5825, 'grad_norm': 0.834335446357727, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 4.5738, 'grad_norm': 0.9541210532188416, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 4.5727, 'grad_norm': 0.9583180546760559, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 4.5376, 'grad_norm': 0.8774888515472412, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 4.583, 'grad_norm': 0.9095102548599243, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 4.6139, 'grad_norm': 0.8344876766204834, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 4.5453, 'grad_norm': 0.9107238054275513, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 4.5297, 'grad_norm': 0.7896857857704163, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 4.54, 'grad_norm': 0.8841339349746704, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 4.5222, 'grad_norm': 0.8691622018814087, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 4.4877, 'grad_norm': 0.880927324295044, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 4.4702, 'grad_norm': 0.8557435870170593, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 4.4655, 'grad_norm': 0.9839643239974976, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 4.5504, 'grad_norm': 0.9460034370422363, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 4.457, 'grad_norm': 0.9450525045394897, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 4.4385, 'grad_norm': 0.9350773692131042, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 4.5125, 'grad_norm': 0.9361116886138916, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 4.4843, 'grad_norm': 0.9770078659057617, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 4.4331, 'grad_norm': 1.0450639724731445, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 4.461, 'grad_norm': 1.105711817741394, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 4.4767, 'grad_norm': 0.8797963857650757, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 4.4578, 'grad_norm': 0.8582158088684082, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 4.4914, 'grad_norm': 0.9478503465652466, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 4.4626, 'grad_norm': 0.8924883604049683, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 4.4274, 'grad_norm': 0.8795104622840881, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 4.3764, 'grad_norm': 0.9075436592102051, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 4.4696, 'grad_norm': 0.9432616829872131, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 4.3367, 'grad_norm': 0.8024809956550598, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 4.3395, 'grad_norm': 0.8065676093101501, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "{'loss': 4.3803, 'grad_norm': 0.8119853734970093, 'learning_rate': 0.0006048903225806451, 'epoch': 0.05}\n",
      "{'loss': 4.4291, 'grad_norm': 0.8572216033935547, 'learning_rate': 0.0006028580645161289, 'epoch': 0.05}\n",
      "{'loss': 4.4599, 'grad_norm': 0.8669925332069397, 'learning_rate': 0.0006008258064516129, 'epoch': 0.05}\n",
      "{'loss': 4.3264, 'grad_norm': 0.894651472568512, 'learning_rate': 0.0005987935483870967, 'epoch': 0.05}\n",
      "{'loss': 4.3719, 'grad_norm': 0.8172014355659485, 'learning_rate': 0.0005967612903225807, 'epoch': 0.05}\n",
      "{'loss': 4.4648, 'grad_norm': 0.8716773986816406, 'learning_rate': 0.0005947290322580645, 'epoch': 0.05}\n",
      "{'loss': 4.3564, 'grad_norm': 0.8373059034347534, 'learning_rate': 0.0005926967741935483, 'epoch': 0.05}\n",
      "{'loss': 4.315, 'grad_norm': 0.8717809915542603, 'learning_rate': 0.0005906645161290322, 'epoch': 0.05}\n",
      "{'loss': 4.2834, 'grad_norm': 0.8495769500732422, 'learning_rate': 0.000588632258064516, 'epoch': 0.05}\n",
      "{'loss': 4.2581, 'grad_norm': 0.8540050983428955, 'learning_rate': 0.0005866000000000001, 'epoch': 0.05}\n",
      "{'loss': 4.2998, 'grad_norm': 0.8447644114494324, 'learning_rate': 0.0005845677419354839, 'epoch': 0.05}\n",
      "{'loss': 4.3678, 'grad_norm': 0.8612174391746521, 'learning_rate': 0.0005825354838709677, 'epoch': 0.05}\n",
      "{'loss': 4.3278, 'grad_norm': 0.9611419439315796, 'learning_rate': 0.0005805032258064516, 'epoch': 0.05}\n",
      "{'loss': 4.195, 'grad_norm': 0.8545340299606323, 'learning_rate': 0.0005784709677419354, 'epoch': 0.05}\n",
      "{'loss': 4.319, 'grad_norm': 0.9166994094848633, 'learning_rate': 0.0005764387096774192, 'epoch': 0.05}\n",
      "{'loss': 4.3153, 'grad_norm': 0.8658009171485901, 'learning_rate': 0.0005744064516129033, 'epoch': 0.05}\n",
      "{'loss': 4.2568, 'grad_norm': 0.8619129061698914, 'learning_rate': 0.000572374193548387, 'epoch': 0.05}\n",
      "{'loss': 4.3265, 'grad_norm': 0.8345709443092346, 'learning_rate': 0.000570341935483871, 'epoch': 0.05}\n",
      "{'loss': 4.2417, 'grad_norm': 0.8969289660453796, 'learning_rate': 0.0005683096774193548, 'epoch': 0.05}\n",
      "{'loss': 4.2441, 'grad_norm': 1.0291699171066284, 'learning_rate': 0.0005662774193548386, 'epoch': 0.05}\n",
      "{'loss': 4.2773, 'grad_norm': 1.01828932762146, 'learning_rate': 0.0005642451612903226, 'epoch': 0.05}\n",
      "{'loss': 4.2018, 'grad_norm': 0.8149434924125671, 'learning_rate': 0.0005622129032258064, 'epoch': 0.05}\n",
      "{'loss': 4.1988, 'grad_norm': 0.8818775415420532, 'learning_rate': 0.0005601806451612902, 'epoch': 0.05}\n",
      "{'loss': 4.2771, 'grad_norm': 0.9484167695045471, 'learning_rate': 0.0005581483870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2271, 'grad_norm': 0.8450266122817993, 'learning_rate': 0.000556116129032258, 'epoch': 0.05}\n",
      "{'loss': 4.1545, 'grad_norm': 0.8303521275520325, 'learning_rate': 0.000554083870967742, 'epoch': 0.05}\n",
      "{'loss': 4.2274, 'grad_norm': 0.8602861762046814, 'learning_rate': 0.0005520516129032258, 'epoch': 0.05}\n",
      "{'loss': 4.2262, 'grad_norm': 0.8777468800544739, 'learning_rate': 0.0005500193548387096, 'epoch': 0.05}\n",
      "{'loss': 4.269, 'grad_norm': 0.8417526483535767, 'learning_rate': 0.0005479870967741935, 'epoch': 0.05}\n",
      "{'loss': 4.274, 'grad_norm': 0.8356276154518127, 'learning_rate': 0.0005459548387096773, 'epoch': 0.05}\n",
      "{'loss': 4.1682, 'grad_norm': 0.8843129277229309, 'learning_rate': 0.0005439225806451613, 'epoch': 0.06}\n",
      "{'loss': 4.1708, 'grad_norm': 0.8013651967048645, 'learning_rate': 0.0005418903225806451, 'epoch': 0.06}\n",
      "{'loss': 4.1895, 'grad_norm': 0.8695991635322571, 'learning_rate': 0.0005398580645161289, 'epoch': 0.06}\n",
      "{'loss': 4.2231, 'grad_norm': 0.8430740833282471, 'learning_rate': 0.0005378258064516129, 'epoch': 0.06}\n",
      "{'loss': 4.2174, 'grad_norm': 0.9432917833328247, 'learning_rate': 0.0005357935483870967, 'epoch': 0.06}\n",
      "{'loss': 4.1528, 'grad_norm': 0.9237110614776611, 'learning_rate': 0.0005337612903225805, 'epoch': 0.06}\n",
      "{'loss': 4.195, 'grad_norm': 0.8522312641143799, 'learning_rate': 0.0005317290322580645, 'epoch': 0.06}\n",
      "{'loss': 4.1067, 'grad_norm': 0.9452381730079651, 'learning_rate': 0.0005296967741935483, 'epoch': 0.06}\n",
      "{'loss': 4.17, 'grad_norm': 0.7784274816513062, 'learning_rate': 0.0005276645161290323, 'epoch': 0.06}\n",
      "{'loss': 4.1252, 'grad_norm': 0.9013524651527405, 'learning_rate': 0.0005256322580645161, 'epoch': 0.06}\n",
      "{'loss': 4.1164, 'grad_norm': 0.7856761813163757, 'learning_rate': 0.0005235999999999999, 'epoch': 0.06}\n",
      "{'loss': 4.1233, 'grad_norm': 0.7758393287658691, 'learning_rate': 0.0005215677419354839, 'epoch': 0.06}\n",
      "{'loss': 4.1145, 'grad_norm': 0.8431938886642456, 'learning_rate': 0.0005195354838709677, 'epoch': 0.06}\n",
      "{'loss': 4.1548, 'grad_norm': 1.0190341472625732, 'learning_rate': 0.0005175032258064515, 'epoch': 0.06}\n",
      "{'loss': 4.1786, 'grad_norm': 0.9283260703086853, 'learning_rate': 0.0005154709677419354, 'epoch': 0.06}\n",
      "{'loss': 4.1751, 'grad_norm': 0.9026321768760681, 'learning_rate': 0.0005134387096774193, 'epoch': 0.06}\n",
      "{'loss': 4.1144, 'grad_norm': 0.9255345463752747, 'learning_rate': 0.0005114064516129032, 'epoch': 0.06}\n",
      "{'loss': 4.123, 'grad_norm': 0.8433995842933655, 'learning_rate': 0.0005093741935483871, 'epoch': 0.06}\n",
      "{'loss': 4.1115, 'grad_norm': 0.827038049697876, 'learning_rate': 0.0005073419354838709, 'epoch': 0.06}\n",
      "{'loss': 4.1883, 'grad_norm': 0.8610161542892456, 'learning_rate': 0.0005053096774193548, 'epoch': 0.06}\n",
      "{'loss': 4.0977, 'grad_norm': 0.8711548447608948, 'learning_rate': 0.0005032774193548386, 'epoch': 0.06}\n",
      "{'loss': 4.0821, 'grad_norm': 0.9176371097564697, 'learning_rate': 0.0005012451612903226, 'epoch': 0.06}\n",
      "{'loss': 4.1035, 'grad_norm': 0.8772139549255371, 'learning_rate': 0.0004992129032258064, 'epoch': 0.06}\n",
      "{'loss': 4.0909, 'grad_norm': 0.874914288520813, 'learning_rate': 0.0004971806451612902, 'epoch': 0.06}\n",
      "{'loss': 4.0282, 'grad_norm': 0.8976165056228638, 'learning_rate': 0.0004951483870967742, 'epoch': 0.06}\n",
      "{'loss': 4.0559, 'grad_norm': 0.8373813033103943, 'learning_rate': 0.000493116129032258, 'epoch': 0.06}\n",
      "{'loss': 4.1733, 'grad_norm': 0.8405694365501404, 'learning_rate': 0.000491083870967742, 'epoch': 0.06}\n",
      "{'loss': 4.0052, 'grad_norm': 0.87267005443573, 'learning_rate': 0.0004890516129032258, 'epoch': 0.06}\n",
      "{'loss': 4.0633, 'grad_norm': 0.9377471804618835, 'learning_rate': 0.0004870193548387096, 'epoch': 0.06}\n",
      "{'loss': 4.1776, 'grad_norm': 0.8689622282981873, 'learning_rate': 0.00048498709677419346, 'epoch': 0.06}\n",
      "{'loss': 4.0152, 'grad_norm': 0.8716417551040649, 'learning_rate': 0.0004829548387096773, 'epoch': 0.06}\n",
      "{'loss': 4.1405, 'grad_norm': 0.7633777856826782, 'learning_rate': 0.0004809225806451613, 'epoch': 0.07}\n",
      "{'loss': 4.0923, 'grad_norm': 0.789438009262085, 'learning_rate': 0.00047889032258064513, 'epoch': 0.07}\n",
      "{'loss': 4.0622, 'grad_norm': 0.8585381507873535, 'learning_rate': 0.000476858064516129, 'epoch': 0.07}\n",
      "{'loss': 4.0008, 'grad_norm': 0.7862159013748169, 'learning_rate': 0.00047482580645161285, 'epoch': 0.07}\n",
      "{'loss': 4.0483, 'grad_norm': 0.8336480855941772, 'learning_rate': 0.0004727935483870967, 'epoch': 0.07}\n",
      "{'loss': 3.936, 'grad_norm': 0.7503958344459534, 'learning_rate': 0.0004707612903225806, 'epoch': 0.07}\n",
      "{'loss': 4.0581, 'grad_norm': 0.8322245478630066, 'learning_rate': 0.0004687290322580644, 'epoch': 0.07}\n",
      "{'loss': 4.0316, 'grad_norm': 0.8769118189811707, 'learning_rate': 0.0004666967741935484, 'epoch': 0.07}\n",
      "{'loss': 3.9827, 'grad_norm': 0.8828204274177551, 'learning_rate': 0.00046466451612903225, 'epoch': 0.07}\n",
      "{'loss': 4.0278, 'grad_norm': 0.826027512550354, 'learning_rate': 0.0004626322580645161, 'epoch': 0.07}\n",
      "{'loss': 3.9745, 'grad_norm': 0.8396434187889099, 'learning_rate': 0.0004606, 'epoch': 0.07}\n",
      "{'loss': 3.985, 'grad_norm': 0.9325892925262451, 'learning_rate': 0.0004585677419354838, 'epoch': 0.07}\n",
      "{'loss': 3.9536, 'grad_norm': 0.7828598618507385, 'learning_rate': 0.0004565354838709677, 'epoch': 0.07}\n",
      "{'loss': 3.9383, 'grad_norm': 0.7971042394638062, 'learning_rate': 0.0004545032258064516, 'epoch': 0.07}\n",
      "{'loss': 3.9207, 'grad_norm': 0.9213614463806152, 'learning_rate': 0.0004524709677419354, 'epoch': 0.07}\n",
      "{'loss': 4.0187, 'grad_norm': 0.8571704030036926, 'learning_rate': 0.00045043870967741937, 'epoch': 0.07}\n",
      "{'loss': 4.004, 'grad_norm': 0.8306555151939392, 'learning_rate': 0.0004484064516129032, 'epoch': 0.07}\n",
      "{'loss': 3.9743, 'grad_norm': 0.7866419553756714, 'learning_rate': 0.0004463741935483871, 'epoch': 0.07}\n",
      "{'loss': 3.9674, 'grad_norm': 0.822945237159729, 'learning_rate': 0.00044434193548387093, 'epoch': 0.07}\n",
      "{'loss': 3.9546, 'grad_norm': 0.7810685038566589, 'learning_rate': 0.00044230967741935477, 'epoch': 0.07}\n",
      "{'loss': 3.9679, 'grad_norm': 0.8626220226287842, 'learning_rate': 0.0004402774193548387, 'epoch': 0.07}\n",
      "{'loss': 4.0492, 'grad_norm': 0.8550321459770203, 'learning_rate': 0.00043824516129032254, 'epoch': 0.07}\n",
      "{'loss': 3.9202, 'grad_norm': 0.9097416400909424, 'learning_rate': 0.0004362129032258064, 'epoch': 0.07}\n",
      "{'loss': 4.0377, 'grad_norm': 0.9113086462020874, 'learning_rate': 0.0004341806451612903, 'epoch': 0.07}\n",
      "{'loss': 3.937, 'grad_norm': 0.9276189804077148, 'learning_rate': 0.00043214838709677416, 'epoch': 0.07}\n",
      "{'loss': 3.8298, 'grad_norm': 0.9051833748817444, 'learning_rate': 0.00043011612903225805, 'epoch': 0.07}\n",
      "  7%|‚ñà‚ñà‚ñã                                 | 1119/15157 [29:59<6:12:42,  1.59s/it]2025-11-02 19:07:18,538 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1800.85 c\n",
      "{'loss': 3.9992, 'grad_norm': 0.824562668800354, 'learning_rate': 0.0004280838709677419, 'epoch': 0.07}\n",
      "{'train_runtime': 1800.8586, 'train_samples_per_second': 1077.299, 'train_steps_per_second': 8.417, 'train_loss': 4.978606733254024, 'epoch': 0.07}\n",
      "  7%|‚ñà‚ñà‚ñã                                 | 1120/15157 [30:00<6:16:10,  1.61s/it]\n",
      "2025-11-02 19:07:18,688 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-02 19:07:18,726 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:19<00:00, 31.85it/s]\n",
      "2025-11-02 19:07:40,431 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-02 19:07:44,718 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-02 19:07:44,718 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\n",
      "\n",
      "–£—á–∏–ª—Å—è –≤ —à–∫–æ–ª–µ –≤ –í–∞—Ä—à–∞–≤–µ –≤ 1978 –≥–æ–¥—É, —Ä–∞–±–æ—Ç–∞–ª –≤ —à–∫–æ–ª–µ, –∞ –∑–∞—Ç–µ–º –≤ ¬´–ù–æ–≤–æ–π¬ª, –≥–¥–µ –≤ 2002 –≥–æ–¥—É –≤–æ–∑–≥–ª–∞–≤–∏–ª ¬´–ï—Ä–º¬ª –∏ ¬´–ê—Å—Å–æ—Ü–∏–∞—Ü–∏—è¬ª, –∞ –≤ 2003 –≥–æ–¥—É —Å—Ç–∞–ª —á–ª–µ–Ω–æ–º-–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç –ù–ê–ù, –∏ –≤ 2007 –≥–æ–¥—É. –° 2004 –ø–æ 2007 –≥–æ–¥\n",
      "2025-11-02 19:07:44,719 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-02 19:07:44,719 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1800.8586, 'train_samples_per_second': 1077.299, 'train_steps_per_second': 8.417, 'total_flos': 3.7784517372542976e+17, 'train_loss': 4.978606733254024, 'epoch': 0.07389446946080129}, 'eval': {'eval_loss': 4.615426540374756, 'eval_runtime': 21.7023, 'eval_samples_per_second': 230.391, 'eval_steps_per_second': 28.799, 'epoch': 0.07389446946080129}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ.\\n\\n–£—á–∏–ª—Å—è –≤ —à–∫–æ–ª–µ –≤ –í–∞—Ä—à–∞–≤–µ –≤ 1978 –≥–æ–¥—É, —Ä–∞–±–æ—Ç–∞–ª –≤ —à–∫–æ–ª–µ, –∞ –∑–∞—Ç–µ–º –≤ ¬´–ù–æ–≤–æ–π¬ª, –≥–¥–µ –≤ 2002 –≥–æ–¥—É –≤–æ–∑–≥–ª–∞–≤–∏–ª ¬´–ï—Ä–º¬ª –∏ ¬´–ê—Å—Å–æ—Ü–∏–∞—Ü–∏—è¬ª, –∞ –≤ 2003 –≥–æ–¥—É —Å—Ç–∞–ª —á–ª–µ–Ω–æ–º-–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç –ù–ê–ù, –∏ –≤ 2007 –≥–æ–¥—É. –° 2004 –ø–æ 2007 –≥–æ–¥'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_1\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/q6scqvdn\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251102_183711-q6scqvdn/logs\u001b[0m\n",
      "[rank0]:[W1102 19:07:48.714223810 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=29502 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 1 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"deep_speed_setup_1\"\n",
    "\n",
    "# ~ –ø–æ 53–≥–± –Ω–∞ –∫–∞–∂–¥—ã–π GPU, –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–∞—è (95-100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412c424",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f9951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-02 18:35:36,764 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: World size: 2\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: CUDA_VISIBLE_DEVICES: 2,3\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: WANDB_PROJECT: llm_hw2-aylesnov\n",
      "2025-11-02 18:35:36,764 [INFO] __main__: –°–æ–∑–¥–∞–Ω–∏–µ DeepSpeed setup (stage 2)\n",
      "2025-11-02 18:35:37,066 [INFO] __main__: ============================================================\n",
      "2025-11-02 18:35:37,066 [INFO] __main__: –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–µ: deepspeed\n",
      "2025-11-02 18:35:37,067 [INFO] __main__: ============================================================\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 250874.26it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 2488.00it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "Resolving data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 213044.01it/s]\n",
      "Loading dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 4174.36it/s]\n",
      "Training samples:   1940063\n",
      "Validation samples: 5000\n",
      "2025-11-02 18:36:14,590 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-02 18:36:15,179 [INFO] lib.modeling: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: 960,881,664 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
      "2025-11-02 18:36:17,365 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-11-02 18:36:17,378 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "2025-11-02 18:36:17,379 [INFO] solution: Deepspeed config:\n",
      "{'bf16': {'enabled': True}, 'gradient_clipping': 1.0, 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'reduce_bucket_size': 5368709120, 'allgather_bucket_size': 5368709120, 'contiguous_gradients': True}, 'train_micro_batch_size_per_gpu': 16, 'gradient_accumulation_steps': 4}\n",
      "/app/lib/training.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/app/hw2_parallel_pretrain/wandb/run-20251102_183618-ohdroyih\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ohdroyih\u001b[0m\n",
      "2025-11-02 18:36:19,340 [INFO] __main__: Setup —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ\n",
      "2025-11-02 18:36:19,340 [INFO] __main__: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {'output_dir': '/app/output_dir/gpt2-1b-russian', 'optim': 'adamw_torch', 'num_train_epochs': 1, 'per_device_train_batch_size': 16, 'save_steps': 2500, 'save_total_limit': 2, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'logging_steps': 5, 'eval_steps': 2500, 'eval_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'gradient_checkpointing': False, 'gradient_accumulation_steps': 4, 'per_device_eval_batch_size': 4, 'dataloader_num_workers': 4, 'torch_compile': True, 'report_to': 'wandb', 'bf16': True}\n",
      "2025-11-02 18:36:19,341 [INFO] __main__: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è...\n",
      "2025-11-02 18:36:19,341 [INFO] lib.training: –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|                                                 | 0/15157 [00:00<?, ?it/s][rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank1]:W1102 18:36:24.319000 74687 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break from `Tensor.item()`, consider setting:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] or:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] to include these operations in the captured graph.\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] Graph break: from user code at:\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     outputs: BaseModelOutputWithPast = self.model(\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 969, in wrapper\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     output = func(self, *args, **kwargs)\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 446, in forward\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     causal_mask = self._update_causal_mask(\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]   File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 503, in _update_causal_mask\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0]     is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:24.627000 74686 torch/_dynamo/variables/tensor.py:780] [12/0] \n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1102 18:36:33.487000 74686 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py:201)\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8]    last reason: 21/0: L['self'].layer_idx == 0                                    \n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1102 18:36:33.524000 74687 torch/_dynamo/convert_frame.py:861] [21/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "{'loss': 10.7802, 'grad_norm': 4.457711696624756, 'learning_rate': 0.00032199999999999997, 'epoch': 0.0}\n",
      "{'loss': 9.814, 'grad_norm': 1.1450443267822266, 'learning_rate': 0.0006369999999999999, 'epoch': 0.0}\n",
      "{'loss': 9.1123, 'grad_norm': 0.6917006373405457, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.6894, 'grad_norm': 0.4294556975364685, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.4075, 'grad_norm': 0.3068470358848572, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.2607, 'grad_norm': 0.46698296070098877, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0793, 'grad_norm': 0.3000651001930237, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 8.0013, 'grad_norm': 0.30320146679878235, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9758, 'grad_norm': 0.2829124927520752, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.9002, 'grad_norm': 0.2770126760005951, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.8058, 'grad_norm': 0.22830359637737274, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.7603, 'grad_norm': 0.30593517422676086, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6773, 'grad_norm': 0.25265219807624817, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6612, 'grad_norm': 0.3519551157951355, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6512, 'grad_norm': 0.2797471582889557, 'learning_rate': 0.0006999999999999999, 'epoch': 0.0}\n",
      "{'loss': 7.6075, 'grad_norm': 0.30536147952079773, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5875, 'grad_norm': 0.35861480236053467, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.5731, 'grad_norm': 0.36572617292404175, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.496, 'grad_norm': 0.33908629417419434, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.4459, 'grad_norm': 0.32602888345718384, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3744, 'grad_norm': 0.3620181679725647, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3807, 'grad_norm': 0.3962878882884979, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.313, 'grad_norm': 0.3175576627254486, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.3354, 'grad_norm': 0.3439221680164337, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1872, 'grad_norm': 0.33288681507110596, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1349, 'grad_norm': 0.33349698781967163, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.2125, 'grad_norm': 0.3194226622581482, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.1455, 'grad_norm': 0.3063974380493164, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0892, 'grad_norm': 0.31547030806541443, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0741, 'grad_norm': 0.31945890188217163, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9759, 'grad_norm': 0.32254472374916077, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 7.0206, 'grad_norm': 0.3286651074886322, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9289, 'grad_norm': 0.29229632019996643, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8534, 'grad_norm': 0.30113258957862854, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.9235, 'grad_norm': 0.3840930759906769, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.8702, 'grad_norm': 0.3180418014526367, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7868, 'grad_norm': 0.36939695477485657, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7187, 'grad_norm': 0.3668995201587677, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7464, 'grad_norm': 0.41002270579338074, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7711, 'grad_norm': 0.3205455243587494, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.7618, 'grad_norm': 0.33544132113456726, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5692, 'grad_norm': 0.3724505603313446, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5679, 'grad_norm': 0.3030220866203308, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5567, 'grad_norm': 0.35912564396858215, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.5775, 'grad_norm': 0.36484673619270325, 'learning_rate': 0.0006999999999999999, 'epoch': 0.01}\n",
      "{'loss': 6.4596, 'grad_norm': 0.4041992425918579, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.5205, 'grad_norm': 0.2982620596885681, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4909, 'grad_norm': 0.31290310621261597, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4435, 'grad_norm': 0.32285112142562866, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3926, 'grad_norm': 0.3311881124973297, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3433, 'grad_norm': 0.366845965385437, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.4061, 'grad_norm': 0.34642431139945984, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.354, 'grad_norm': 0.3501901924610138, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.3238, 'grad_norm': 0.29291340708732605, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2694, 'grad_norm': 0.33467787504196167, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2843, 'grad_norm': 0.327462375164032, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1601, 'grad_norm': 0.34262213110923767, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2047, 'grad_norm': 0.30100950598716736, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2935, 'grad_norm': 0.3307054340839386, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1703, 'grad_norm': 0.43660768866539, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1843, 'grad_norm': 0.31481683254241943, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.2083, 'grad_norm': 0.31412428617477417, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1647, 'grad_norm': 0.3290216326713562, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1138, 'grad_norm': 0.3842146396636963, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0524, 'grad_norm': 0.4256753623485565, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.1355, 'grad_norm': 0.3093962073326111, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9936, 'grad_norm': 0.35658201575279236, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.0147, 'grad_norm': 0.3519841432571411, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9549, 'grad_norm': 0.36895686388015747, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9765, 'grad_norm': 0.34011754393577576, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 6.009, 'grad_norm': 0.34997400641441345, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9609, 'grad_norm': 0.3140244483947754, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9476, 'grad_norm': 0.3796195983886719, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.9339, 'grad_norm': 0.3468732535839081, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.952, 'grad_norm': 0.3201925456523895, 'learning_rate': 0.0006999999999999999, 'epoch': 0.02}\n",
      "{'loss': 5.8772, 'grad_norm': 0.3055405914783478, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8731, 'grad_norm': 0.29584717750549316, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8292, 'grad_norm': 0.3274056911468506, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7677, 'grad_norm': 0.31807228922843933, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8797, 'grad_norm': 0.3259202837944031, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8626, 'grad_norm': 0.35894858837127686, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.8613, 'grad_norm': 0.3046533465385437, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.781, 'grad_norm': 0.3606114983558655, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.83, 'grad_norm': 0.30257540941238403, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6409, 'grad_norm': 0.3422982394695282, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7197, 'grad_norm': 0.3144806921482086, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.671, 'grad_norm': 0.33248984813690186, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7495, 'grad_norm': 0.34157228469848633, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6899, 'grad_norm': 0.30253836512565613, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.7077, 'grad_norm': 0.3258510231971741, 'learning_rate': 0.0006999999999999999, 'epoch': 0.03}\n",
      "{'loss': 5.6644, 'grad_norm': 0.3498285710811615, 'learning_rate': 0.000698374193548387, 'epoch': 0.03}\n",
      "{'loss': 5.6336, 'grad_norm': 0.341901570558548, 'learning_rate': 0.0006963419354838709, 'epoch': 0.03}\n",
      "{'loss': 5.6662, 'grad_norm': 0.33505648374557495, 'learning_rate': 0.0006943096774193547, 'epoch': 0.03}\n",
      "{'loss': 5.5652, 'grad_norm': 0.2948814034461975, 'learning_rate': 0.0006922774193548388, 'epoch': 0.03}\n",
      "{'loss': 5.6279, 'grad_norm': 0.34074094891548157, 'learning_rate': 0.0006902451612903226, 'epoch': 0.03}\n",
      "{'loss': 5.6393, 'grad_norm': 0.313926637172699, 'learning_rate': 0.0006882129032258064, 'epoch': 0.03}\n",
      "{'loss': 5.5563, 'grad_norm': 0.3419014811515808, 'learning_rate': 0.0006861806451612903, 'epoch': 0.03}\n",
      "{'loss': 5.5615, 'grad_norm': 0.42348378896713257, 'learning_rate': 0.0006841483870967741, 'epoch': 0.03}\n",
      "{'loss': 5.6167, 'grad_norm': 0.36434975266456604, 'learning_rate': 0.0006821161290322579, 'epoch': 0.03}\n",
      "{'loss': 5.5421, 'grad_norm': 0.2790813148021698, 'learning_rate': 0.0006800838709677419, 'epoch': 0.03}\n",
      "{'loss': 5.5693, 'grad_norm': 0.3176637291908264, 'learning_rate': 0.0006780516129032257, 'epoch': 0.03}\n",
      "{'loss': 5.5641, 'grad_norm': 0.31082892417907715, 'learning_rate': 0.0006760193548387097, 'epoch': 0.03}\n",
      "{'loss': 5.5002, 'grad_norm': 0.3292393982410431, 'learning_rate': 0.0006739870967741935, 'epoch': 0.03}\n",
      "{'loss': 5.5474, 'grad_norm': 0.3477751910686493, 'learning_rate': 0.0006719548387096773, 'epoch': 0.03}\n",
      "{'loss': 5.4902, 'grad_norm': 0.3017272353172302, 'learning_rate': 0.0006699225806451613, 'epoch': 0.03}\n",
      "{'loss': 5.4458, 'grad_norm': 0.544732391834259, 'learning_rate': 0.0006678903225806451, 'epoch': 0.03}\n",
      "{'loss': 5.4659, 'grad_norm': 0.3258715271949768, 'learning_rate': 0.000665858064516129, 'epoch': 0.04}\n",
      "{'loss': 5.4492, 'grad_norm': 0.3748248517513275, 'learning_rate': 0.0006638258064516128, 'epoch': 0.04}\n",
      "{'loss': 5.4243, 'grad_norm': 0.37313714623451233, 'learning_rate': 0.0006617935483870967, 'epoch': 0.04}\n",
      "{'loss': 5.4582, 'grad_norm': 0.3185122311115265, 'learning_rate': 0.0006597612903225807, 'epoch': 0.04}\n",
      "{'loss': 5.4027, 'grad_norm': 0.3214268088340759, 'learning_rate': 0.0006577290322580645, 'epoch': 0.04}\n",
      "{'loss': 5.4507, 'grad_norm': 0.38999342918395996, 'learning_rate': 0.0006556967741935483, 'epoch': 0.04}\n",
      "{'loss': 5.4698, 'grad_norm': 0.30842965841293335, 'learning_rate': 0.0006536645161290322, 'epoch': 0.04}\n",
      "{'loss': 5.3942, 'grad_norm': 0.296842485666275, 'learning_rate': 0.000651632258064516, 'epoch': 0.04}\n",
      "{'loss': 5.4003, 'grad_norm': 0.3190489113330841, 'learning_rate': 0.0006495999999999999, 'epoch': 0.04}\n",
      "{'loss': 5.3966, 'grad_norm': 0.3261561095714569, 'learning_rate': 0.0006475677419354838, 'epoch': 0.04}\n",
      "{'loss': 5.3826, 'grad_norm': 0.3070858120918274, 'learning_rate': 0.0006455354838709677, 'epoch': 0.04}\n",
      "{'loss': 5.3638, 'grad_norm': 0.4066886603832245, 'learning_rate': 0.0006435032258064516, 'epoch': 0.04}\n",
      "{'loss': 5.3252, 'grad_norm': 0.3182021379470825, 'learning_rate': 0.0006414709677419354, 'epoch': 0.04}\n",
      "{'loss': 5.3245, 'grad_norm': 0.34209904074668884, 'learning_rate': 0.0006394387096774192, 'epoch': 0.04}\n",
      "{'loss': 5.4032, 'grad_norm': 0.31804507970809937, 'learning_rate': 0.0006374064516129032, 'epoch': 0.04}\n",
      "{'loss': 5.3064, 'grad_norm': 0.33731040358543396, 'learning_rate': 0.000635374193548387, 'epoch': 0.04}\n",
      "{'loss': 5.293, 'grad_norm': 0.31189024448394775, 'learning_rate': 0.0006333419354838709, 'epoch': 0.04}\n",
      "{'loss': 5.3553, 'grad_norm': 0.3341033458709717, 'learning_rate': 0.0006313096774193548, 'epoch': 0.04}\n",
      "{'loss': 5.3405, 'grad_norm': 0.3273971676826477, 'learning_rate': 0.0006292774193548386, 'epoch': 0.04}\n",
      "{'loss': 5.2921, 'grad_norm': 0.3570025861263275, 'learning_rate': 0.0006272451612903226, 'epoch': 0.04}\n",
      "{'loss': 5.3044, 'grad_norm': 0.8820928931236267, 'learning_rate': 0.0006252129032258064, 'epoch': 0.04}\n",
      "{'loss': 5.3403, 'grad_norm': 0.3153234124183655, 'learning_rate': 0.0006231806451612903, 'epoch': 0.04}\n",
      "{'loss': 5.3268, 'grad_norm': 0.33260735869407654, 'learning_rate': 0.0006211483870967741, 'epoch': 0.04}\n",
      "{'loss': 5.3429, 'grad_norm': 0.33333227038383484, 'learning_rate': 0.0006191161290322579, 'epoch': 0.04}\n",
      "{'loss': 5.2959, 'grad_norm': 0.31877583265304565, 'learning_rate': 0.000617083870967742, 'epoch': 0.04}\n",
      "{'loss': 5.274, 'grad_norm': 0.32106199860572815, 'learning_rate': 0.0006150516129032258, 'epoch': 0.04}\n",
      "{'loss': 5.2137, 'grad_norm': 0.3771252930164337, 'learning_rate': 0.0006130193548387097, 'epoch': 0.04}\n",
      "{'loss': 5.307, 'grad_norm': 0.3325613737106323, 'learning_rate': 0.0006109870967741935, 'epoch': 0.04}\n",
      "{'loss': 5.1725, 'grad_norm': 0.317754328250885, 'learning_rate': 0.0006089548387096773, 'epoch': 0.04}\n",
      "{'loss': 5.1742, 'grad_norm': 0.29978471994400024, 'learning_rate': 0.0006069225806451612, 'epoch': 0.04}\n",
      "  4%|‚ñà‚ñå                                  | 680/15157 [29:59<10:33:58,  2.63s/it]2025-11-02 19:06:25,953 [INFO] lib.callbacks: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ç–∞–π–º–∞—É—Ç—É: 1801.95 c\n",
      "{'train_runtime': 1801.9604, 'train_samples_per_second': 1076.64, 'train_steps_per_second': 8.411, 'train_loss': 6.316314328959685, 'epoch': 0.04}\n",
      "  4%|‚ñà‚ñå                                  | 681/15157 [30:01<10:38:24,  2.65s/it]\n",
      "2025-11-02 19:06:26,111 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "2025-11-02 19:06:26,134 [INFO] lib.training: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 625/625 [00:20<00:00, 30.66it/s]\n",
      "2025-11-02 19:06:48,639 [INFO] lib.training: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "2025-11-02 19:06:59,416 [INFO] lib.training: \n",
      "=== SAMPLE GENERATION ===\n",
      "2025-11-02 19:06:59,416 [INFO] lib.training: –í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ, –∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤ –µ–≥–æ.\n",
      "\n",
      "–ö–∞—Ä—å–µ—Ä–∞—Ç—É—Ä–∞ \n",
      "\n",
      "–í —Ñ–µ–≤—Ä–∞–ª–µ 2006 –≥–æ–¥–∞ –±—ã–ª —É–ø—Ä–∞–∑–¥–Ω—ë–Ω –≤ –≥–æ—Ä–æ–¥–µ –û–æ–≥–∏–Ω, –∞ –∑–∞—Ç–µ–º —Ä–∞–±–æ—Ç–∞–ª –≤ –Ω–∞—á–∞–ª–µ XX –≤–µ–∫–∞.\n",
      "\n",
      "–í 2013 –≥–æ–¥—É –æ–∫–æ–Ω—á–∏–ª –í.¬†–ò.¬†–ß–∞–π—Å–∫–æ–µ –∫–ª–∞–¥–±–∏—â–µ.\n",
      "\n",
      "–í 1937 –≥–æ–¥—É –Ω–∞–∑–Ω–∞—á–µ–Ω –≤ —Å–æ—Å—Ç–∞–≤ —Å–±–æ—Ä–Ω–æ–π –ü–æ–ª—å—à–∏.\n",
      "2025-11-02 19:06:59,417 [INFO] __main__: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "2025-11-02 19:06:59,417 [INFO] __main__: –ú–µ—Ç—Ä–∏–∫–∏: {'train': {'train_runtime': 1801.9604, 'train_samples_per_second': 1076.64, 'train_steps_per_second': 8.411, 'total_flos': 2.297433600955515e+17, 'train_loss': 6.316314328959685, 'epoch': 0.044930476520362216}, 'eval': {'eval_loss': 5.872274398803711, 'eval_runtime': 22.525, 'eval_samples_per_second': 221.976, 'eval_steps_per_second': 27.747, 'epoch': 0.044930476520362216}, 'generation': '–í –Ω–∞—á–∞–ª–µ –±—ã–ª–æ —Å–ª–æ–≤–æ, –∏ —Å–ª–æ–≤–æ –±—ã–ª–æ, –∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤ –µ–≥–æ.\\n\\n–ö–∞—Ä—å–µ—Ä–∞—Ç—É—Ä–∞ \\n\\n–í —Ñ–µ–≤—Ä–∞–ª–µ 2006 –≥–æ–¥–∞ –±—ã–ª —É–ø—Ä–∞–∑–¥–Ω—ë–Ω –≤ –≥–æ—Ä–æ–¥–µ –û–æ–≥–∏–Ω, –∞ –∑–∞—Ç–µ–º —Ä–∞–±–æ—Ç–∞–ª –≤ –Ω–∞—á–∞–ª–µ XX –≤–µ–∫–∞.\\n\\n–í 2013 –≥–æ–¥—É –æ–∫–æ–Ω—á–∏–ª –í.\\xa0–ò.\\xa0–ß–∞–π—Å–∫–æ–µ –∫–ª–∞–¥–±–∏—â–µ.\\n\\n–í 1937 –≥–æ–¥—É –Ω–∞–∑–Ω–∞—á–µ–Ω –≤ —Å–æ—Å—Ç–∞–≤ —Å–±–æ—Ä–Ω–æ–π –ü–æ–ª—å—à–∏.'}\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mbs16_ga4_lr5e-05_adamw_torch_deep_speed_setup_2\u001b[0m at: \u001b[34mhttps://wandb.ai/ksorzz/llm_hw2-aylesnov/runs/ohdroyih\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251102_183618-ohdroyih/logs\u001b[0m\n",
      "[rank0]:[W1102 19:07:02.380080020 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2,3 torchrun --nproc_per_node=2 --master_port=29501 ../train_distributed.py \\\n",
    "    --mode deepspeed \\\n",
    "    --deepspeed-stage 2 \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"deep_speed_setup_2\"\n",
    "\n",
    "# ~ –ø–æ 50–≥–± –Ω–∞ –∫–∞–∂–¥—ã–π GPU, –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–∞—è (95-100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29710b33",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e3d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 ../train_distributed.py \\\n",
    "    --mode fsdp \\\n",
    "    --batch-size 16 \\\n",
    "    --grad-accum 4 \\\n",
    "    --run-name \"fsdp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ba0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
